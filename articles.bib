@article{agarwalDevelopmentEfficientCNN2020,
  title = {Development of {{Efficient CNN}} Model for {{Tomato}} Crop Disease Identification},
  author = {Agarwal, Mohit and Gupta, Suneet Kr. and Biswas, K.K.},
  date = {2020-12},
  journaltitle = {Sustainable Computing: Informatics and Systems},
  shortjournal = {Sustainable Computing: Informatics and Systems},
  volume = {28},
  pages = {100407},
  issn = {22105379},
  doi = {10.1016/j.suscom.2020.100407},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2210537920301347},
  urldate = {2022-05-16},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/GQSTPD7F/Agarwal et al. - 2020 - Development of Efficient CNN model for Tomato crop.pdf}
}

@online{ainsworthPlateauPhenomenonGradient2020,
  title = {Plateau {{Phenomenon}} in {{Gradient Descent Training}} of {{ReLU}} Networks: {{Explanation}}, {{Quantification}} and {{Avoidance}}},
  shorttitle = {Plateau {{Phenomenon}} in {{Gradient Descent Training}} of {{ReLU}} Networks},
  author = {Ainsworth, Mark and Shin, Yeonjong},
  date = {2020-07-14},
  eprint = {2007.07213},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2007.07213},
  urldate = {2023-02-01},
  abstract = {The ability of neural networks to provide `best in class' approximation across a wide range of applications is well-documented. Nevertheless, the powerful expressivity of neural networks comes to naught if one is unable to effectively train (choose) the parameters defining the network. In general, neural networks are trained by gradient descent type optimization methods, or a stochastic variant thereof. In practice, such methods result in the loss function decreases rapidly at the beginning of training but then, after a relatively small number of steps, significantly slow down. The loss may even appear to stagnate over the period of a large number of epochs, only to then suddenly start to decrease fast again for no apparent reason. This so-called plateau phenomenon manifests itself in many learning tasks. The present work aims to identify and quantify the root causes of plateau phenomenon. No assumptions are made on the number of neurons relative to the number of training data, and our results hold for both the lazy and adaptive regimes. The main findings are: plateaux correspond to periods during which activation patterns remain constant, where activation pattern refers to the number of data points that activate a given neuron; quantification of convergence of the gradient flow dynamics; and, characterization of stationary points in terms solutions of local least squares regression lines over subsets of the training data. Based on these conclusions, we propose a new iterative training method, the Active Neuron Least Squares (ANLS), characterised by the explicit adjustment of the activation pattern at each step, which is designed to enable a quick exit from a plateau. Illustrative numerical examples are included throughout.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/AVT8C843/Ainsworth and Shin - 2020 - Plateau Phenomenon in Gradient Descent Training of.pdf;/Users/mavi/Zotero/storage/C9ZC7INX/2007.html}
}

@article{alzubaidiReviewDeepLearning2021,
  title = {Review of Deep Learning: Concepts, {{CNN}} Architectures, Challenges, Applications, Future Directions},
  shorttitle = {Review of Deep Learning},
  author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamaría, J. and Fadhel, Mohammed A. and Al-Amidie, Muthana and Farhan, Laith},
  date = {2021-12},
  journaltitle = {Journal of Big Data},
  shortjournal = {J Big Data},
  volume = {8},
  number = {1},
  pages = {53},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00444-8},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8},
  urldate = {2022-05-19},
  abstract = {In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achiev‑ing outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contrib‑uted several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most impor‑tant aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL tech‑niques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/8P5TG4H3/Alzubaidi et al. - 2021 - Review of deep learning concepts, CNN architectur.pdf}
}

@inproceedings{anfengheMultiorganPlantIdentification2016,
  title = {Multi-Organ Plant Identification with Multi-Column Deep Convolutional Neural Networks},
  booktitle = {2016 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {{Anfeng He} and {Xinmei Tian}},
  date = {2016-10},
  pages = {002020--002025},
  publisher = {{IEEE}},
  location = {{Budapest, Hungary}},
  doi = {10.1109/SMC.2016.7844537},
  url = {http://ieeexplore.ieee.org/document/7844537/},
  urldate = {2022-05-19},
  abstract = {Automatically identifying plants from images is a hot research topic due to its importance in production and science popularization. This process attempts to automatically identify the name of a plant with a known taxon from a given image. The majority of existing studies on automatic plant identification focus on identifying plants with a single organ, such as flower, leaf, or fruits. Plant identification using a single organ is not sufficiently reliable because different plants many have similar organs. To overcome this problem, this paper is devoted to automatically identifying plants by combining multiple organs of plants. Specifically, we propose a multi-column deep convolutional neural networks (MCDCNN) model to combine multiple organs for efficient plant identification. Extensive experiments demonstrate the effectiveness of our model, and the plant identification performance is greatly improved.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  isbn = {978-1-5090-1897-0},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/NMI3YCPJ/Anfeng He and Xinmei Tian - 2016 - Multi-organ plant identification with multi-column.pdf}
}

@article{arganda-carrerasTrainableWekaSegmentation2017,
  title = {Trainable {{Weka Segmentation}}: A Machine Learning Tool for Microscopy Pixel Classification},
  shorttitle = {Trainable {{Weka Segmentation}}},
  author = {Arganda-Carreras, Ignacio and Kaynig, Verena and Rueden, Curtis and Eliceiri, Kevin W and Schindelin, Johannes and Cardona, Albert and Sebastian Seung, H},
  date = {2017-08-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {33},
  number = {15},
  pages = {2424--2426},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btx180},
  url = {https://doi.org/10.1093/bioinformatics/btx180},
  urldate = {2023-05-07},
  abstract = {State-of-the-art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time-consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user-designed image features or classifiers.TWS is distributed as open-source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable\_Weka\_Segmentation.Supplementary data are available at Bioinformatics online.},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/RQFM4SEF/Arganda-Carreras et al_2017_Trainable Weka Segmentation.pdf;/Users/mavi/Zotero/storage/2XZLT4HW/3092362.html}
}

@article{arnalbarbedoPlantDiseaseIdentification2019,
  title = {Plant Disease Identification from Individual Lesions and Spots Using Deep Learning},
  author = {Arnal Barbedo, Jayme Garcia},
  date = {2019-04},
  journaltitle = {Biosystems Engineering},
  shortjournal = {Biosystems Engineering},
  volume = {180},
  pages = {96--107},
  issn = {15375110},
  doi = {10.1016/j.biosystemseng.2019.02.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1537511018307797},
  urldate = {2022-05-16},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/2UYA67ZK/Arnal Barbedo - 2019 - Plant disease identification from individual lesio.pdf}
}

@article{arsenovicSolvingCurrentLimitations2019,
  title = {Solving {{Current Limitations}} of {{Deep Learning Based Approaches}} for {{Plant Disease Detection}}},
  author = {Arsenovic, Marko and Karanovic, Mirjana and Sladojevic, Srdjan and Anderla, Andras and Stefanovic, Darko},
  date = {2019-07-19},
  journaltitle = {Symmetry},
  shortjournal = {Symmetry},
  volume = {11},
  number = {7},
  pages = {939},
  issn = {2073-8994},
  doi = {10.3390/sym11070939},
  url = {https://www.mdpi.com/2073-8994/11/7/939},
  urldate = {2022-07-04},
  abstract = {Plant diseases cause great damage in agriculture, resulting in significant yield losses. The recent expansion of deep learning methods has found its application in plant disease detection, offering a robust tool with highly accurate results. The current limitations and shortcomings of existing plant disease detection models are presented and discussed in this paper. Furthermore, a new dataset containing 79,265 images was introduced with the aim to become the largest dataset containing leaf images. Images were taken in various weather conditions, at different angles, and daylight hours with an inconsistent background mimicking practical situations. Two approaches were used to augment the number of images in the dataset: traditional augmentation methods and state-of-the-art style generative adversarial networks. Several experiments were conducted to test the impact of training in a controlled environment and usage in real-life situations to accurately identify plant diseases in a complex background and in various conditions including the detection of multiple diseases in a single leaf. Finally, a novel two-stage architecture of a neural network was proposed for plant disease classification focused on a real environment. The trained model achieved an accuracy of 93.67\%.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/9D3BH6MV/Arsenovic et al. - 2019 - Solving Current Limitations of Deep Learning Based.pdf}
}

@article{ashwinkumarAutomatedPlantLeaf2022,
  title = {Automated Plant Leaf Disease Detection and Classification Using Optimal {{MobileNet}} Based Convolutional Neural Networks},
  author = {Ashwinkumar, S. and Rajagopal, S. and Manimaran, V. and Jegajothi, B.},
  date = {2022},
  journaltitle = {Materials Today: Proceedings},
  shortjournal = {Materials Today: Proceedings},
  volume = {51},
  pages = {480--487},
  issn = {22147853},
  doi = {10.1016/j.matpr.2021.05.584},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2214785321042115},
  urldate = {2022-05-16},
  abstract = {Agriculture is the major occupation in India and it loses 35\% of the crop productivity annually owing to plant diseases. Earlier plant disease detection is a tedious process because of improper laboratory facilities and expert knowledge. Automated plant disease detection techniques are advantageous for reducing the laborious task of monitoring large crop farms and for identifying disease symptoms early on, i.e., when they appear on plant leaves. Recent advances in computer vision and deep learning (DL) models have demonstrated the value of developing automatic plant disease detection models based on visible symptoms on leaves. With this in mind, this article proposes an automated model for detecting and classifying plant leaf diseases using an optimal mobile network-based convolutional neural network (OMNCNN). The proposed OMNCNN model operates on different stages namely preprocessing, segmentation, feature extraction, and classification. It involves bilateral filtering (BF) based preprocessing and Kapur’s thresholding based image segmentation to identify the affected portions of the leaf image. In addition, the MobileNet model is applied as a feature extraction technique in which the hyperparameters are optimized by the use of emperor penguin optimizer (EPO) algorithm to enhance the plant disease detection rate. Finally, extreme learning machine (ELM) based classifier is utilized to allocate proper class labels to the applied plant leaf images. An extensive set of simulations were performed to highlight the superior performance of the OMNCNN model. The experimental outcome has shown promising results of the OMNCNN model over the recent state-of-art methods with the maximum precision of 0.985, recall of 0.9892, accuracy of 0.987, F-score of 0.985, and kappa of 0. 985.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/TZQM4YIN/Ashwinkumar et al. - 2022 - Automated plant leaf disease detection and classif.pdf}
}

@article{ayoniUrbanAgriculturePolicy2022,
  title = {Urban Agriculture and Policy: {{Mitigating}} Urban Negative Externalities},
  shorttitle = {Urban Agriculture and Policy},
  author = {Ayoni, V. D. Nirusha and Ramli, Nurul Nadia and Shamsudin, Mad Nasir and Hadi, Ahmad Hanis Izani Abdul},
  date = {2022-09-01},
  journaltitle = {Urban Forestry \& Urban Greening},
  shortjournal = {Urban Forestry \& Urban Greening},
  volume = {75},
  pages = {127710},
  issn = {1618-8667},
  doi = {10.1016/j.ufug.2022.127710},
  url = {https://www.sciencedirect.com/science/article/pii/S1618866722002539},
  urldate = {2022-09-29},
  abstract = {Agriculture and urban agglomerations can be combined in different perspectives, including building-integrated food production. Indeed, urban agriculture is less concerned with helping urban communities achieve food self-sufficiency and more concerned with strengthening urban inhabitants differently. "Different" signifies multifunctional forms of access to fresh and nutritious vegetables, food bill reduction, personal well-being, and user-friendly agriculture from socioeconomic perspectives. The study estimated willingness to pay and the value of socioeconomic benefits of urban agriculture practices. A choice experiment was performed in Colombo, Sri Lanka's most urbanized district in 2020. The data were estimated using the random parameter logit model. Positive and significant contributions to urban agriculture related to the utility of urban agriculture practitioners and non-practitioners were evident. The socioeconomic value of urban agriculture benefits generated is equivalent to USD 136,400 per community practices urban agriculture. Urban agriculture thus reveals its vitality as a tool for mitigating the negative impacts of urbanization. The research advocates Colombo municipality to actively encourage urban agriculture by including it in their planning policies.},
  langid = {english},
  keywords = {Choice experiment,Colombo,Multifunction,Random parameter logit,Urban agriculture,Willingness to pay},
  file = {/Users/mavi/Zotero/storage/5UD96IAE/Ayoni et al. - 2022 - Urban agriculture and policy Mitigating urban neg.pdf;/Users/mavi/Zotero/storage/TUUGFB2X/S1618866722002539.html}
}

@online{badrinarayananSegNetDeepConvolutional2016,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  date = {2016-10-10},
  eprint = {1511.00561},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.00561},
  urldate = {2022-05-20},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/mavi/Zotero/storage/VX9DAEG7/Badrinarayanan et al. - 2016 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf}
}

@article{bajwaSoybeanDiseaseMonitoring2017,
  title = {Soybean {{Disease Monitoring}} with {{Leaf Reflectance}}},
  author = {Bajwa, Sreekala and Rupe, John and Mason, Johnny},
  date = {2017-02-04},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {9},
  number = {2},
  pages = {127},
  issn = {2072-4292},
  doi = {10.3390/rs9020127},
  url = {http://www.mdpi.com/2072-4292/9/2/127},
  urldate = {2022-05-16},
  abstract = {Crop disease detection with remote sensing is a challenging area that can have significant economic and environmental impact on crop disease management. Spectroscopic remote sensing in the visible and near-infrared (NIR) region has the potential to detect crop changes due to diseases. Soybean cyst nematode (SCN) and sudden death syndrome (SDS) are two common soybean diseases that are extremely difficult to detect in the early stages under mild to moderate infestation levels. The objective of this research study was to relate leaf reflectance to disease conditions and to identify wavebands that best discriminated these crop diseases. A microplot experiment was conducted. Data collected included 800 leaf spectra, corresponding leaf chlorophyll content and disease rating of four soybean cultivars grown under different disease conditions. Disease conditions were created by introducing four disease treatments of control (no disease), SCN, SDS, and SCN+SDS. Crop data were collected on a weekly basis over a 10-week period, starting from 71 days after planting (DAP). The correlation between disease rating and selected vegetation indices (VI) were evaluated. Wavebands with the most disease discrimination capability were identified with stepwise linear discriminant analysis (LDA), logistic discriminant analysis (LgDA) and linear correlation analysis of pooled data. The identified band combinations were used to develop a classification function to identify plant disease condition. The best correlation ({$>$}0.8) between disease rating and VI occurred during 112 DAP. Both LDA and LgDA identified several bands in the NIR, red, green and blue regions as critical for disease discrimination. The discriminant models were able to detect over 80\% of the healthy plants accurately under cross-validation but showed poor accuracy in discriminating individual diseases. A two-class discriminant model was able to identify 97\% of the healthy plants and 58\% of the infested plants as having some disease from the plant spectra.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/I5LDZ6IZ/Bajwa et al. - 2017 - Soybean Disease Monitoring with Leaf Reflectance.pdf}
}

@online{baoBEiTBERTPreTraining2022,
  title = {{{BEiT}}: {{BERT Pre-Training}} of {{Image Transformers}}},
  shorttitle = {{{BEiT}}},
  author = {Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  date = {2022-09-03},
  eprint = {2106.08254},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.08254},
  url = {http://arxiv.org/abs/2106.08254},
  urldate = {2023-05-02},
  abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read},
  file = {/Users/mavi/Zotero/storage/ICRYXVWL/Bao et al_2022_BEiT.pdf;/Users/mavi/Zotero/storage/8IUWJ63N/2106.html}
}

@online{BeallListPotential,
  title = {Beall's {{List}} – of {{Potential Predatory Journals}} and {{Publishers}}},
  url = {https://beallslist.net/},
  urldate = {2023-01-16},
  langid = {american}
}

@article{bellinResistancePlasmoparaViticola2009,
  title = {Resistance to {{Plasmopara}} Viticola in Grapevine ‘{{Bianca}}’ Is Controlled by a Major Dominant Gene Causing Localised Necrosis at the Infection Site},
  author = {Bellin, Diana and Peressotti, Elisa and Merdinoglu, Didier and Wiedemann-Merdinoglu, Sabine and Adam-Blondon, Anne-Françoise and Cipriani, Guido and Morgante, Michele and Testolin, Raffaele and Di Gaspero, Gabriele},
  date = {2009-12},
  journaltitle = {Theoretical and Applied Genetics},
  shortjournal = {Theor Appl Genet},
  volume = {120},
  number = {1},
  pages = {163--176},
  issn = {0040-5752, 1432-2242},
  doi = {10.1007/s00122-009-1167-2},
  url = {http://link.springer.com/10.1007/s00122-009-1167-2},
  urldate = {2022-08-30},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/E68FXKQZ/Bellin et al. - 2009 - Resistance to Plasmopara viticola in grapevine ‘Bi.pdf}
}

@article{bergIlastikInteractiveMachine2019,
  title = {Ilastik: Interactive Machine Learning for (Bio)Image Analysis},
  shorttitle = {Ilastik},
  author = {Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and Straehle, Christoph N. and Kausler, Bernhard X. and Haubold, Carsten and Schiegg, Martin and Ales, Janez and Beier, Thorsten and Rudy, Markus and Eren, Kemal and Cervantes, Jaime I. and Xu, Buote and Beuttenmueller, Fynn and Wolny, Adrian and Zhang, Chong and Koethe, Ullrich and Hamprecht, Fred A. and Kreshuk, Anna},
  date = {2019-12},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {16},
  number = {12},
  pages = {1226--1232},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0582-9},
  url = {https://www.nature.com/articles/s41592-019-0582-9},
  urldate = {2023-04-28},
  abstract = {We present ilastik, an easy-to-use interactive tool that brings machine-learning-based (bio)image analysis to end users without substantial computational expertise. It contains pre-defined workflows for image segmentation, object classification, counting and tracking. Users adapt the workflows to the problem at hand by interactively providing sparse training annotations for a nonlinear classifier. ilastik can process data in up to five dimensions (3D, time and number of channels). Its computational back end runs operations on-demand wherever possible, allowing for interactive prediction on data larger than RAM. Once the classifiers are trained, ilastik workflows can be applied to new data from the command line without further user interaction. We describe all ilastik workflows in detail, including three case studies and a discussion on the expected performance.},
  issue = {12},
  langid = {english},
  keywords = {Image processing,Machine learning,Software},
  file = {/Users/mavi/Zotero/storage/AUZRHKPS/Berg et al_2019_ilastik.pdf}
}

@article{bernotIntroductionAuxTypes,
  title = {Introduction aux Types Abstraits Algébriques Définitions et Résultats},
  author = {Bernot, Gilles},
  langid = {french},
  keywords = {⛔ No DOI found},
  file = {/Users/mavi/Zotero/storage/FYVIQD8Y/Bernot - Introduction aux Types Abstraits Algébriques Déﬁni.pdf}
}

@inproceedings{bhugraHierarchicalFrameworkLeaf2021,
  title = {A {{Hierarchical Framework}} for {{Leaf Instance Segmentation}}: {{Application}} to {{Plant Phenotyping}}},
  shorttitle = {A {{Hierarchical Framework}} for {{Leaf Instance Segmentation}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Bhugra, Swati and Garg, Kanish and Chaudhury, Santanu and Lall, Brejesh},
  date = {2021-01-10},
  pages = {10173--10179},
  publisher = {{IEEE}},
  location = {{Milan, Italy}},
  doi = {10.1109/ICPR48806.2021.9411981},
  url = {https://ieeexplore.ieee.org/document/9411981/},
  urldate = {2022-10-21},
  abstract = {Image based analysis of plants is a high-throughput and non-invasive approach to study plant traits. The quantitative estimation of many plant traits (leaf area index, biomass etc.) from plant images is primarily based on accurate segmentation of individual leaves. This is a challenging task due to the presence of overlapped leaves and lack of discernible boundaries between them. To overcome these limitations, state-of-the-art supervised deep learning algorithms have been recently employed. However, the annotations of individual leaf instances is time consuming, in addition the variability in leaf shapes and its arrangement among different plant species limits the broad utilisation of these algorithms. To relieve this bottleneck, we propose a novel framework that relies on a graph based formulation to extract leaf shape knowledge for the task of leaf instance segmentation. These shape priors are generated based on leaf shape characteristics independent of plant species. Evaluation of the proposed framework on multiple plant datasets i.e. Arabidopsis, Komatsuna and salad demonstrates its broad utility.},
  eventtitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  isbn = {978-1-72818-808-9},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/7L3YGMFW/Bhugra et al_2021_A Hierarchical Framework for Leaf Instance Segmentation.pdf}
}

@article{biermanHighThroughputPhenotypingSystem2019,
  title = {A {{High-Throughput Phenotyping System Using Machine Vision}} to {{Quantify Severity}} of {{Grapevine Powdery Mildew}}},
  author = {Bierman, Andrew and LaPlumm, Tim and Cadle-Davidson, Lance and Gadoury, David and Martinez, Dani and Sapkota, Surya and Rea, Mark},
  date = {2019-08-25},
  journaltitle = {Plant Phenomics},
  shortjournal = {Plant Phenomics},
  volume = {2019},
  pages = {1--13},
  issn = {2643-6515},
  doi = {10.34133/2019/9209727},
  url = {https://spj.sciencemag.org/journals/plantphenomics/2019/9209727/},
  urldate = {2022-06-03},
  abstract = {Powdery mildews present specific challenges to phenotyping systems that are based on imaging. Having previously developed low-throughput, quantitative microscopy approaches for phenotyping resistance to               Erysiphe necator               on thousands of grape leaf disk samples for genetic analysis, here we developed automated imaging and analysis methods for               E. necator               severity on leaf disks. By pairing a 46-megapixel CMOS sensor camera, a long-working distance lens providing 3.5× magnification, X-Y sample positioning, and Z-axis focusing movement, the system captured 78\% of the area of a 1-cm diameter leaf disk in 3 to 10 focus-stacked images within 13.5 to 26 seconds. Each image pixel represented 1.44               μ               m               2               of the leaf disk. A convolutional neural network (CNN) based on GoogLeNet determined the presence or absence of               E. necator               hyphae in approximately 800 subimages per leaf disk as an assessment of severity, with a training validation accuracy of 94.3\%. For an independent image set the CNN was in agreement with human experts for 89.3\% to 91.7\% of subimages. This live-imaging approach was nondestructive, and a repeated measures time course of infection showed differentiation among susceptible, moderate, and resistant samples. Processing over one thousand samples per day with good accuracy, the system can assess host resistance, chemical or biological efficacy, or other phenotypic responses of grapevine to               E. necator               . In addition, new CNNs could be readily developed for phenotyping within diverse pathosystems or for diverse traits amenable to leaf disk assays.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/7CFN9S7C/Bierman et al_2019_A High-Throughput Phenotyping System Using Machine Vision to Quantify Severity.pdf}
}

@article{biermannFacilitatedEndosporeDetection2022,
  title = {Facilitated Endospore Detection for {{{\emph{Bacillus}}}} Spp. through Automated Algorithm‐based Image Processing},
  author = {Biermann, Riekje and Niemeyer, Laura and Rösner, Laura and Ude, Christian and Lindner, Patrick and Bice, Ismet and Beutel, Sascha},
  date = {2022-03},
  journaltitle = {Engineering in Life Sciences},
  shortjournal = {Engineering in Life Sciences},
  volume = {22},
  number = {3-4},
  pages = {299--307},
  issn = {1618-0240, 1618-2863},
  doi = {10.1002/elsc.202100137},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/elsc.202100137},
  urldate = {2022-05-20},
  abstract = {Bacillus spp. endospores are important dormant cell forms and are distributed widely in environmental samples. While these endospores can have important industrial value (e.g. use in animal feed as probiotics), they can also be pathogenic for humans and animals, emphasizing the need for effective endospore detection. Standard spore detection by colony forming units (CFU) is time-consuming, elaborate and prone to error. Manual spore detection by spore count in cell counting chambers via phase-contrast microscopy is less timeconsuming. However, it requires a trained person to conduct. Thus, the development of a facilitated spore detection tool is necessary. This work presents two alternative quantification methods: first, a colorimetric assay for detecting the biomarker dipicolinic acid (DPA) adapted to modern needs and applied for Bacillus spp. and second, a model-based automated spore detection algorithm for spore count in phase-contrast microscopic pictures. This automated spore count tool advances manual spore detection in cell counting chambers, and does not require human overview after sample preparation. In conclusion, this developed model detected various Bacillus spp. endospores with a correctness of 85–89\%, and allows an automation and time-saving of Bacillus endospore detection. In the laboratory routine, endospore detection and counting was achieved within 5–10 min, compared to up to 48 h with conventional methods. The DPA-assay on the other hand enabled very accurate spore detection by simple colorimetric measurement and can thus be applied as a reference method.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/ZMBWVWRF/Biermann et al. - 2022 - Facilitated endospore detection for Bacillusi.pdf}
}

@article{bowmanFlatFieldColourCorrection2020,
  title = {Flat-{{Field}} and {{Colour Correction}} for the {{Raspberry Pi Camera Module}}},
  author = {Bowman, Richard W. and Vodenicharski, Boyko and Collins, Joel T. and Stirling, Julian},
  date = {2020-04-13},
  journaltitle = {Journal of Open Hardware},
  volume = {4},
  number = {1},
  pages = {1},
  issn = {2514-1708},
  doi = {10.5334/joh.20},
  url = {http://openhardware.metajnl.com/articles/10.5334/joh.20/},
  urldate = {2022-06-09},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/N5XSWRX5/Bowman et al. - 2020 - Flat-Field and Colour Correction for the Raspberry.pdf}
}

@article{bozinovskiReminderFirstPaper2020,
  title = {Reminder of the {{First Paper}} on {{Transfer Learning}} in {{Neural Networks}}, 1976},
  author = {Bozinovski, Stevo},
  date = {2020-09-15},
  journaltitle = {Informatica},
  volume = {44},
  number = {3},
  issn = {1854-3871},
  doi = {10.31449/inf.v44i3.2828},
  url = {https://www.informatica.si/index.php/informatica/article/view/2828},
  urldate = {2023-05-02},
  abstract = {This paper describes a work on transfer learning in neural networks carried out in 1970s and early 1980s, which produced its first publication in 1976. In the contemporary research on transfer learning there is a belief that pioneering work on transfer learning took place in early 1990s, and this paper updates that knowledge, pointing out that the transfer learning research started more than a decade earlier. This paper reviews that 1970s research and addresses important issues relevant for the current transfer learning research. It gives a mathematical model and geometric interpretation of transfer learning, and ~a measure of transfer learning indicating positive, negative, and no transfer learning. It presents experimental investigation in the mentioned types of transfer learning. And it gives an application of transfer learning in pattern recognition using datasets of images.},
  issue = {3},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/S6U2BWJW/Bozinovski_2020_Reminder of the First Paper on Transfer Learning in Neural Networks, 1976.pdf}
}

@article{bradskiOpenCVLibrary2000,
  title = {The {{OpenCV Library}}},
  author = {Bradski, G.},
  date = {2000},
  journaltitle = {Dr. Dobb's Journal of Software Tools},
  keywords = {bibtex-import,read}
}

@online{burdaImportanceWeightedAutoencoders2016,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  date = {2016-11-07},
  eprint = {1509.00519},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1509.00519},
  url = {http://arxiv.org/abs/1509.00519},
  urldate = {2022-12-25},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/N7JSIIVZ/Burda et al. - 2016 - Importance Weighted Autoencoders.pdf;/Users/mavi/Zotero/storage/C2X4YZNT/1509.html}
}

@online{burgessUnderstandingDisentanglingBeta2018,
  title = {Understanding Disentangling in \$\textbackslash beta\$-{{VAE}}},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  date = {2018-04-10},
  eprint = {1804.03599},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.03599},
  url = {http://arxiv.org/abs/1804.03599},
  urldate = {2022-12-25},
  abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in \$\textbackslash beta\$-VAE, as training progresses. From these insights, we propose a modification to the training regime of \$\textbackslash beta\$-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \$\textbackslash beta\$-VAE, without the previous trade-off in reconstruction accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/GDZAUQJU/Burgess et al. - 2018 - Understanding disentangling in $beta$-VAE.pdf;/Users/mavi/Zotero/storage/3WKFUS5A/1804.html}
}

@online{caiCascadeRCNNHigh2019,
  title = {Cascade {{R-CNN}}: {{High Quality Object Detection}} and {{Instance Segmentation}}},
  shorttitle = {Cascade {{R-CNN}}},
  author = {Cai, Zhaowei and Vasconcelos, Nuno},
  date = {2019-06-24},
  eprint = {1906.09756},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.09756},
  urldate = {2022-05-17},
  abstract = {In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its quality. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset, and significantly improves high-quality detection on generic and specific object detection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance segmentation, with nontrivial improvements over the Mask R-CNN. To facilitate future research, two implementations are made available at https://github.com/zhaoweicai/cascade-rcnn (Caffe) and https://github.com/zhaoweicai/Detectron-Cascade-RCNN (Detectron).},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/4FL883VV/Cai and Vasconcelos - 2019 - Cascade R-CNN High Quality Object Detection and I.pdf}
}

@article{caiPanopticSegmentationBasedAttention2020,
  title = {Panoptic {{Segmentation-Based Attention}} for {{Image Captioning}}},
  author = {Cai, Wenjie and Xiong, Zheng and Sun, Xianfang and Rosin, Paul L. and Jin, Longcun and Peng, Xinyi},
  date = {2020-01-04},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {10},
  number = {1},
  pages = {391},
  issn = {2076-3417},
  doi = {10.3390/app10010391},
  url = {https://www.mdpi.com/2076-3417/10/1/391},
  urldate = {2022-05-17},
  abstract = {Image captioning is the task of generating textual descriptions of images. In order to obtain a better image representation, attention mechanisms have been widely adopted in image captioning. However, in existing models with detection-based attention, the rectangular attention regions are not fine-grained, as they contain irrelevant regions (e.g., background or overlapped regions) around the object, making the model generate inaccurate captions. To address this issue, we propose panoptic segmentation-based attention that performs attention at a mask-level (i.e., the shape of the main part of an instance). Our approach extracts feature vectors from the corresponding segmentation regions, which is more fine-grained than current attention mechanisms. Moreover, in order to process features of different classes independently, we propose a dual-attention module which is generic and can be applied to other frameworks. Experimental results showed that our model could recognize the overlapped objects and understand the scene better. Our approach achieved competitive performance against state-of-the-art methods. We made our code available.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/Z4N8BL67/Cai et al. - 2020 - Panoptic Segmentation-Based Attention for Image Ca.pdf}
}

@article{caoRankConsistentOrdinal2020,
  title = {Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation},
  author = {Cao, Wenzhi and Mirjalili, Vahid and Raschka, Sebastian},
  date = {2020-12-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {140},
  pages = {325--331},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2020.11.008},
  url = {https://www.sciencedirect.com/science/article/pii/S016786552030413X},
  urldate = {2023-06-22},
  abstract = {In many real-world prediction tasks, class labels include information about the relative ordering between labels, which is not captured by commonly-used loss functions such as multi-category cross-entropy. Recently, the deep learning community adopted ordinal regression frameworks to take such ordering information into account. Neural networks were equipped with ordinal regression capabilities by transforming ordinal targets into binary classification subtasks. However, this method suffers from inconsistencies among the different binary classifiers. To resolve these inconsistencies, we propose the COnsistent RAnk Logits (CORAL) framework with strong theoretical guarantees for rank-monotonicity and consistent confidence scores. Moreover, the proposed method is architecture-agnostic and can extend arbitrary state-of-the-art deep neural network classifiers for ordinal regression tasks. The empirical evaluation of the proposed rank-consistent method on a range of face-image datasets for age prediction shows a substantial reduction of the prediction error compared to the reference ordinal regression network.},
  langid = {english},
  keywords = {\_tablet,Age prediction,Biometrics,Convolutional neural networks,Deep learning,Machine learning,Ordinal regression},
  file = {/Users/mavi/Zotero/storage/NZ6NDZ77/Cao et al_2020_Rank consistent ordinal regression for neural networks with application to age.pdf;/Users/mavi/Zotero/storage/TRV6AV6G/S016786552030413X.html}
}

@article{capilla-perezHEMLinesNew2018,
  title = {The {{HEM Lines}}: {{A New Library}} of {{Homozygous Arabidopsis}} Thaliana {{EMS Mutants}} and Its {{Potential}} to {{Detect Meiotic Phenotypes}}},
  shorttitle = {The {{HEM Lines}}},
  author = {Capilla-Perez, Laia and Solier, Victor and Portemer, Virginie and Chambon, Aurelie and Hurel, Aurelie and Guillebaux, Alexia and Vezon, Daniel and Cromer, Laurence and Grelon, Mathilde and Mercier, Raphael},
  date = {2018-09-19},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front Plant Sci},
  volume = {9},
  eprint = {30283471},
  eprinttype = {pmid},
  pages = {1339},
  issn = {1664-462X},
  doi = {10.3389/fpls.2018.01339},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157545/},
  urldate = {2023-04-28},
  abstract = {Genetic screens have been crucial for deciphering many important biological processes, including meiosis. In Arabidopsis thaliana, previous forward screens have likely identified almost all the meiotic genes that when mutated lead to a pronounced decrease in fertility. However, the increasing number of genes identified in reverse genetics studies that play crucial roles in meiosis, but do not exhibit strong phenotypes when mutated, suggests that there are still many genes with meiotic function waiting to be discovered. In this study, we produced 897 A. thaliana homozygous mutant lines using Ethyl Methyl Sulfonate (EMS) mutagenesis followed by either single seed descent or haploid doubling. Whole genome sequencing of a subset of lines showed an average of 696 homozygous mutations per line, 195 of which (28\%) modify a protein sequence. To test the power of this library, we carried out a forward screen looking for meiotic defects by observing chromosomes at metaphase I of male meiosis. Among the 649 lines analyzed, we identified 43 lines with meiotic defects. Of these, 21 lines had an obvious candidate causal mutation, namely a STOP or splicing site mutation in a gene previously shown to play a role in meiosis (ATM, MLH3, MLH1, MER3, HEI10, FLIP, ASY4, FLIP, PRD2, REC8, FANCL, and PSS1). Interestingly, this was the first time that six of these genes were identified in a forward screen in Arabidopsis (MLH3, MLH1, SGO1, PSS1, FANCL, and ASY4). These results illustrate the potential of this mutant population for screening for any qualitative or quantitative phenotype. Thus, this new mutant library is a powerful tool for functional genomics in A. thaliana. The HEM (Homozygote EMS Mutants) lines are available at the Versailles Arabidopsis stock center.},
  pmcid = {PMC6157545},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/HYYZ4B97/Capilla-Perez et al_2018_The HEM Lines.pdf}
}

@online{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  date = {2020-05-28},
  eprint = {2005.12872},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.12872},
  urldate = {2022-10-26},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/PMCNPQM3/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf;/Users/mavi/Zotero/storage/2QYJQK9I/2005.html}
}

@online{carterTensorflowNeuralNetwork,
  title = {Tensorflow — {{Neural Network Playground}}},
  author = {Carter, Daniel Smilkov {and} Shan},
  url = {http://playground.tensorflow.org},
  urldate = {2023-01-17},
  abstract = {Tinker with a real neural network right here in your browser.},
  file = {/Users/mavi/Zotero/storage/UFK4YFYJ/playground.tensorflow.org.html}
}

@article{caruanaMultitaskLearning,
  title = {Multitask {{Learning}}},
  author = {Caruana, Rich},
  pages = {35},
  abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/HMYPN9CE/Caruana - Multitask Learning.pdf}
}

@incollection{caruanaMultitaskLearningKnowledgeBased1993,
  title = {Multitask {{Learning}}: {{A Knowledge-Based Source}} of {{Inductive Bias}}},
  shorttitle = {Multitask {{Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1993},
  author = {Caruana, Richard A.},
  date = {1993},
  pages = {41--48},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-307-3.50012-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603073500125},
  urldate = {2022-05-19},
  abstract = {This paper suggests that it may be easier to learn several hard tasks at one time than to learn these same tasks separately. In effect, the informa­ tion provided by the training signal for each task serves as a domain-specific inductive bias for the other tasks. Frequently the world gives us clusters of related tasks to learn. When it does not, it is of­ ten straightforward to create additional tasks. For many domains, acquiring inductive bias by col­ lecting additional teaching signal may be more practical than the traditional approach of codify­ ing domain-specific biases acquired from human expertise. We call this approach Multitask Learn­ ing (MTL). Since much of the power of an induc­ tive learner follows directly from its inductive bias, multitask learning may yield more power­ ful learning. An empirical example of multitask connectionist learning is presented where learn­ ing improves by training one network on several related tasks at the same time. Multitask decision tree induction is also outlined.},
  isbn = {978-1-55860-307-3},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/FI5DLMFQ/Caruana - 1993 - Multitask Learning A Knowledge-Based Source of In.pdf}
}

@article{chamierDemocratisingDeepLearning2021,
  title = {Democratising Deep Learning for Microscopy with {{ZeroCostDL4Mic}}},
  author = {Chamier, Lucas and Laine, Romain and Jukkala, Johanna and Spahn, Christoph and Krentzel, Daniel and Nehme, Elias and Lerche, Martina and Hernández Pérez, Sara and Mattila, Pieta and Karinou, Eleni and Holden, Séamus and Solak, Ahmet and Krull, Alexander and Buchholz, Tim-Oliver and Jones, Martin and Royer, Loic and Leterrier, Christophe and Shechtman, Yoav and Jug, Florian and Henriques, Ricardo},
  date = {2021-04-15},
  journaltitle = {Nature Communications},
  shortjournal = {Nature Communications},
  volume = {12},
  doi = {10.1038/s41467-021-22518-0},
  abstract = {Deep Learning (DL) methods are powerful analytical tools for microscopy and can outperform conventional image processing pipelines. Despite the enthusiasm and innovations fuelled by DL technology, the need to access powerful and compatible resources to train DL networks leads to an accessibility barrier that novice users often find difficult to overcome. Here, we present ZeroCostDL4Mic, an entry-level platform simplifying DL access by leveraging the free, cloud-based computational resources of Google Colab. ZeroCostDL4Mic allows researchers with no coding expertise to train and apply key DL networks to perform tasks including segmentation (using U-Net and StarDist), object detection (using YOLOv2), denoising (using CARE and Noise2Void), super-resolution microscopy (using Deep-STORM), and image-to-image translation (using Label-free prediction - fnet, pix2pix and CycleGAN). Importantly, we provide suitable quantitative tools for each network to evaluate model performance, allowing model optimisation. We demonstrate the application of the platform to study multiple biological processes.},
  file = {/Users/mavi/Zotero/storage/S9AXTR7A/Chamier et al. - 2021 - Democratising deep learning for microscopy with Ze.pdf}
}

@article{chapeau-blondeauFractalStructureColor2009,
  title = {Fractal Structure in the Color Distribution of Natural Images},
  author = {Chapeau-Blondeau, François and Chauveau, Julien and Rousseau, David and Richard, Paul},
  date = {2009-10},
  journaltitle = {Chaos, Solitons \& Fractals},
  shortjournal = {Chaos, Solitons \& Fractals},
  volume = {42},
  number = {1},
  pages = {472--482},
  issn = {09600779},
  doi = {10.1016/j.chaos.2009.01.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960077909000083},
  urldate = {2022-05-16},
  abstract = {The colorimetric organization of RGB color images is investigated through the computation of the correlation integral of their three-dimensional histogram. For natural color images, as a common behavior, the correlation integral is found to follow a power law, with a noninteger exponent characteristic of a given image. This behavior identifies a fractal or multiscale self-similar distribution of the colors contained in typical natural images. This finding of a possible fractal structure in the colorimetric organization of natural images complement other fractal properties previously observed in their spatial organization. Such fractal colorimetric properties may be helpful to the characterization and modeling of natural images, and may contribute to progress in vision.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/M5IZITZ2/Chapeau-Blondeau et al. - 2009 - Fractal structure in the color distribution of nat.pdf}
}

@article{chapeau-blondeauRaisingNoiseImprove2009,
  title = {Raising the Noise to Improve Performance in Optimal Processing},
  author = {Chapeau-Blondeau, François and Rousseau, David},
  date = {2009-01-05},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2009},
  number = {01},
  pages = {P01003},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2009/01/P01003},
  url = {https://iopscience.iop.org/article/10.1088/1742-5468/2009/01/P01003},
  urldate = {2022-05-16},
  abstract = {We formulate, in general terms, the classical theory of optimal detection and optimal estimation of signal in noise. In this framework, we exhibit specific examples of optimal detectors and optimal estimators endowed with a performance which can be improved by injecting more noise. From this proof of feasibility by examples, we suggest a general mechanism by which noise improvement of optimal processing, although seemingly paradoxical, may indeed occur. Beyond specific examples, this leads us to the formulation of open problems concerning the general characterization, including the conditions of formal feasibility and of practical realizability, of such situations of optimal processing improved by noise.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/GPNSLRY9/Chapeau-Blondeau and Rousseau - 2009 - Raising the noise to improve performance in optima.pdf}
}

@article{chenGenerativePretrainingPixels,
  title = {Generative {{Pretraining}} from {{Pixels}}},
  author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  pages = {13},
  abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full fine-tuning, matching the top supervised pretrained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0\% top-1 accuracy on a linear probe of our features.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/ZPYMLC7P/Chen et al. - Generative Pretraining from Pixels.pdf}
}

@online{chengNeuralNetworkApproach2007,
  title = {A Neural Network Approach to Ordinal Regression},
  author = {Cheng, Jianlin},
  date = {2007-04-08},
  eprint = {0704.1028},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.0704.1028},
  url = {http://arxiv.org/abs/0704.1028},
  urldate = {2023-06-20},
  abstract = {Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data processing tasks such as information retrieval, web page ranking, collaborative filtering, and protein ranking in Bioinformatics.},
  pubstate = {preprint},
  keywords = {\_tablet,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/mavi/Zotero/storage/FQ2ZEAKD/Cheng_2007_A neural network approach to ordinal regression.pdf;/Users/mavi/Zotero/storage/5PSG52YB/0704.html}
}

@online{chenIsolatingSourcesDisentanglement2019,
  title = {Isolating {{Sources}} of {{Disentanglement}} in {{Variational Autoencoders}}},
  author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  date = {2019-04-23},
  eprint = {1802.04942},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1802.04942},
  url = {http://arxiv.org/abs/1802.04942},
  urldate = {2022-12-25},
  abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \$\textbackslash beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \$\textbackslash beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/49EVXCPP/Chen et al. - 2019 - Isolating Sources of Disentanglement in Variationa.pdf;/Users/mavi/Zotero/storage/ZAH6BZGP/1802.html}
}

@online{chenTransUNetTransformersMake2021,
  title = {{{TransUNet}}: {{Transformers Make Strong Encoders}} for {{Medical Image Segmentation}}},
  shorttitle = {{{TransUNet}}},
  author = {Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L. and Zhou, Yuyin},
  date = {2021-02-08},
  eprint = {2102.04306},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.04306},
  urldate = {2022-07-04},
  abstract = {Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the ushaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/KFS9XXJV/Chen et al. - 2021 - TransUNet Transformers Make Strong Encoders for M.pdf}
}

@article{cooperClimateChangeinducedVariations2022,
  title = {Climate Change-Induced Variations in Blue and Green Water Usage in {{U}}.{{S}}. Urban Agriculture},
  author = {Cooper, Carolyn M. and Troutman, Jacob P. and Awal, Ripendra and Habibi, Hamideh and Fares, Ali},
  date = {2022-05-10},
  journaltitle = {Journal of Cleaner Production},
  shortjournal = {Journal of Cleaner Production},
  volume = {348},
  pages = {131326},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2022.131326},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652622009544},
  urldate = {2022-09-29},
  abstract = {Urban agriculture could assist in meeting the growing global demand for food without overburdening agricultural areas. To fully realize the potential of urban agriculture, it is necessary to better understand the implications of urban agriculture and climate change on the food-energy-water nexus. The objective of this study was to investigate the influence of local climate change on irrigation requirements, and green and blue water usages for turf grass and three common urban agriculture crops (carrots, spinach, and sweet corn) in eight mid-sized U.S. cities. Baseline (1980–2010) and Future (2040–2050) daily climate data were combined with site-specific crop water uptake data to calculate irrigation requirements using the Irrigation Management System Model, IManSys, a numerical simulation model that uses a water balance approach. The irrigation requirements (IRRs) were further used to calculate the energy requirements and associated greenhouse gas emissions for the four crops in each location. Results showed the spatio-temporal impact of climate change on precipitation and evapotranspiration and consequently on crop IRRs. On the east coast, increases in summer precipitation during the crop growing seasons result in relatively small increases in blue water contributions ({$<$}222\%) to crop water demands. On the west coast, though, decreases in precipitation lead to more drastic increases in blue water contributions ({$>$}222\%) for these same crops. The energy requirements and greenhouse gas footprints of urban agriculture were weakly correlated to the blue water portion of the IRRs in individual cities but were largely impacted by the source of the water used. Overall, the results highlight the importance of appropriate and thoughtful crop selection for urban agriculture paired with environmentally sustainable water sourcing to maintain, or even reduce, future water and energy footprints of urban agriculture.},
  langid = {english},
  keywords = {Blue water,Climate change adaptation,Green water,IManSys,Irrigation water requirements,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/BPJXJMMW/Cooper et al. - 2022 - Climate change-induced variations in blue and gree.pdf;/Users/mavi/Zotero/storage/9ZCMHYSJ/S0959652622009544.html}
}

@article{couttTreeStabilityMediterranean1986,
  title = {Tree Stability on {{Mediterranean}} Soil},
  author = {Coutt, M.P},
  date = {1986},
  journaltitle = {Springer},
  pages = {250}
}

@article{cruzDetectionGrapevineYellows2019,
  title = {Detection of Grapevine Yellows Symptoms in {{Vitis}} Vinifera {{L}}. with Artificial Intelligence},
  author = {Cruz, Albert and Ampatzidis, Yiannis and Pierro, Roberto and Materazzi, Alberto and Panattoni, Alessandra and De Bellis, Luigi and Luvisi, Andrea},
  date = {2019-02},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {157},
  pages = {63--76},
  issn = {01681699},
  doi = {10.1016/j.compag.2018.12.028},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169918312353},
  urldate = {2022-05-16},
  abstract = {Grapevine yellows (GY) are a significant threat to grapes due to the severe symptoms and lack of treatments. Conventional diagnosis of the phytoplasmas associated to GYs relies on symptom identification, due to sensitivity limits of diagnostic tools (e.g. real time PCR) in asymptomatic vines, where the low concentration of the pathogen or its erratic distribution can lead to a high rate of false-negatives. GY’s primary symptoms are leaf discoloration and irregular wood ripening, which can be easily confused for symptoms of other diseases making recognition a difficult task. Herein, we present a novel system, utilizing convolutional neural networks, for endto-end detection of GY in red grape vine (cv. Sangiovese), using color images of leaf clippings. The diagnostic test detailed in this work does not require the user to be an expert at identifying GY. Data augmentation strategies make the system robust to alignment errors during data capture. When applied to the task of recognizing GY from digital images of leaf clippings—amongst many other diseases and a healthy control—the system has a sensitivity of 98.96\% and a specificity of 99.40\%. Deep learning has 35.97\% and 9.88\% better predictive value (PPV) when recognizing GY from sight, than a baseline system without deep learning and trained humans respectively. We evaluate six neural network architectures: AlexNet, GoogLeNet, Inception v3, ResNet-50, ResNet-101 and SqueezeNet. We find ResNet-50 to be the best compromise of accuracy and training cost. The trained neural networks, code to reproduce the experiments, and data of leaf clipping images are available on the internet. This work will advance the frontier of GY detection by improving detection speed, enabling a more effective response to the disease.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/8UJN5JR6/Cruz et al. - 2019 - Detection of grapevine yellows symptoms in Vitis v.pdf}
}

@article{decarvalhoPanopticSegmentationMeets2022,
  title = {Panoptic {{Segmentation Meets Remote Sensing}}},
  author = {family=Carvalho, given=Osmar Luiz Ferreira, prefix=de, useprefix=true and family=Carvalho Júnior, given=Osmar Abílio, prefix=de, useprefix=true and family=Silva, given=Cristiano Rosa, prefix=e, useprefix=false and family=Albuquerque, given=Anesmar Olino, prefix=de, useprefix=true and Santana, Nickolas Castro and Borges, Dibio Leandro and Gomes, Roberto Arnaldo Trancoso and Guimarães, Renato Fontes},
  date = {2022-02-16},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {14},
  number = {4},
  pages = {965},
  issn = {2072-4292},
  doi = {10.3390/rs14040965},
  url = {https://www.mdpi.com/2072-4292/14/4/965},
  urldate = {2022-05-17},
  abstract = {Panoptic segmentation combines instance and semantic predictions, allowing the detection of countable objects and different backgrounds simultaneously. Effectively approaching panoptic segmentation in remotely sensed data is very promising since it provides a complete classification, especially in areas with many elements as the urban setting. However, some difficulties have prevented the growth of this task: (a) it is very laborious to label large images with many classes, (b) there is no software for generating DL samples in the panoptic segmentation format, (c) remote sensing images are often very large requiring methods for selecting and generating samples, and (d) most available software is not friendly to remote sensing data formats (e.g., TIFF). Thus, this study aims to increase the operability of panoptic segmentation in remote sensing by providing: (1) a pipeline for generating panoptic segmentation datasets, (2) software to create deep learning samples in the Common Objects in Context (COCO) annotation format automatically, (3) a novel dataset, (4) leverage the Detectron2 software for compatibility with remote sensing data, and (5) evaluate this task on the urban setting. The proposed pipeline considers three inputs (original image, semantic image, and panoptic image), and our software uses these inputs alongside point shapefiles to automatically generate samples in the COCO annotation format. We generated 3400 samples with 512 × 512 pixel dimensions and evaluated the dataset using Panoptic-FPN. Besides, the metric analysis considered semantic, instance, and panoptic metrics, obtaining 93.865 mean intersection over union (mIoU), 47.691 Average (AP) Precision, and 64.979 Panoptic Quality (PQ). Our study presents the first effective pipeline for generating panoptic segmentation data for remote sensing targets.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/IH8VAPT4/de Carvalho et al. - 2022 - Panoptic Segmentation Meets Remote Sensing.pdf}
}

@article{delplaceRobustnessPlantQuantitative2020,
  title = {Robustness of Plant Quantitative Disease Resistance Is Provided by a Decentralized Immune Network},
  author = {Delplace, Florent and Huard-Chauveau, Carine and Dubiella, Ullrich and Khafif, Mehdi and Alvarez, Eva and Langin, Gautier and Roux, Fabrice and Peyraud, Rémi and Roby, Dominique},
  date = {2020-07-28},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  volume = {117},
  number = {30},
  eprint = {32669441},
  eprinttype = {pmid},
  pages = {18099--18109},
  issn = {0027-8424},
  doi = {10.1073/pnas.2000078117},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7395444/},
  urldate = {2023-05-03},
  abstract = {Molecular studies of plant immune responses have mainly focused on qualitative resistance, a form of immunity determined by a few large effect genes. In contrast, very limited information exists about quantitative disease resistance (QDR), although it is extensively observed in wild and crop species. We used systems biology approaches to describe this form of immunity in Arabidopsis thaliana. On the basis of gene regulation studies and search for protein–protein interactions, we report the reconstruction of a highly interconnected and distributed network, organized in five modules with differential robustness to genetic mutations. These studies revealed key functions of QDR, mainly distinct from those previously identified for plant immunity, and shed some light on the complexity of this plant immune response., Quantitative disease resistance (QDR) represents the predominant form of resistance in natural populations and crops. Surprisingly, very limited information exists on the biomolecular network of the signaling machineries underlying this form of plant immunity. This lack of information may result from its complex and quantitative nature. Here, we used an integrative approach including genomics, network reconstruction, and mutational analysis to identify and validate molecular networks that control QDR in Arabidopsis thaliana in response to the bacterial pathogen Xanthomonas campestris. To tackle this challenge, we first performed a transcriptomic analysis focused on the early stages of infection and using transgenic lines deregulated for the expression of RKS1, a gene underlying a QTL conferring quantitative and broad-spectrum resistance to X. campestris. RKS1-dependent gene expression was shown to involve multiple cellular activities (signaling, transport, and metabolism processes), mainly distinct from effector-triggered immunity (ETI) and pathogen-associated molecular pattern (PAMP)-triggered immunity (PTI) responses already characterized in A. thaliana. Protein–protein interaction network reconstitution then revealed a highly interconnected and distributed RKS1-dependent network, organized in five gene modules. Finally, knockout mutants for 41 genes belonging to the different functional modules of the network revealed that 76\% of the genes and all gene modules participate partially in RKS1-mediated resistance. However, these functional modules exhibit differential robustness to genetic mutations, indicating that, within the decentralized structure of the QDR network, some modules are more resilient than others. In conclusion, our work sheds light on the complexity of QDR and provides comprehensive understanding of a QDR immune network.},
  pmcid = {PMC7395444},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/5X8JPR8X/Delplace et al_2020_Robustness of plant quantitative disease resistance is provided by a.pdf}
}

@inproceedings{dempsterMINIROCKETVeryFast2021,
  title = {{{MINIROCKET}}: {{A Very Fast}} ({{Almost}}) {{Deterministic Transform}} for {{Time Series Classification}}},
  shorttitle = {{{MINIROCKET}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Dempster, Angus and Schmidt, Daniel F. and Webb, Geoffrey I.},
  date = {2021-08-14},
  eprint = {2012.08791},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {248--257},
  doi = {10.1145/3447548.3467231},
  url = {http://arxiv.org/abs/2012.08791},
  urldate = {2023-03-25},
  abstract = {Until recently, the most accurate methods for time series classification were limited by high computational complexity. ROCKET achieves state-of-the-art accuracy with a fraction of the computational expense of most existing methods by transforming input time series using random convolutional kernels, and using the transformed features to train a linear classifier. We reformulate ROCKET into a new method, MINIROCKET, making it up to 75 times faster on larger datasets, and making it almost deterministic (and optionally, with additional computational expense, fully deterministic), while maintaining essentially the same accuracy. Using this method, it is possible to train and test a classifier on all of 109 datasets from the UCR archive to state-of-the-art accuracy in less than 10 minutes. MINIROCKET is significantly faster than any other method of comparable accuracy (including ROCKET), and significantly more accurate than any other method of even roughly-similar computational expense. As such, we suggest that MINIROCKET should now be considered and used as the default variant of ROCKET.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/ETJMFVJL/Dempster et al_2021_MINIROCKET.pdf;/Users/mavi/Zotero/storage/T8YE2I7F/2012.html}
}

@article{desaiDiscriminativeModelsMultiClass2011,
  title = {Discriminative {{Models}} for {{Multi-Class Object Layout}}},
  author = {Desai, Chaitanya and Ramanan, Deva and Fowlkes, Charless C.},
  date = {2011-10},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {95},
  number = {1},
  pages = {1--12},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-011-0439-x},
  url = {http://link.springer.com/10.1007/s11263-011-0439-x},
  urldate = {2022-06-03},
  abstract = {Many state-of-the-art approaches for object recognition reduce the problem to a 0-1 classification task. This allows one to leverage sophisticated machine learning techniques for training classifiers from labeled examples. However, these models are typically trained independently for each class using positive and negative examples cropped from images. At test-time, various post-processing heuristics such as non-maxima suppression (NMS) are required to reconcile multiple detections within and between different classes for each image. Though crucial to good performance on benchmarks, this post-processing is usually defined heuristically.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/RDB97C5R/Desai et al. - 2011 - Discriminative Models for Multi-Class Object Layou.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2023-02-06},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read},
  file = {/Users/mavi/Zotero/storage/UMXJIAFU/Dosovitskiy et al_2021_An Image is Worth 16x16 Words.pdf;/Users/mavi/Zotero/storage/VAWUC9ZY/2010.html}
}

@article{douarreNovelDataAugmentation2019,
  title = {Novel Data Augmentation Strategies to Boost Supervised Segmentation of Plant Disease},
  author = {Douarre, Clément and Crispim-Junior, Carlos F. and Gelibert, Anthony and Tougne, Laure and Rousseau, David},
  date = {2019-10},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {165},
  pages = {104967},
  issn = {01681699},
  doi = {10.1016/j.compag.2019.104967},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169919304879},
  urldate = {2022-05-16},
  abstract = {Annotation of images in supervised learning is notably costly and time-consuming. In order to reduce this cost, our objective was to generate images from a small dataset of annotated images, and then use those synthesized images to help the network’s training process. In this article, we tackled for illustration with agricultural material the difficult segmentation task of apple scab on images of apple plant canopy by using convolutional neural networks. We devised two novel methods of generating data for this use case: one based on a plant canopy simulation and the other on Generative Adversatial Networks (GANs). As a result, we found that simulated data could provide an important increase in segmentation performance, up to a 17\% increase of F1 score (a measure taking into account precision and recall), compared to segmenting with weights initialized on ImageNet. In this way, we managed to obtain, with small datasets, higher segmentation scores than the ones obtained with bigger datasets if using no such augmentations. Moreover, we left our annotated dataset of scab available for the plant science imaging community. The proposed method is of large applicability for plant diseases observed at a canopy scale.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/XYYHNYGR/Douarre et al. - 2019 - Novel data augmentation strategies to boost superv.pdf}
}

@online{dupontLearningDisentangledJoint2018,
  title = {Learning {{Disentangled Joint Continuous}} and {{Discrete Representations}}},
  author = {Dupont, Emilien},
  date = {2018-10-22},
  eprint = {1804.00104},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.00104},
  url = {http://arxiv.org/abs/1804.00104},
  urldate = {2022-12-25},
  abstract = {We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/J88TFQDR/Dupont - 2018 - Learning Disentangled Joint Continuous and Discret.pdf;/Users/mavi/Zotero/storage/6RXU8IGY/1804.html}
}

@article{elharroussPanopticSegmentationReview,
  title = {Panoptic {{Segmentation}}: {{A Review}}},
  author = {Elharrouss, Omar and Al-Maadeed, Somaya and Subramanian, Nandhini and Ottakath, Najmath},
  pages = {29},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/GQ7KIDI8/Elharrouss et al. - Panoptic Segmentation A Review.pdf}
}

@article{espejo-garciaImprovingWeedsIdentification2020,
  title = {Improving Weeds Identification with a Repository of Agricultural Pre-Trained Deep Neural Networks},
  author = {Espejo-Garcia, Borja and Mylonas, Nikolaos and Athanasakos, Loukas and Fountas, Spyros},
  date = {2020-08},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {175},
  pages = {105593},
  issn = {01681699},
  doi = {10.1016/j.compag.2020.105593},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S016816992030692X},
  urldate = {2022-05-20},
  abstract = {Nowadays, several studies in the field of deep learning in agriculture obtain high performances in weeds identification by fine-tuning neural networks, previously trained on general-purpose datasets containing images unrelated to agriculture. This work examines whether these achievements could be further improved by finetuning neural networks pre-trained on agricultural datasets instead of ImageNet. The experimental results showed that with the suggested method the overall performance can increase. Some architectures such as Xception and Inception-Resnet presented an improvement of 0.51\% and 1.89\% respectively, while reducing the number of epochs by 13.67\%. It is then argued that an agricultural repository should be developed to engage research into making their pre-trained neural networks publicly available, for the benefit of research progress and efficiency.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/3GBJZXQA/Espejo-Garcia et al. - 2020 - Improving weeds identification with a repository o.pdf}
}

@article{espejo-garciaWeedsIdentificationAssistance2020,
  title = {Towards Weeds Identification Assistance through Transfer Learning},
  author = {Espejo-Garcia, Borja and Mylonas, Nikos and Athanasakos, Loukas and Fountas, Spyros and Vasilakoglou, Ioannis},
  date = {2020-04},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {171},
  pages = {105306},
  issn = {01681699},
  doi = {10.1016/j.compag.2020.105306},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169919319854},
  urldate = {2022-05-20},
  abstract = {Reducing the use of pesticides through selective spraying is an important component towards a more sustainable computer-assisted agriculture. Weed identification at early growth stage contributes to reduced herbicide rates. However, while computer vision alongside deep learning have overcome the performance of approaches that use hand-crafted features, there are still some open challenges in the development of a reliable automatic plant identification system. These type of systems have to take into account different sources of variability, such as growth stages and soil conditions, with the added constraint of the limited size of usual datasets. This study proposes a novel crop/weed identification system that relies on a combination of fine-tuning pre-trained convolutional networks (Xception, Inception-Resnet, VGNets, Mobilenet and Densenet) with the “traditional” machine learning classifiers (Support Vector Machines, XGBoost and Logistic Regression) trained with the previously deep extracted features. The aim of this approach was to avoid overfitting and to obtain a robust and consistent performance. To evaluate this approach, an open access dataset of two crop [tomato (Solanum lycopersicum L.) and cotton (Gossypium hirsutum L.)] and two weed species [black nightshade (Solanum nigrum L.) and velvetleaf (Abutilon theophrasti Medik.)] was generated. The pictures were taken by different production sites across Greece under natural variable light conditions from RGB cameras. The results revealed that a combination of fine-tuned Densenet and Support Vector Machine achieved a micro F1 score of 99.29\% with a very low performance difference between train and test sets. Other evaluated approaches also obtained repeatedly more than 95\% F1 score. Additionally, our results analysis provides some heuristics for designing transfer-learning based systems to avoid overfitting without decreasing performance.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/5A9AVNNA/Espejo-Garcia et al. - 2020 - Towards weeds identification assistance through tr.pdf}
}

@article{espositoGHOSTAdjustingDecision2021,
  title = {{{GHOST}}: {{Adjusting}} the {{Decision Threshold}} to {{Handle Imbalanced Data}} in {{Machine Learning}}},
  shorttitle = {{{GHOST}}},
  author = {Esposito, Carmen and Landrum, Gregory A. and Schneider, Nadine and Stiefl, Nikolaus and Riniker, Sereina},
  date = {2021-06-28},
  journaltitle = {Journal of Chemical Information and Modeling},
  shortjournal = {J. Chem. Inf. Model.},
  volume = {61},
  number = {6},
  pages = {2623--2640},
  publisher = {{American Chemical Society}},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.1c00160},
  url = {https://doi.org/10.1021/acs.jcim.1c00160},
  urldate = {2023-04-18},
  abstract = {Machine learning classifiers trained on class imbalanced data are prone to overpredict the majority class. This leads to a larger misclassification rate for the minority class, which in many real-world applications is the class of interest. For binary data, the classification threshold is set by default to 0.5 which, however, is often not ideal for imbalanced data. Adjusting the decision threshold is a good strategy to deal with the class imbalance problem. In this work, we present two different automated procedures for the selection of the optimal decision threshold for imbalanced classification. A major advantage of our procedures is that they do not require retraining of the machine learning models or resampling of the training data. The first approach is specific for random forest (RF), while the second approach, named GHOST, can be potentially applied to any machine learning classifier. We tested these procedures on 138 public drug discovery data sets containing structure–activity data for a variety of pharmaceutical targets. We show that both thresholding methods improve significantly the performance of RF. We tested the use of GHOST with four different classifiers in combination with two molecular descriptors, and we found that most classifiers benefit from threshold optimization. GHOST also outperformed other strategies, including random undersampling and conformal prediction. Finally, we show that our thresholding procedures can be effectively applied to real-world drug discovery projects, where the imbalance and characteristics of the data vary greatly between the training and test sets.},
  file = {/Users/mavi/Zotero/storage/S62VLIQ6/Esposito et al_2021_GHOST.pdf;/Users/mavi/Zotero/storage/AC7WK8N7/acs.jcim.html}
}

@article{evansEcosystemServiceDelivery2022,
  title = {Ecosystem Service Delivery by Urban Agriculture and Green Infrastructure – a Systematic Review},
  author = {Evans, D. L. and Falagán, N. and Hardman, C. A. and Kourmpetli, S. and Liu, L. and Mead, B. R. and Davies, J. A. C.},
  date = {2022-04-01},
  journaltitle = {Ecosystem Services},
  shortjournal = {Ecosystem Services},
  volume = {54},
  pages = {101405},
  issn = {2212-0416},
  doi = {10.1016/j.ecoser.2022.101405},
  url = {https://www.sciencedirect.com/science/article/pii/S2212041622000018},
  urldate = {2022-09-29},
  abstract = {The ability for urban ecosystems to deliver provisioning, regulating, cultural, and supporting services is vital for the health, sustainability, and resilience of urban environments. The increasing pressures being placed on urban environments by global climate change and the need to create sustainable food systems contributes to rising interest in green infrastructure and urban agriculture solutions. Yet, few studies have systematically assessed the ecosystem service provision of urban agriculture and green infrastructure in parallel. In this systematic review of 157 peer-reviewed journal articles, we synthesize the benefits and disbenefits of implementing various forms of urban agriculture and green infrastructure for the delivery of ecosystem services in urban areas. While both provide a diverse variety of ecosystem services, our review suggests that some services are provided more prevalently when green infrastructure is solely adopted (e.g., Local Climate and Air Quality Regulation), while other services are best delivered when green infrastructure is combined with urban agriculture (e.g., Biological Control and Maintenance of Genetic Diversity). Our data also show that ecosystem service delivery is partly modulated by the spaces in which urban growing takes place. Community Gardens, Green Spaces, Allotments, and Parks are found to be most conducive for diverse service provision, although it is also clear that some growing spaces have not been studied as frequently in urban ecosystem service research. We conclude by highlighting some key research gaps and priorities for urban ecosystem service research, including a stronger focus on under-represented services and growing spaces, the need for more systematic data collection, and the value of incorporating ecosystem service assessments into wider suitability and cost-benefit analyses.},
  langid = {english},
  keywords = {Ecosystem services,Green infrastructure,Growing space,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/W9VVD9VK/Evans et al. - 2022 - Ecosystem service delivery by urban agriculture an.pdf;/Users/mavi/Zotero/storage/V5ZJVBF6/S2212041622000018.html}
}

@inproceedings{faregarnotPanopticSegmentationSatellite2021,
  title = {Panoptic {{Segmentation}} of {{Satellite Image Time Series}} with {{Convolutional Temporal Attention Networks}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Fare Garnot, Vivien Sainte and Landrieu, Loic},
  date = {2021-10},
  pages = {4852--4861},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00483},
  url = {https://ieeexplore.ieee.org/document/9711189/},
  urldate = {2022-05-17},
  abstract = {Unprecedented access to multi-temporal satellite imagery has opened new perspectives for a variety of Earth observation tasks. Among them, pixel-precise panoptic segmentation of agricultural parcels has major economic and environmental implications. While researchers have explored this problem for single images, we argue that the complex temporal patterns of crop phenology are better addressed with temporal sequences of images. In this paper, we present the first end-to-end, single-stage method for panoptic segmentation of Satellite Image Time Series (SITS). This module can be combined with our novel image sequence encoding network which relies on temporal selfattention to extract rich and adaptive multi-scale spatiotemporal features. We also introduce PASTIS, the first openaccess SITS dataset with panoptic annotations. We demonstrate the superiority of our encoder for semantic segmentation against multiple competing architectures, and set up the first state-of-the-art of panoptic segmentation of SITS. Our implementation and PASTIS are publicly available.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/S4WKAFW3/Fare Garnot and Landrieu - 2021 - Panoptic Segmentation of Satellite Image Time Seri.pdf}
}

@online{FindingBestClassification,
  title = {Finding the {{Best Classification Threshold}} in {{Imbalanced Classification}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.bdr.2015.12.001},
  url = {https://reader.elsevier.com/reader/sd/pii/S2214579615000611?token=AAF7D56BE182B89A1112846AE8E4C97613D6CBB57FC7595C1A3FAB175BE927087F9CEB2329116D401F4D5A1240F563C5&originRegion=eu-west-1&originCreation=20230407092608},
  urldate = {2023-04-07},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/PKD5CNRW/Finding the Best Classification Threshold in Imbalanced Classification.pdf}
}

@article{fiskDistinguishingPhotosyntheticNonPhotosynthetic2019,
  title = {Distinguishing {{Photosynthetic}} and {{Non-Photosynthetic Vegetation}}: {{How Do Traditional Observations}} and {{Spectral Classification Compare}}?},
  shorttitle = {Distinguishing {{Photosynthetic}} and {{Non-Photosynthetic Vegetation}}},
  author = {Fisk, Claire and Clarke, Kenneth and Delean, Steven and Lewis, Megan},
  date = {2019-11-04},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {11},
  number = {21},
  pages = {2589},
  issn = {2072-4292},
  doi = {10.3390/rs11212589},
  url = {https://www.mdpi.com/2072-4292/11/21/2589},
  urldate = {2022-05-16},
  abstract = {Remotely sensed ground cover maps are routinely validated using field data collected by observers who classify ground cover into defined categories such as photosynthetic vegetation (PV), non-photosynthetic vegetation (NPV), bare soil (BS), and rock. There is an element of subjectivity to the classification of PV and NPV, and classifications may differ between observers. An alternative is to estimate ground cover based on in situ hyperspectral reflectance measurements (HRM). This study examines observer consistency when classifying vegetation samples of wheat (Triticum aestivum var. Gladius) covering the full range of photosynthetic activity, from completely senesced (0\% PV) to completely green (100\% PV), as photosynthetic or non-photosynthetic. We also examine how the classification of spectra of the same vegetation samples compares to the observer results. We collected HRM and photographs, over two months, to capture the transition of wheat leaves from 100\% PV to 100\% NPV. To simulate typical field methodology, observers viewed the photographs and classified each leaf as either PV or NPV, while spectral unmixing was used to decompose the HRM of the leaves into proportions of PV and NPV. The results showed that when a leaf was ≤25\% or ≥75\% PV observers tended to agree, and assign the leaf to the expected category. However, as leaves transitioned from PV to NPV (i.e., PV ≥ 25\% but ≤ 75\%) observers’ decisions differed more widely and their classifications showed little agreement with the spectral proportions of PV and NPV. This has significant implications for the reliability of data collected using binary methods in areas containing a significant proportion of vegetation in this intermediate range such as the over/underestimation of PV and NPV vegetation and how reliably this data can then be used to validate remotely sensed products.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/QQVWB2IH/Fisk et al. - 2019 - Distinguishing Photosynthetic and Non-Photosynthet.pdf}
}

@article{fontaineEuropeBridgeheadWorldwide2021,
  title = {Europe as a Bridgehead in the Worldwide Invasion History of Grapevine Downy Mildew, {{Plasmopara}} Viticola},
  author = {Fontaine, Michael C. and Labbé, Frédéric and Dussert, Yann and Delière, Laurent and Richart-Cervera, Sylvie and Giraud, Tatiana and Delmotte, François},
  date = {2021-05},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {31},
  number = {10},
  pages = {2155-2166.e4},
  issn = {09609822},
  doi = {10.1016/j.cub.2021.03.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982221003481},
  urldate = {2023-05-06},
  abstract = {Europe is the historical cradle of viticulture, but grapevines (Vitis vinifera) have been increasingly threatened by pathogens of American origin. The invasive oomycete Plasmopara viticola causes downy mildew, one of the most devastating grapevine diseases worldwide. Despite major economic consequences, its invasion history remains poorly understood. We analyzed a comprehensive dataset of  2,000 samples, collected from the most important wine-producing countries, using nuclear and mitochondrial gene sequences and microsatellite markers. Population genetic analyses revealed very low genetic diversity in invasive downy mildew populations worldwide and little evidence of admixture. All the invasive populations originated from only one of the five native North American lineages, the one parasitizing wild summer grape (V. aestivalis). An approximate Bayesian computation-random forest approach allowed inferring the worldwide invasion scenario of P. viticola. After an initial introduction into Europe, invasive European populations served as a secondary source of introduction into vineyards worldwide, including China, South Africa, and twice independently, Australia. Only the invasion of Argentina probably represents a tertiary introduction, from Australia. Our findings provide a striking example of a global pathogen invasion resulting from secondary dispersal of a successful invasive population. Our study will also help designing quarantine regulations and efficient breeding for resistance against grapevine downy mildew.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/DCZWKJTX/Fontaine et al_2021_Europe as a bridgehead in the worldwide invasion history of grapevine downy.pdf}
}

@article{furciRapidNondestructiveMethod2021,
  title = {A Rapid and Non-Destructive Method for Spatial–Temporal Quantification of Colonization by {{Pseudomonas}} Syringae Pv. Tomato {{DC3000}} in {{Arabidopsis}} and Tomato},
  author = {Furci, Leonardo and Pascual-Pardo, David and Ton, Jurriaan},
  date = {2021-12-13},
  journaltitle = {Plant Methods},
  shortjournal = {Plant Methods},
  volume = {17},
  number = {1},
  pages = {126},
  issn = {1746-4811},
  doi = {10.1186/s13007-021-00826-2},
  url = {https://doi.org/10.1186/s13007-021-00826-2},
  urldate = {2023-05-03},
  abstract = {The bacterial leaf pathogen Pseudomonas syringae pv tomato (Pst) is the most popular model pathogen for plant pathology research. Previous methods to study the plant-Pst interactions rely on destructive quantification of Pst colonisation, which can be labour- and time-consuming and does not allow for spatial–temporal monitoring of the bacterial colonisation. Here, we describe a rapid and non-destructive method to quantify and visualise spatial–temporal colonisation by Pst in intact leaves of Arabidopsis and tomato.},
  keywords = {Arabidopsis,Bioluminescence,Non-destructive assay,Pseudomonas syringae pv. tomato,Spatial–temporal pathogen colonisation,Tomato},
  file = {/Users/mavi/Zotero/storage/95K5VU58/Furci et al_2021_A rapid and non-destructive method for spatial–temporal quantification of.pdf;/Users/mavi/Zotero/storage/4CYGYMHD/s13007-021-00826-2.html}
}

@article{garzaQuantifyingCitrusTree2020,
  title = {Quantifying {{Citrus Tree Health Using True Color UAV Images}}},
  author = {Garza, Blanca N. and Ancona, Veronica and Enciso, Juan and Perotto-Baldivieso, Humberto L. and Kunta, Madhurababu and Simpson, Catherine},
  date = {2020-01-03},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {12},
  number = {1},
  pages = {170},
  issn = {2072-4292},
  doi = {10.3390/rs12010170},
  url = {https://www.mdpi.com/2072-4292/12/1/170},
  urldate = {2022-05-16},
  abstract = {Huanglongbing (HLB) and Phytophthora foot and root rot are diseases that affect citrus production and profitability. The symptoms and physiological changes associated with these diseases are diagnosed through expensive and time-consuming field measurements. Unmanned aerial vehicles (UAVs) using red/green/blue (RGB, true color) imaging, may be an economic alternative to diagnose diseases. A methodology using a UAV with a RGB camera was developed to assess citrus health. The UAV was flown in April 2018 on a grapefruit field infected with HLB and foot rot. Ten trees were selected for each of the following disease classifications: (HLB-, foot rot–), (HLB+, foot rot–), (HLB-, foot rot+) (HLB+, foot rot+). Triangular greenness index (TGI) images were correlated with field measurements such as tree nutritional status, leaf area, SPAD (leaf greenness), foot rot disease severity and HLB. It was found that 61\% of the TGI differences could be explained by Na, Fe, foot rot, Ca, and K. This study shows that diseased citrus trees can be monitored using UAVs equipped with RGB cameras, and that TGI can be used to explain subtle differences in tree health caused by multiple diseases.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/PTUJY8YY/Garza et al. - 2020 - Quantifying Citrus Tree Health Using True Color UA.pdf}
}

@article{geethaPlantLeafDisease2020,
  title = {Plant {{Leaf Disease Classification}} and {{Detection System Using Machine Learning}}},
  author = {Geetha, G. and Samundeswari, S. and Saranya, G. and Meenakshi, K. and Nithya, M.},
  date = {2020-12-01},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {1712},
  number = {1},
  pages = {012012},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1712/1/012012},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/1712/1/012012},
  urldate = {2022-05-16},
  abstract = {In a developing country like India agriculture plays a noteworthy role. Agricultural intervention in the livelihood of rural India indulges by about 58\%. Among the agricultural products, tomato is one of the most used crops. Thus, preventing significant loss in quantity and yield of tomato is majorly dependent on recognition and classification of diseases a tomato plant might possess. Latest and fostering technologies like Image processing is used to rectify such issues using different types of techniques and algorithms. Initially, the leaves of a tomato plant get affected, when plant develops a particular type of disease. In this project, four consecutive stages are used to discover the type of disease. The four stages include preprocessing, leaf segmentation, feature extraction and classification. To remove the noise we are doing the pre-processing and to part the affected or damages area of the leaf, image segmentation is used. The k-nearest neighbors (KNN) algorithm, which is a guided, supervised and advance machine learning algorithm, is implemented to find solutions for both the problems related to classification and regression. During the terminal stage, user is recommended with the treatment. Mostly live plants are adversely affected by the diseases. This paper imparts representation of leaf disease detection employing image processing that can identify drawbacks in tomato plant from images, based on color, bound and texture to give the brisk and reliable results to the farmer.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/XIM6A4FK/Geetha et al. - 2020 - Plant Leaf Disease Classification and Detection Sy.pdf}
}

@article{gehanPlantCVV2Image2017,
  title = {{{PlantCV}} v2: {{Image}} Analysis Software for High-Throughput Plant Phenotyping},
  author = {family=Gehan, given=MA, given-i=MA and Fahlgren, N and Abbasi, A and family=Berry, given=JC, given-i=JC and family=Callen, given=ST, given-i=ST and Chavez, L and family=Doust, given=AN, given-i=AN and family=Feldman, given=MJ, given-i=MJ and family=Gilbert, given=KB, given-i=KB and family=Hodge, given=JG, given-i=JG and family=Hoyer, given=JS, given-i=JS and Lin, A and Liu, S and Lizárraga, C and Lorence, A and Miller, M and Platon, E and Tessman, M and Sax, T},
  date = {2017},
  journaltitle = {PeerJ},
  number = {4088},
  doi = {10.7717/peerj.4088},
  url = {https://peerj.com/articles/4088/},
  keywords = {read}
}

@article{ghosalExplainableDeepMachine2018,
  title = {An Explainable Deep Machine Vision Framework for Plant Stress Phenotyping},
  author = {Ghosal, Sambuddha and Blystone, David and Singh, Asheesh K. and Ganapathysubramanian, Baskar and Singh, Arti and Sarkar, Soumik},
  date = {2018-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {18},
  pages = {4613--4618},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1716999115},
  url = {https://pnas.org/doi/full/10.1073/pnas.1716999115},
  urldate = {2022-10-25},
  abstract = {Significance             Plant stress identification based on visual symptoms has predominately remained a manual exercise performed by trained pathologists, primarily due to the occurrence of confounding symptoms. However, the manual rating process is tedious, is time-consuming, and suffers from inter- and intrarater variabilities. Our work resolves such issues via the concept of explainable deep machine learning to automate the process of plant stress identification, classification, and quantification. We construct a very accurate model that can not only deliver trained pathologist-level performance but can also explain which visual symptoms are used to make predictions. We demonstrate that our method is applicable to a large variety of biotic and abiotic stresses and is transferable to other imaging conditions and plants.           ,                             Current approaches for accurate identification, classification, and quantification of biotic and abiotic stresses in crop research and production are predominantly visual and require specialized training. However, such techniques are hindered by subjectivity resulting from inter- and intrarater cognitive variability. This translates to erroneous decisions and a significant waste of resources. Here, we demonstrate a machine learning framework’s ability to identify and classify a diverse set of foliar stresses in soybean [               Glycine max               (L.) Merr.] with remarkable accuracy. We also present an explanation mechanism, using the top-K high-resolution feature maps that isolate the visual symptoms used to make predictions. This unsupervised identification of visual symptoms provides a quantitative measure of stress severity, allowing for identification (type of foliar stress), classification (low, medium, or high stress), and quantification (stress severity) in a single framework without detailed symptom annotation by experts. We reliably identified and classified several biotic (bacterial and fungal diseases) and abiotic (chemical injury and nutrient deficiency) stresses by learning from over 25,000 images. The learned model is robust to input image perturbations, demonstrating viability for high-throughput deployment. We also noticed that the learned model appears to be agnostic to species, seemingly demonstrating an ability of transfer learning. The availability of an explainable model that can consistently, rapidly, and accurately identify and quantify foliar stresses would have significant implications in scientific research, plant breeding, and crop production. The trained model could be deployed in mobile platforms (e.g., unmanned air vehicles and automated ground scouts) for rapid, large-scale scouting or as a mobile application for real-time detection of stress by farmers and researchers.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/RPAFWQJC/Ghosal et al. - 2018 - An explainable deep machine vision framework for p.pdf}
}

@article{gillComprehensiveReviewHigh2022,
  title = {A {{Comprehensive Review}} of {{High Throughput Phenotyping}} and {{Machine Learning}} for {{Plant Stress Phenotyping}}},
  author = {Gill, Taqdeer and Gill, Simranveer K. and Saini, Dinesh K. and Chopra, Yuvraj and family=Koff, given=Jason P., prefix=de, useprefix=true and Sandhu, Karansher S.},
  date = {2022-06},
  journaltitle = {Phenomics (Cham, Switzerland)},
  shortjournal = {Phenomics},
  volume = {2},
  number = {3},
  eprint = {36939773},
  eprinttype = {pmid},
  pages = {156--183},
  issn = {2730-5848},
  doi = {10.1007/s43657-022-00048-z},
  abstract = {During the last decade, there has been rapid adoption of ground and aerial platforms with multiple sensors for phenotyping various biotic and abiotic stresses throughout the developmental stages of the crop plant. High throughput phenotyping (HTP) involves the application of these tools to phenotype the plants and can vary from ground-based imaging to aerial phenotyping to remote sensing. Adoption of these HTP tools has tried to reduce the phenotyping bottleneck in breeding programs and help to increase the pace of genetic gain. More specifically, several root phenotyping tools are discussed to study the plant's hidden half and an area long neglected. However, the use of these HTP technologies produces big data sets that impede the inference from those datasets. Machine learning and deep learning provide an alternative opportunity for the extraction of useful information for making conclusions. These are interdisciplinary approaches for data analysis using probability, statistics, classification, regression, decision theory, data visualization, and neural networks to relate information extracted with the phenotypes obtained. These techniques use feature extraction, identification, classification, and prediction criteria to identify pertinent data for use in plant breeding and pathology activities. This review focuses on the recent findings where machine learning and deep learning approaches have been used for plant stress phenotyping with data being collected using various HTP platforms. We have provided a comprehensive overview of different machine learning and deep learning tools available with their potential advantages and pitfalls. Overall, this review provides an avenue for studying various HTP platforms with particular emphasis on using the machine learning and deep learning tools for drawing legitimate conclusions. Finally, we propose the conceptual challenges being faced and provide insights on future perspectives for managing those issues.},
  langid = {english},
  pmcid = {PMC9590503},
  keywords = {Biotic and abiotic stresses,Deep learning,Ground-based imaging,High throughput phenotyping,Machine learning,Unmanned aerial vehicle}
}

@online{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  date = {2015-09-27},
  eprint = {1504.08083},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1504.08083},
  url = {http://arxiv.org/abs/1504.08083},
  urldate = {2023-05-08},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/WFZZR352/Girshick_2015_Fast R-CNN.pdf;/Users/mavi/Zotero/storage/QZUDIN7Z/1504.html}
}

@article{gosseauHeliaphenOutdoorHighThroughput2019,
  title = {Heliaphen, an {{Outdoor High-Throughput Phenotyping Platform}} for {{Genetic Studies}} and {{Crop Modeling}}},
  author = {Gosseau, Florie and Blanchet, Nicolas and Varès, Didier and Burger, Philippe and Campergue, Didier and Colombet, Céline and Gody, Louise and Liévin, Jean-François and Mangin, Brigitte and Tison, Gilles and Vincourt, Patrick and Casadebaig, Pierre and Langlade, Nicolas},
  date = {2019-01-16},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {9},
  pages = {1908},
  issn = {1664-462X},
  doi = {10.3389/fpls.2018.01908},
  url = {https://www.frontiersin.org/article/10.3389/fpls.2018.01908/full},
  urldate = {2022-05-16},
  abstract = {Heliaphen is an outdoor platform designed for high-throughput phenotyping. It allows the automated management of drought scenarios and monitoring of plants throughout their lifecycles. A robot moving between plants growing in 15-L pots monitors the plant water status and phenotypes the leaf or whole-plant morphology. From these measurements, we can compute more complex traits, such as leaf expansion (LE) or transpiration rate (TR) in response to water deficit. Here, we illustrate the capabilities of the platform with two practical cases in sunflower (Helianthus annuus): a genetic and genomic study of the response of yield-related traits to drought, and a modeling study using measured parameters as inputs for a crop simulation. For the genetic study, classical measurements of thousand-kernel weight (TKW) were performed on a biparental population under automatically managed drought stress and control conditions. These data were used for an association study, which identified five genetic markers of the TKW drought response. A complementary transcriptomic analysis identified candidate genes associated with these markers that were differentially expressed in the parental backgrounds in drought conditions. For the simulation study, we used a crop simulation model to predict the impact on crop yield of two traits measured on the platform (LE and TR) for a large number of environments. We conducted simulations in 42 contrasting locations across Europe using 21 years of climate data. We defined the pattern of abiotic stresses occurring at the continental scale and identified ideotypes (i.e., genotypes with specific trait values) that are more adapted to specific environment types. This study exemplifies how phenotyping platforms can assist the identification of the genetic architecture controlling complex response traits and facilitate the estimation of ecophysiological model parameters to define ideotypes adapted to different environmental conditions.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/MBPLF4SE/Gosseau et al. - 2019 - Heliaphen, an Outdoor High-Throughput Phenotyping .pdf}
}

@article{gozdziewicz-biechonskaProtectingEcosystemServices2022,
  title = {Protecting Ecosystem Services of Urban Agriculture against Land-Use Change Using Market-Based Instruments. {{A Polish}} Perspective},
  author = {Goździewicz-Biechońska, Justyna and Brzezińska-Rawa, Anna},
  date = {2022-09-01},
  journaltitle = {Land Use Policy},
  shortjournal = {Land Use Policy},
  volume = {120},
  pages = {106296},
  issn = {0264-8377},
  doi = {10.1016/j.landusepol.2022.106296},
  url = {https://www.sciencedirect.com/science/article/pii/S0264837722003234},
  urldate = {2022-09-29},
  abstract = {Urban agriculture is the collective name for a wide variety of farming activities that occur within a city’s boundaries or direct sphere of influence. A shared feature of urban agriculture types is that they are spatially limited,. Farming is generally not the primary function in the urban ecosystem; however, its spatial role should not be underestimated. Urban green spaces connected with urban agriculture are widely accepted as a nature-based solution for effectively addressing societal challenges related to urbanization. Competition for land between agricultural and urban land use is addressed mostly by regulatory command-and-control planning approaches. However, there is growing interest in the use of market-based instruments. Agricultural subsidies and legal protection against agricultural land conversion are primarily oriented towards rural areas, so the research regarding market-based instruments for agricultural land uses in cities is undeveloped. In Polish cities, urban agricultural areas comprise about 43.5 \% of total urban area, and 7.8 \% of individual farms in Poland are urban farms. The specific attitude towards ownership of land in Poland (a strong private-property ideology) influences (restricts) the implementation of market-based instruments. The article aims to present the potential for market-based instruments for urban agriculture and to identify barriers to the implementation of such solutions. The paper analyzes theoretical frameworks and practices for protecting agricultural land use through market-based instruments in Poland. We focus on two types of market-based instruments for ecosystem services of urban agriculture. The first encompasses the existing tools in Polish law and policy. We identified the following market-based instruments protecting urban agriculture: (1) charges and annual fees for excluding land from agricultural production, aimed at preventing agricultural land conversion in cities; and (2) land leasing charge of allotment gardens. We assess those instruments, their strengths and weaknesses and their effectiveness in protecting agricultural land use in cities. The second type are market-based instruments supporting urban agriculture cities that have potential for future implementation, We consider voluntary land readjustment and development agreement (currently very limited) to be the most promising and implementable. In this regard, we formulate de lege ferenda conclusions and recommendations.},
  langid = {english},
  keywords = {Allotment gardens,Ecosystem services,Land-use change,Market-based instruments,Poland,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/XK8U4HX6/Goździewicz-Biechońska and Brzezińska-Rawa - 2022 - Protecting ecosystem services of urban agriculture.pdf;/Users/mavi/Zotero/storage/A3K4WCTE/S0264837722003234.html}
}

@article{griraUnsupervisedSemisupervisedClustering,
  title = {Unsupervised and {{Semi-supervised Clustering}}: A {{Brief Survey}}},
  author = {Grira, Nizar and Crucianu, Michel and Boujemaa, Nozha},
  pages = {12},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/Users/mavi/Zotero/storage/T9LEFMQ6/Grira et al. - Unsupervised and Semi-supervised Clustering a Bri.pdf}
}

@article{gutierrezDeepLearningDifferentiation2021,
  title = {Deep Learning for the Differentiation of Downy Mildew and Spider Mite in Grapevine under Field Conditions},
  author = {Gutiérrez, Salvador and Hernández, Inés and Ceballos, Sara and Barrio, Ignacio and Díez-Navajas, Ana M. and Tardaguila, Javier},
  date = {2021-03},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {182},
  pages = {105991},
  issn = {01681699},
  doi = {10.1016/j.compag.2021.105991},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169921000090},
  urldate = {2022-06-03},
  abstract = {Diseases and pests cause serious damage in crop production, reducing yield and fruit quality. Their identification is often time-consuming and requires trained personnel. New sensing technologies and artificial intelligence could be used for automatic identification of disease and pest symptoms on grapevine in precision viticulture. The aim of this work was to apply deep learning modelling and computer vision for the detection and differ­ entiation of downy mildew and spider mite symptoms in grapevine leaves under field conditions. RGB images of grapevine canopy leaves with downy mildew symptoms, with spider mite symptoms and without symptoms were taken under field conditions in a commercial vineyard. The images were prepared using computer vision tech­ niques to increase disease visual features. Finally, deep learning was used to train a model capable of differ­ entiating leaf images of the three classes. An accuracy up to 0.94 (F1-score of 0.94) was obtained by classifying leaves with downy mildew, spider mite and without symptoms at the same time, using a hold-out validation. Additionally, accuracies between 0.89 and 0.91 (F1-scores between 0.89 and 0.91) were obtained in the binary classification of the disease and pest, obtaining the best results in differentiating downy mildew from spider mite symptoms. This high accuracy demonstrates the effectiveness of deep learning and computer vision techniques for the classification of grapevine leaf images taken under field conditions, automatically finding complex fea­ tures capable of differentiating leaves with spider mite symptoms, with downy mildew symptoms and without any. These results prove the potential of these non-invasive techniques in the detection and differentiation of pests and diseases in commercial crop production.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/3USI8BRJ/Gutiérrez et al. - 2021 - Deep learning for the differentiation of downy mil.pdf}
}

@online{hanVisionGNNImage2022,
  title = {Vision {{GNN}}: {{An Image}} Is {{Worth Graph}} of {{Nodes}}},
  shorttitle = {Vision {{GNN}}},
  author = {Han, Kai and Wang, Yunhe and Guo, Jianyuan and Tang, Yehui and Wu, Enhua},
  date = {2022-06-01},
  eprint = {2206.00272},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.00272},
  urldate = {2022-06-09},
  abstract = {Network architecture plays a key role in the deep learning-based computer vision system. The widely-used convolutional neural network and transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex objects. In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graphlevel feature for visual tasks. We first split the image to a number of patches which are viewed as nodes, and construct a graph by connecting the nearest neighbors. Based on the graph representation of images, we build our ViG model to transform and exchange information among all the nodes. ViG consists of two basic modules: Grapher module with graph convolution for aggregating and updating graph information, and FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our ViG architecture. We hope this pioneering study of GNN on general visual tasks will provide useful inspiration and experience for future research.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/G76EXIM8/Han et al. - 2022 - Vision GNN An Image is Worth Graph of Nodes.pdf}
}

@article{harakannanavarPlantLeafDisease2022,
  title = {Plant {{Leaf Disease Detection}} Using {{Computer Vision}} and {{Machine Learning Algorithms}}},
  author = {Harakannanavar, Sunil S. and Rudagi, Jayashri M. and Puranikmath, Veena I and Siddiqua, Ayesha and Pramodhini, R},
  date = {2022-04},
  journaltitle = {Global Transitions Proceedings},
  shortjournal = {Global Transitions Proceedings},
  pages = {S2666285X22000218},
  issn = {2666285X},
  doi = {10.1016/j.gltp.2022.03.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666285X22000218},
  urldate = {2022-05-16},
  abstract = {Agriculture provides food to all the human beings even in case of rapid increase in the population. It is recommended to predict the plant diseases at their early stage in the field of agriculture is essential to cater the food to the overall population. But it unfortunate to predict the diseases at the early stage of the crops. The idea behind the paper is to bring awareness amongst the farmers about the cutting-edge technologies to reduces diseases in plant leaf. Since tomato is merely available vegetable, the approaches of machine learning and image processing with an accurate algorithm is identified to detect the leaf diseases in the tomato plant. In this investigation, the samples of tomato leaves having disorders are considered. With these disorder samples of tomato leaves, the farmers will easily find the diseases based on the early symptoms. Firstly, the samples of tomato leaves are resized to 256x256 pixels and then Histogram Equalization is used to improve the quality of tomato samples. The K-means clustering is introduced for partitioning of dataspace into Voronoi cells. The boundary of leaf samples is extracted using contour tracing. The multiple descriptors viz., Discrete Wavelet Transform, Principal Component Analysis and Grey Level Co-occurrence Matrix are used to extract the informative features of the leaf samples. Finally, the extracted features are classified using machine learning approaches such as Support Vector Machine (SVM), Convolutional Neural Network (CNN) and K-Nearest Neighbor (K-NN). The accuracy of the proposed model is tested using SVM (88\%), K-NN (97\%) and CNN (99.6\%) on tomato disordered samples.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/DWKN8RKN/Harakannanavar et al. - 2022 - Plant Leaf Disease Detection using Computer Vision.pdf}
}

@article{harveyIntegerMultiplicationTime,
  title = {Integer Multiplication in Time {{O}}(n Log n)},
  author = {Harvey, David},
  doi = {10.4007/annals.2021.193.2.4},
  abstract = {We present an algorithm that computes the product of two n-bit integers in O(n log n) bit operations, thus confirming a conjecture of Sch¨onhage and Strassen from 1971. Our complexity analysis takes place in the multitape Turing machine model, with integers encoded in the usual binary representation. Central to the new algorithm is a novel “Gaussian resampling” technique that enables us to reduce the integer multiplication problem to a collection of multidimensional discrete Fourier transforms over the complex numbers, whose dimensions are all powers of two. These transforms may then be evaluated rapidly by means of Nussbaumer’s fast polynomial transforms.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/VHBADFWJ/Harvey - Integer multiplication in time O(n log n).pdf}
}

@article{hawesDoesUrbanAgriculture2022,
  title = {Does Urban Agriculture Lead to Gentrification?},
  author = {Hawes, Jason K and Gounaridis, Dimitrios and Newell, Joshua P},
  date = {2022-09-01},
  journaltitle = {Landscape and Urban Planning},
  shortjournal = {Landscape and Urban Planning},
  volume = {225},
  pages = {104447},
  issn = {0169-2046},
  doi = {10.1016/j.landurbplan.2022.104447},
  url = {https://www.sciencedirect.com/science/article/pii/S0169204622000962},
  urldate = {2022-09-29},
  abstract = {Urban agriculture, experiencing a resurgence across the Global North, features prominently in food system sustainability and urban resilience discourse, planning, and policy. Research, however, indicates that racialized gentrification tends to accompany urban agriculture, similar to a phenomenon documented with other green space. This study used remote sensing to map home (N~=~478) and community (N~=~130) gardens across Detroit, an emblematic legacy city undergoing significant redevelopment. Despite being a city in which seventy-eight percent of the residents are Black, spatial regression revealed that gardens in Detroit are actually more prevalent in non-Black-neighborhoods. Community gardens predominate in neighborhoods where residents are younger, wealthier, and college-educated, while home gardens are more numerous in areas with high rates of home ownership. Modeling also indicated that gardens are in areas with limited access to fresh produce. Contrary to the literature, we did not find a correlation between the presence of gardens and potential gentrification. Gardens, however, are consistently more prevalent in neighborhoods that have stabilized after experiencing high rates of vacancy, foreclosure, and housing demolition. These results have three important implications. First, redevelopment processes in legacy cities such as Detroit, through urban agriculture and other green infrastructure, are likely to lead to garden distributions different than those found in cities with more typical development trajectories. Second, the research calls into question generalized assumptions that expanding green space inevitably leads to gentrification, necessitating deeper investigation of these dynamics in diverse urban settings. And finally, racialized narratives around gardens and redevelopment risk undermining long-standing connections between Detroit’s gardens and environmental justice.},
  langid = {english},
  keywords = {Detroit,Environmental justice,Gentrification,Income,Race,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/DDMTXV7R/Hawes et al. - 2022 - Does urban agriculture lead to gentrification.pdf;/Users/mavi/Zotero/storage/YYZQJEKI/S0169204622000962.html}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2023-04-28},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read},
  file = {/Users/mavi/Zotero/storage/IVNUEPS3/He et al_2015_Deep Residual Learning for Image Recognition.pdf;/Users/mavi/Zotero/storage/PAEVKBP5/1512.html}
}

@online{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  date = {2021-12-19},
  eprint = {2111.06377},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.06377},
  url = {http://arxiv.org/abs/2111.06377},
  urldate = {2022-12-31},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/Q6CWHC9I/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf;/Users/mavi/Zotero/storage/L4IFAY84/2111.html}
}

@article{hernandezAssessmentDownyMildew2022,
  title = {Assessment of Downy Mildew in Grapevine Using Computer Vision and Fuzzy Logic. {{Development}} and Validation of a New Method},
  author = {Hernández, Inés and Gutiérrez, Salvador and Ceballos, Sara and Palacios, Fernando and Toffolatti, Silvia Laura and Maddalena, Giuliana and Diago, María Paz and Tardaguila, Javier},
  date = {2022-07-01},
  journaltitle = {OENO One},
  shortjournal = {OENO One},
  volume = {56},
  number = {3},
  pages = {41--53},
  issn = {2494-1271},
  doi = {10.20870/oeno-one.2022.56.3.5359},
  url = {https://oeno-one.eu/article/view/5359},
  urldate = {2022-07-18},
  abstract = {Downy mildew is a major disease of grapevine. Conventional methods for assessing crop diseases are time-consuming and require trained personnel. This work aimed to develop and validate a new method to automatically estimate the severity of downy mildew in grapevine leaves using fuzzy logic and computer vision techniques. Leaf discs of two grapevine varieties were inoculated with Plasmopara viticola and subsequently, RGB images were acquired under indoor conditions. Computer vision techniques were applied for leaf disc location in Petri dishes, image pre-processing and segmentation of pre-processed disc images to separate the pixels representing downy mildew sporulation from the rest of the leaf. Fuzzy logic was applied to improve the segmentation of disc images, rating pixels with a degree of infection according to the intensity of sporulation. To validate the new method, the downy mildew severity was visually evaluated by eleven experts and averaged score was used as the reference value. A coefficient of determination (R2) of 0.87 and a root mean squared error (RMSE) of 7.61 \% was observed between the downy mildew severity obtained by the new method and the visual assessment values. Classification of the severity of the infection into three levels was also attempted, achieving an accuracy of 86 \% and an F1 score of 0.78. These results indicate that computer vision and fuzzy logic can be used to automatically estimate the severity of downy mildew in grapevine leaves. A new method has been developed and validated to assess the severity of downy mildew in grapevine. The new method can be adapted to assess the severity of other diseases and crops in agriculture.},
  langid = {english},
  keywords = {\emph{Plasmopara viticola},read},
  file = {/Users/mavi/Zotero/storage/C3Z6BWMW/Hernández et al_2022_Assessment of downy mildew in grapevine using computer vision and fuzzy logic.pdf}
}

@article{herrmannLeafCanopyLevel2018,
  title = {Leaf and {{Canopy Level Detection}} of {{Fusarium Virguliforme}} ({{Sudden Death Syndrome}}) in {{Soybean}}},
  author = {Herrmann, Ittai and Vosberg, Steven and Ravindran, Prabu and Singh, Aditya and Chang, Hao-Xun and Chilvers, Martin and Conley, Shawn and Townsend, Philip},
  date = {2018-03-09},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {10},
  number = {3},
  pages = {426},
  issn = {2072-4292},
  doi = {10.3390/rs10030426},
  url = {http://www.mdpi.com/2072-4292/10/3/426},
  urldate = {2022-05-16},
  abstract = {Pre-visual detection of crop disease is critical for food security. Field-based spectroscopic remote sensing offers a method to enable timely detection, but still requires appropriate instrumentation and testing. Soybean plants were spectrally measured throughout a growing season to assess the capacity of leaf and canopy level spectral measurements to detect non-visual foliage symptoms induced by Fusarium virguliforme (Fv, which causes sudden death syndrome). Canopy reflectance measurements were made using the Piccolo Doppio dual field-of-view, two-spectrometer (400 to 1630 nm) system on a tractor. Leaf level measurements were obtained, in different plots, using a handheld spectrometer (400 to 2500 nm). Partial least squares discriminant analysis (PLSDA) was applied to the spectroscopic data to discriminate between Fv-inoculated and control plants. Canopy and leaf spectral data allowed identification of Fv infection, prior to visual symptoms, with classification accuracy of 88\% and 91\% for calibration, 79\% and 87\% for cross-validation, and 82\% and 92\% for validation, respectively. Differences in wavelengths important to prediction by canopy vs. leaf data confirm that there are different bases for accurate predictions among methods. Partial least square regression (PLSR) was used on a late-stage canopy level data to predict soybean seed yield, with calibration, cross-validation and validation R2 values 0.71, 0.59 and 0.62 (p {$<$} 0.01), respectively, and validation root mean square error of 0.31 t·ha−1. Spectral data from the tractor mounted system are thus sensitive to the expression of Fv root infection at canopy scale prior to canopy symptoms, suggesting such systems may be effective for precision agricultural research and management.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/RFKNRP87/Herrmann et al. - 2018 - Leaf and Canopy Level Detection of Fusarium Virgul.pdf}
}

@inproceedings{higginsBetaVAELearningBasic2022,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  shorttitle = {Beta-{{VAE}}},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  date = {2022-07-21},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  urldate = {2022-12-25},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta {$>$} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/EDE74JKY/Higgins et al. - 2022 - beta-VAE Learning Basic Visual Concepts with a Co.pdf}
}

@incollection{hintonTransformingAutoEncoders2011,
  title = {Transforming {{Auto-Encoders}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2011},
  author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
  editor = {Honkela, Timo and Duch, Włodzisław and Girolami, Mark and Kaski, Samuel},
  date = {2011},
  volume = {6791},
  pages = {44--51},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-21735-7_6},
  url = {http://link.springer.com/10.1007/978-3-642-21735-7_6},
  urldate = {2022-05-25},
  abstract = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the handengineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.},
  isbn = {978-3-642-21734-0 978-3-642-21735-7},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/ZMCJI2M8/Hinton et al. - 2011 - Transforming Auto-Encoders.pdf}
}

@inproceedings{hosangLearningNonmaximumSuppression2017,
  title = {Learning {{Non-maximum Suppression}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
  date = {2017-07},
  pages = {6469--6477},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.685},
  url = {http://ieeexplore.ieee.org/document/8100168/},
  urldate = {2022-06-03},
  abstract = {Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and — being based on greedy clustering with a fixed distance threshold — forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/IJ6KEHWR/Hosang et al. - 2017 - Learning Non-maximum Suppression.pdf}
}

@article{hosangWhatMakesEffective2016,
  title = {What {{Makes}} for {{Effective Detection Proposals}}?},
  author = {Hosang, Jan and Benenson, Rodrigo and Dollar, Piotr and Schiele, Bernt},
  date = {2016-04-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {38},
  number = {4},
  pages = {814--830},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2015.2465908},
  url = {http://ieeexplore.ieee.org/document/7182356/},
  urldate = {2022-05-20},
  abstract = {Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL and ImageNet, and impact on DPM and R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detector performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/5H6T7R3S/Hosang et al. - 2016 - What Makes for Effective Detection Proposals.pdf}
}

@online{houDeepFeatureConsistent2016,
  title = {Deep {{Feature Consistent Variational Autoencoder}}},
  author = {Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},
  date = {2016-10-02},
  eprint = {1610.00291},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1610.00291},
  url = {http://arxiv.org/abs/1610.00291},
  urldate = {2022-12-25},
  abstract = {We present a novel method for constructing Variational Autoencoder (VAE). Instead of using pixel-by-pixel loss, we enforce deep feature consistency between the input and the output of a VAE, which ensures the VAE's output to preserve the spatial correlation characteristics of the input, thus leading the output to have a more natural visual appearance and better perceptual quality. Based on recent deep learning works such as style transfer, we employ a pre-trained deep convolutional neural network (CNN) and use its hidden features to define a feature perceptual loss for VAE training. Evaluated on the CelebA face dataset, we show that our model produces better results than other methods in the literature. We also show that our method can produce latent vectors that can capture the semantic information of face expressions and can be used to achieve state-of-the-art performance in facial attribute prediction.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/MVGEC3WM/Hou et al. - 2016 - Deep Feature Consistent Variational Autoencoder.pdf;/Users/mavi/Zotero/storage/TEL6JV73/1610.html}
}

@online{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017-04-16},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.04861},
  url = {http://arxiv.org/abs/1704.04861},
  urldate = {2023-05-15},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/NSDKIPWH/Howard et al_2017_MobileNets.pdf;/Users/mavi/Zotero/storage/6AZXRSRR/1704.html}
}

@article{hutherARADEEPOPSISAutomatedWorkflow2020,
  title = {{{ARADEEPOPSIS}}, an {{Automated Workflow}} for {{Top-View Plant Phenomics}} Using {{Semantic Segmentation}} of {{Leaf States}}},
  author = {Hüther, Patrick and Schandry, Niklas and Jandrasits, Katharina and Bezrukov, Ilja and Becker, Claude},
  date = {2020-12},
  journaltitle = {The Plant Cell},
  shortjournal = {Plant Cell},
  volume = {32},
  number = {12},
  pages = {3674--3688},
  issn = {1040-4651, 1532-298X},
  doi = {10.1105/tpc.20.00318},
  url = {https://academic.oup.com/plcell/article/32/12/3674-3688/6118590},
  urldate = {2022-05-16},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/G9IGHELG/Hüther et al. - 2020 - ARADEEPOPSIS, an Automated Workflow for Top-View P.pdf}
}

@article{iqbalAutomatedDetectionClassification2018,
  title = {An Automated Detection and Classification of Citrus Plant Diseases Using Image Processing Techniques: {{A}} Review},
  shorttitle = {An Automated Detection and Classification of Citrus Plant Diseases Using Image Processing Techniques},
  author = {Iqbal, Zahid and Khan, Muhammad Attique and Sharif, Muhammad and Shah, Jamal Hussain and family=Rehman, given=Muhammad Habib, prefix=ur, useprefix=true and Javed, Kashif},
  date = {2018-10},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {153},
  pages = {12--32},
  issn = {01681699},
  doi = {10.1016/j.compag.2018.07.032},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169917311870},
  urldate = {2022-05-19},
  abstract = {The citrus plants such as lemons, mandarins, oranges, tangerines, grapefruits, and limes are commonly grown fruits all over the world. The citrus producing companies create a large amount of waste every year whereby 50\% of citrus peel is destroyed every year due to different plant diseases. This paper presents a survey on the different methods relevant to citrus plants leaves diseases detection and the classification. The article presents a detailed taxonomy of citrus leaf diseases. Initially, the challenges of each step are discussed in detail, which affects the detection and classification accuracy. In addition, a thorough literature review of automated disease detection and classification methods is presented. To this end, we study different image preprocessing, segmentation, feature extraction, features selection, and classification methods. In addition, also discuss the importance of features extraction and deep learning methods. The survey presents the detailed discussion on studies, outlines their strengths and limitations, and uncovers further research issues. The survey results reveal that the adoption of automated detection and classification methods for citrus plants diseases is still in its infancy. Hence new tools are needed to fully automate the detection and classification processes.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/QNCKN2MT/Iqbal et al. - 2018 - An automated detection and classification of citru.pdf}
}

@article{iqbalPlantDefenseResponses2021,
  title = {Plant {{Defense Responses}} to {{Biotic Stress}} and {{Its Interplay With Fluctuating Dark}}/{{Light Conditions}}},
  author = {Iqbal, Zahra and Iqbal, Mohammed Shariq and Hashem, Abeer and Abd\_Allah, Elsayed Fathi and Ansari, Mohammad Israil},
  date = {2021},
  journaltitle = {Frontiers in Plant Science},
  volume = {12},
  issn = {1664-462X},
  doi = {10.3389/fpls.2021.631810},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2021.631810},
  urldate = {2022-10-20},
  abstract = {Plants are subjected to a plethora of environmental cues that cause extreme losses to crop productivity. Due to fluctuating environmental conditions, plants encounter difficulties in attaining full genetic potential for growth and reproduction. One such environmental condition is the recurrent attack on plants by herbivores and microbial pathogens. To surmount such attacks, plants have developed a complex array of defense mechanisms. The defense mechanism can be either preformed, where toxic secondary metabolites are stored; or can be inducible, where defense is activated upon detection of an attack. Plants sense biotic stress conditions, activate the regulatory or transcriptional machinery, and eventually generate an appropriate response. Plant defense against pathogen attack is well understood, but the interplay and impact of different signals to generate defense responses against biotic stress still remain elusive. The impact of light and dark signals on biotic stress response is one such area to comprehend. Light and dark alterations not only regulate defense mechanisms impacting plant development and biochemistry but also bestow resistance against invading pathogens. The interaction between plant defense and dark/light environment activates a signaling cascade. This signaling cascade acts as a connecting link between perception of biotic stress, dark/light environment, and generation of an appropriate physiological or biochemical response. The present review highlights molecular responses arising from dark/light fluctuations vis-à-vis elicitation of defense mechanisms in plants.},
  file = {/Users/mavi/Zotero/storage/5M65GALH/Iqbal et al. - 2021 - Plant Defense Responses to Biotic Stress and Its I.pdf}
}

@article{isenseeNnUNetSelfconfiguringMethod2021,
  title = {{{nnU-Net}}: A Self-Configuring Method for Deep Learning-Based Biomedical Image Segmentation},
  shorttitle = {{{nnU-Net}}},
  author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and Maier-Hein, Klaus H.},
  date = {2021-02},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {18},
  number = {2},
  pages = {203--211},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-020-01008-z},
  url = {https://www.nature.com/articles/s41592-020-01008-z},
  urldate = {2022-10-05},
  abstract = {Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.},
  issue = {2},
  langid = {english},
  keywords = {Image processing,Translational research},
  file = {/Users/mavi/Zotero/storage/7HSDAH97/Isensee et al_2021_nnU-Net.pdf;/Users/mavi/Zotero/storage/ZFKDW7NT/s41592-020-01008-z.html}
}

@article{itakuraAutomaticLeafSegmentation2018,
  title = {Automatic {{Leaf Segmentation}} for {{Estimating Leaf Area}} and {{Leaf Inclination Angle}} in {{3D Plant Images}}},
  author = {Itakura, Kenta and Hosoi, Fumiki},
  date = {2018-10-22},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {18},
  number = {10},
  pages = {3576},
  issn = {1424-8220},
  doi = {10.3390/s18103576},
  url = {http://www.mdpi.com/1424-8220/18/10/3576},
  urldate = {2022-05-16},
  abstract = {Automatic and efficient plant monitoring offers accurate plant management. Construction of three-dimensional (3D) models of plants and acquisition of their spatial information is an effective method for obtaining plant structural parameters. Here, 3D images of leaves constructed with multiple scenes taken from different positions were segmented automatically for the automatic retrieval of leaf areas and inclination angles. First, for the initial segmentation, leave images were viewed from the top, then leaves in the top-view images were segmented using distance transform and the watershed algorithm. Next, the images of leaves after the initial segmentation were reduced by 90\%, and the seed regions for each leaf were produced. The seed region was re-projected onto the 3D images, and each leaf was segmented by expanding the seed region with the 3D information. After leaf segmentation, the leaf area of each leaf and its inclination angle were estimated accurately via a voxel-based calculation. As a result, leaf area and leaf inclination angle were estimated accurately after automatic leaf segmentation. This method for automatic plant structure analysis allows accurate and efficient plant breeding and growth management.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/4S7RVBKF/Itakura and Hosoi - 2018 - Automatic Leaf Segmentation for Estimating Leaf Ar.pdf}
}

@online{jangCategoricalReparameterizationGumbelSoftmax2017,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  date = {2017-08-05},
  eprint = {1611.01144},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1611.01144},
  url = {http://arxiv.org/abs/1611.01144},
  urldate = {2022-12-25},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/TSFWYJYG/Jang et al. - 2017 - Categorical Reparameterization with Gumbel-Softmax.pdf;/Users/mavi/Zotero/storage/8KATY5Z7/1611.html}
}

@article{jiangConvolutionalNeuralNetworks2020,
  title = {Convolutional {{Neural Networks}} for {{Image-Based High-Throughput Plant Phenotyping}}: {{A Review}}},
  shorttitle = {Convolutional {{Neural Networks}} for {{Image-Based High-Throughput Plant Phenotyping}}},
  author = {Jiang, Yu and Li, Changying},
  date = {2020-04-09},
  journaltitle = {Plant Phenomics},
  shortjournal = {Plant Phenomics},
  volume = {2020},
  pages = {1--22},
  issn = {2643-6515},
  doi = {10.34133/2020/4152816},
  url = {https://spj.sciencemag.org/journals/plantphenomics/2020/4152816/},
  urldate = {2022-05-25},
  abstract = {Plant phenotyping has been recognized as a bottleneck for improving the efficiency of breeding programs, understanding plant-environment interactions, and managing agricultural systems. In the past five years, imaging approaches have shown great potential for high-throughput plant phenotyping, resulting in more attention paid to imaging-based plant phenotyping. With this increased amount of image data, it has become urgent to develop robust analytical tools that can extract phenotypic traits accurately and rapidly. The goal of this review is to provide a comprehensive overview of the latest studies using deep convolutional neural networks (CNNs) in plant phenotyping applications. We specifically review the use of various CNN architecture for plant stress evaluation, plant development, and postharvest quality assessment. We systematically organize the studies based on technical developments resulting from imaging classification, object detection, and image segmentation, thereby identifying state-of-the-art solutions for certain phenotyping applications. Finally, we provide several directions for future research in the use of CNN architecture for plant phenotyping purposes.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/49WPTRM4/Jiang and Li - 2020 - Convolutional Neural Networks for Image-Based High.pdf}
}

@article{jiangRealTimeDetectionApple2019,
  title = {Real-{{Time Detection}} of {{Apple Leaf Diseases Using Deep Learning Approach Based}} on {{Improved Convolutional Neural Networks}}},
  author = {Jiang, Peng and Chen, Yuehan and Liu, Bin and He, Dongjian and Liang, Chunquan},
  date = {2019},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {7},
  pages = {59069--59080},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2914929},
  url = {https://ieeexplore.ieee.org/document/8706936/},
  urldate = {2022-07-04},
  abstract = {Alternaria leaf spot, Brown spot, Mosaic, Grey spot, and Rust are five common types of apple leaf diseases that severely affect apple yield. However, the existing research lacks an accurate and fast detector of apple diseases for ensuring the healthy development of the apple industry. This paper proposes a deep learning approach that is based on improved convolutional neural networks (CNNs) for the real-time detection of apple leaf diseases. In this paper, the apple leaf disease dataset (ALDD), which is composed of laboratory images and complex images under real field conditions, is first constructed via data augmentation and image annotation technologies. Based on this, a new apple leaf disease detection model that uses deep-CNNs is proposed by introducing the GoogLeNet Inception structure and Rainbow concatenation. Finally, under the hold-out testing dataset, using a dataset of 26,377 images of diseased apple leaves, the proposed INAR-SSD (SSD with Inception module and Rainbow concatenation) model is trained to detect these five common apple leaf diseases. The experimental results show that the INAR-SSD model realizes a detection performance of 78.80\% mAP on ALDD, with a high-detection speed of 23.13 FPS. The results demonstrate that the novel INAR-SSD model provides a high-performance solution for the early diagnosis of apple leaf diseases that can perform real-time detection of these diseases with higher accuracy and faster detection speed than previous methods.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/FMR9ZJNW/Jiang et al. - 2019 - Real-Time Detection of Apple Leaf Diseases Using D.pdf}
}

@article{jiAutomaticGrapeLeaf2020,
  title = {Automatic Grape Leaf Diseases Identification via {{UnitedModel}} Based on Multiple Convolutional Neural Networks},
  author = {Ji, Miaomiao and Zhang, Lei and Wu, Qiufeng},
  date = {2020-09},
  journaltitle = {Information Processing in Agriculture},
  shortjournal = {Information Processing in Agriculture},
  volume = {7},
  number = {3},
  pages = {418--426},
  issn = {22143173},
  doi = {10.1016/j.inpa.2019.10.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2214317319301003},
  urldate = {2022-06-03},
  abstract = {Grape diseases are main factors causing serious grapes reduction. So it is urgent to develop an automatic identification method for grape leaf diseases. Deep learning techniques have recently achieved impressive successes in various computer vision problems, which inspires us to apply them to grape diseases identification task. In this paper, a united convolutional neural networks (CNNs) architecture based on an integrated method is proposed. The proposed CNNs architecture, i.e., UnitedModel is designed to distinguish leaves with common grape diseases i.e., black rot, esca and isariopsis leaf spot from healthy leaves. The combination of multiple CNNs enables the proposed UnitedModel to extract complementary discriminative features. Thus the representative ability of UnitedModel has been enhanced. The UnitedModel has been evaluated on the hold-out PlantVillage dataset and has been compared with several state-of-the-art CNN models. The experimental results have shown that UnitedModel achieves the best performance on various evaluation metrics. The UnitedModel achieves an average validation accuracy of 99.17\% and a test accuracy of 98.57\%, which can serve as a decision support tool to help farmers identify grape diseases.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/U4MWQHER/Ji et al. - 2020 - Automatic grape leaf diseases identification via U.pdf}
}

@article{juberyUsingMachineLearning2021,
  title = {Using {{Machine Learning}} to {{Develop}} a {{Fully Automated Soybean Nodule Acquisition Pipeline}} ({{SNAP}})},
  author = {Jubery, Talukder Zaki and Carley, Clayton N. and Singh, Arti and Sarkar, Soumik and Ganapathysubramanian, Baskar and Singh, Asheesh K.},
  date = {2021-07-28},
  journaltitle = {Plant Phenomics},
  shortjournal = {Plant Phenomics},
  volume = {2021},
  pages = {1--12},
  issn = {2643-6515},
  doi = {10.34133/2021/9834746},
  url = {https://spj.sciencemag.org/journals/plantphenomics/2021/9834746/},
  urldate = {2022-05-16},
  abstract = {Nodules form on plant roots through the symbiotic relationship between soybean (               Glycine max               L. Merr.) roots and bacteria (               Bradyrhizobium japonicum               ) and are an important structure where atmospheric nitrogen (N               2               ) is fixed into bioavailable ammonia (NH               3               ) for plant growth and development. Nodule quantification on soybean roots is a laborious and tedious task; therefore, assessment is frequently done on a numerical scale that allows for rapid phenotyping, but is less informative and suffers from subjectivity. We report the Soybean Nodule Acquisition Pipeline (SNAP) for nodule quantification that combines RetinaNet and UNet deep learning architectures for object (i.e., nodule) detection and segmentation. SNAP was built using data from 691 unique roots from diverse soybean genotypes, vegetative growth stages, and field locations and has a good model fit (                                                                        R                                                           2                                                     =                 0.99                              ). SNAP reduces the human labor and inconsistencies of counting nodules, while acquiring quantifiable traits related to nodule growth, location, and distribution on roots. The ability of SNAP to phenotype nodules on soybean roots at a higher throughput enables researchers to assess the genetic and environmental factors, and their interactions on nodulation from an early development stage. The application of SNAP in research and breeding pipelines may lead to more nitrogen use efficiency for soybean and other legume species cultivars, as well as enhanced insight into the plant-               Bradyrhizobium               relationship.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/DVB2KZCW/Jubery et al. - 2021 - Using Machine Learning to Develop a Fully Automate.pdf}
}

@article{k.hackenbergerBayesNotBayes2019,
  title = {Bayes or Not {{Bayes}}, Is This the Question?},
  author = {K. Hackenberger, Branimir},
  date = {2019-02},
  journaltitle = {Croatian Medical Journal},
  shortjournal = {Croat Med J},
  volume = {60},
  number = {1},
  eprint = {30825279},
  eprinttype = {pmid},
  pages = {50--52},
  issn = {0353-9504},
  doi = {10.3325/cmj.2019.60.50},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6406060/},
  urldate = {2022-10-17},
  pmcid = {PMC6406060},
  file = {/Users/mavi/Zotero/storage/SWUZ329A/K. Hackenberger - 2019 - Bayes or not Bayes, is this the question.pdf}
}

@article{kamilarisDeepLearningAgriculture2018,
  title = {Deep Learning in Agriculture: {{A}} Survey},
  shorttitle = {Deep Learning in Agriculture},
  author = {Kamilaris, Andreas and Prenafeta-Boldú, Francesc X.},
  date = {2018-04},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {147},
  pages = {70--90},
  issn = {01681699},
  doi = {10.1016/j.compag.2018.02.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169917308803},
  urldate = {2022-05-19},
  abstract = {Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/BXFUPKGH/Kamilaris and Prenafeta-Boldú - 2018 - Deep learning in agriculture A survey.pdf}
}

@article{katagiriArabidopsisThalianaPseudomonasSyringae2002,
  title = {The {{Arabidopsis Thaliana-Pseudomonas Syringae Interaction}}},
  author = {Katagiri, Fumiaki and Thilmony, Roger and He, Sheng Yang},
  date = {2002-03},
  journaltitle = {The Arabidopsis Book},
  shortjournal = {arbo.j},
  volume = {2002},
  number = {1},
  publisher = {{The American Society of Plant Biologists}},
  issn = {1543-8120},
  doi = {10.1199/tab.0039},
  url = {https://bioone.org/journals/the-arabidopsis-book/volume-2002/issue-1/tab.0039/The-Arabidopsis-Thaliana-Pseudomonas-Syringae-Interaction/10.1199/tab.0039.full},
  urldate = {2023-05-10},
  abstract = {The Arabidopsis Book contains comprehensive information about a broad range of topics in research on Arabidopsis thaliana and related species.},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/GILLJ67E/Katagiri et al_2002_The Arabidopsis Thaliana-Pseudomonas Syringae Interaction.pdf}
}

@article{kaurRecognitionLeafDisease2022,
  title = {Recognition of {{Leaf Disease Using Hybrid Convolutional Neural Network}} by {{Applying Feature Reduction}}},
  author = {Kaur, Prabhjot and Harnal, Shilpi and Tiwari, Rajeev and Upadhyay, Shuchi and Bhatia, Surbhi and Mashat, Arwa and Alabdali, Aliaa M.},
  date = {2022-01-12},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {22},
  number = {2},
  pages = {575},
  issn = {1424-8220},
  doi = {10.3390/s22020575},
  url = {https://www.mdpi.com/1424-8220/22/2/575},
  urldate = {2022-05-16},
  abstract = {Agriculture is crucial to the economic prosperity and development of India. Plant diseases can have a devastating influence towards food safety and a considerable loss in the production of agricultural products. Disease identification on the plant is essential for long-term agriculture sustainability. Manually monitoring plant diseases is difficult due to time limitations and the diversity of diseases. In the realm of agricultural inputs, automatic characterization of plant diseases is widely required. Based on performance out of all image-processing methods, is better suited for solving this task. This work investigates plant diseases in grapevines. Leaf blight, Black rot, stable, and Black measles are the four types of diseases found in grape plants. Several earlier research proposals using machine learning algorithms were created to detect one or two diseases in grape plant leaves; no one offers a complete detection of all four diseases. The photos are taken from the plant village dataset in order to use transfer learning to retrain the EfficientNet B7 deep architecture. Following the transfer learning, the collected features are down-sampled using a Logistic Regression technique. Finally, the most discriminant traits are identified with the highest constant accuracy of 98.7\% using state-of-the-art classifiers after 92 epochs. Based on the simulation findings, an appropriate classifier for this application is also suggested. The proposed technique’s effectiveness is confirmed by a fair comparison to existing procedures.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/UELD9EMF/Kaur et al. - 2022 - Recognition of Leaf Disease Using Hybrid Convoluti.pdf}
}

@article{kayaAnalysisTransferLearning2019,
  title = {Analysis of Transfer Learning for Deep Neural Network Based Plant Classification Models},
  author = {Kaya, Aydin and Keceli, Ali Seydi and Catal, Cagatay and Yalic, Hamdi Yalin and Temucin, Huseyin and Tekinerdogan, Bedir},
  date = {2019-03},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {158},
  pages = {20--29},
  issn = {01681699},
  doi = {10.1016/j.compag.2019.01.041},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169918315308},
  urldate = {2022-05-20},
  abstract = {Plant species classification is crucial for biodiversity protection and conservation. Manual classification is timeconsuming, expensive, and requires experienced experts who are often limited available. To cope with these issues, various machine learning algorithms have been proposed to support the automated classification of plant species. Among these machine learning algorithms, Deep Neural Networks (DNNs) have been applied to different data sets. DNNs have been however often applied in isolation and no effort has been made to reuse and transfer the knowledge of different applications of DNNs. Transfer learning in the context of machine learning implies the usage of the results of multiple applications of DNNs. In this article, the results of the effect of four different transfer learning models for deep neural network-based plant classification is investigated on four public datasets. Our experimental study demonstrates that transfer learning can provide important benefits for automated plant identification and can improve low-performance plant classification models.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/EVJ7ZSNR/Kaya et al. - 2019 - Analysis of transfer learning for deep neural netw.pdf}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2022-12-25},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/JMAMW9LA/Kingma_Welling_2022_Auto-Encoding Variational Bayes.pdf;/Users/mavi/Zotero/storage/F6248VJG/1312.html}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2022-12-21},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/QIADKS2I/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/Users/mavi/Zotero/storage/36SPUUFC/1906.html}
}

@article{kingsleyUrbanAgricultureNaturebased2021,
  title = {Urban Agriculture as a Nature-Based Solution to Address Socio-Ecological Challenges in {{Australian}} Cities},
  author = {Kingsley, Jonathan and Egerer, Monika and Nuttman, Sonia and Keniger, Lucy and Pettitt, Philip and Frantzeskaki, Niki and Gray, Tonia and Ossola, Alessandro and Lin, Brenda and Bailey, Aisling and Tracey, Danielle and Barron, Sara and Marsh, Pauline},
  date = {2021-05-01},
  journaltitle = {Urban Forestry \& Urban Greening},
  shortjournal = {Urban Forestry \& Urban Greening},
  volume = {60},
  pages = {127059},
  issn = {1618-8667},
  doi = {10.1016/j.ufug.2021.127059},
  url = {https://www.sciencedirect.com/science/article/pii/S1618866721000844},
  urldate = {2022-09-29},
  abstract = {Australia is currently grappling with a range of social and environmental challenges, many of which impact the way our public health system, and society more broadly, function. In this short communication paper we explore urban agriculture in Australia as a Nature-Based Solution (NBS) to address some of the ecological, social, economic and health challenges facing the continent. We argue that urban agriculture has the potential to mitigate the effects of climate change extremes while simultaneously providing multiple benefits such as improving wellbeing, people-nature connections, and food security. We present three exemplar case studies diverse in geography, context and governance from Queensland, Tasmania, and New South Wales exploring verge gardening, market gardening, and a community greening program respectively to highlight the benefits of urban agriculture as a NBS. We advocate that various forms of urban agriculture need to be researched and considered for their potential impacts and multiple benefits to be fully supported, governed, and understood in light of the social-ecological challenges Australian cities face.},
  langid = {english},
  keywords = {Australia,Nature-Based solution,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/NSS4BTBH/Kingsley et al. - 2021 - Urban agriculture as a nature-based solution to ad.pdf;/Users/mavi/Zotero/storage/K9GKK7H5/S1618866721000844.html}
}

@inproceedings{kirillovPanopticSegmentation2019,
  title = {Panoptic {{Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr},
  date = {2019-06},
  pages = {9396--9405},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00963},
  url = {https://ieeexplore.ieee.org/document/8953237/},
  urldate = {2022-05-20},
  abstract = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-todate results, please check the arXiv version of the paper: https://arxiv.org/abs/1801.00868.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/C4MV7YBT/Kirillov et al. - 2019 - Panoptic Segmentation.pdf}
}

@article{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
  date = {2023},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2304.02643},
  url = {https://arxiv.org/abs/2304.02643},
  urldate = {2023-07-03},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  version = {1},
  keywords = {\_tablet,Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/mavi/Zotero/storage/L8B9A9N4/Kirillov et al_2023_Segment Anything.pdf}
}

@book{knappBasicAlgebra2006,
  title = {Basic Algebra},
  author = {Knapp, Anthony W.},
  date = {2006},
  publisher = {{Birkhäuser}},
  location = {{Basel ; Boston}},
  isbn = {978-0-8176-3248-9},
  langid = {english},
  pagetotal = {717},
  keywords = {Algebra},
  annotation = {OCLC: ocm74862732},
  file = {/Users/mavi/Zotero/storage/24UI6IHL/Knapp - 2006 - Basic algebra.pdf}
}

@article{kochSiameseNeuralNetworks,
  title = {Siamese {{Neural Networks}} for {{One-shot Image Recognition}}},
  author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
  pages = {8},
  abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/N7BVYB6V/Koch et al. - Siamese Neural Networks for One-shot Image Recogni.pdf}
}

@article{koenigReligionCopingSerious2001,
  title = {Religion and Coping with Serious Medical Illness},
  author = {Koenig, H. G. and Larson, D. B. and Larson, S. S.},
  date = {2001-03},
  journaltitle = {The Annals of Pharmacotherapy},
  shortjournal = {Ann Pharmacother},
  volume = {35},
  number = {3},
  eprint = {11261534},
  eprinttype = {pmid},
  pages = {352--359},
  issn = {1060-0280},
  doi = {10.1345/aph.10215},
  abstract = {OBJECTIVE: To review and discuss some of the research published in the last several decades that has addressed the role that religion plays in helping patients cope with serious medical illness. DATA SOURCES: Although this is not a systematic review of the literature, it provides a sampling of the studies that have examined the relationship between religious involvement, coping with illness, and health outcomes. This sampling of studies reflects the findings of a much larger systematic review of research (MEDLINE, Current Contents, Psychlit, Soclit, HealthStar, Cancerlit, CINAHL, and others) during the past century that was recently completed by the authors. DATA EXTRACTION: Epidemiologic studies published in the English-language literature were reviewed and discussed. DATA SYNTHESIS: A number of well-designed cross-sectional and prospective studies have examined the relationship between religious beliefs and activities and adaptation to physical illness in patients with general medical conditions, neurologic disorders, heart disease, renal failure, AIDS, and a host of other physical disorders. This review demonstrates the widespread use of religion in coping with medical illness and provides circumstantial evidence for the possible benefits of this lifestyle factor. CONCLUSIONS: When people become physically ill, many rely heavily on religious beliefs and practices to relieve stress, retain a sense of control, and maintain hope and their sense of meaning and purpose in life. Religious involvement appears to enable the sick, particularly those with serious and disabling medical illness, to cope better and experience psychological growth from their negative health experiences, rather than be defeated or overcome by them.},
  langid = {english},
  keywords = {{Adaptation, Psychological},Aged,Health,Humans,Male,Mental Disorders,Religion and Psychology}
}

@online{kokkinosUberNetTrainingUniversal2016,
  title = {{{UberNet}}: {{Training}} a `{{Universal}}' {{Convolutional Neural Network}} for {{Low-}}, {{Mid-}}, and {{High-Level Vision}} Using {{Diverse Datasets}} and {{Limited Memory}}},
  shorttitle = {{{UberNet}}},
  author = {Kokkinos, Iasonas},
  date = {2016-09-07},
  eprint = {1609.02132},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1609.02132},
  urldate = {2022-06-03},
  abstract = {In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a ‘swiss knife’ for vision tasks; we call this architecture an UberNet to indicate its overarching nature.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/CLIBDS6D/Kokkinos - 2016 - UberNet Training a `Universal' Convolutional Neur.pdf}
}

@incollection{kolesnikovBigTransferBiT2020,
  title = {Big {{Transfer}} ({{BiT}}): {{General Visual Representation Learning}}},
  shorttitle = {Big {{Transfer}} ({{BiT}})},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  volume = {12350},
  pages = {491--507},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58558-7_29},
  url = {https://link.springer.com/10.1007/978-3-030-58558-7_29},
  urldate = {2022-05-31},
  abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes — from 1 example per class to 1 M total examples. BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\% on CIFAR-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8\% on ILSVRC-2012 with 10 examples per class, and 97.0\% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
  isbn = {978-3-030-58557-0 978-3-030-58558-7},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/FEE3AXS2/Kolesnikov et al. - 2020 - Big Transfer (BiT) General Visual Representation .pdf}
}

@online{kolouriSlicedWassersteinAutoencoderEmbarrassingly2018,
  title = {Sliced-{{Wasserstein Autoencoder}}: {{An Embarrassingly Simple Generative Model}}},
  shorttitle = {Sliced-{{Wasserstein Autoencoder}}},
  author = {Kolouri, Soheil and Pope, Phillip E. and Martin, Charles E. and Rohde, Gustavo K.},
  date = {2018-06-26},
  eprint = {1804.01947},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.01947},
  url = {http://arxiv.org/abs/1804.01947},
  urldate = {2022-12-25},
  abstract = {In this paper we study generative modeling via autoencoders while using the elegant geometric properties of the optimal transport (OT) problem and the Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE), which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution. In short, we regularize the autoencoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a predefined samplable distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an embarrassingly simple implementation.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/YGCAQ5UK/Kolouri et al. - 2018 - Sliced-Wasserstein Autoencoder An Embarrassingly .pdf;/Users/mavi/Zotero/storage/P6G7GLG4/1804.html}
}

@article{kosevPhenotypicMolecularCharacterization2017,
  title = {Phenotypic and Molecular Characterization of 18 {{Bulgarian}} Newly Bred Grapevine Varieties in Relation to Their Resistance to Downy Mildew},
  author = {Kosev, Kalin and Simeonov, Ilian and Ivanov, Miroslav and Nakov, Zdravko and Hvarleva, Tzvetanka},
  date = {2017-01-02},
  journaltitle = {Biotechnology \& Biotechnological Equipment},
  shortjournal = {Biotechnology \& Biotechnological Equipment},
  volume = {31},
  number = {1},
  pages = {68--74},
  issn = {1310-2818, 1314-3530},
  doi = {10.1080/13102818.2016.1259019},
  url = {https://www.tandfonline.com/doi/full/10.1080/13102818.2016.1259019},
  urldate = {2022-07-18},
  abstract = {Eighteen Bulgarian newly bred grapevine varieties obtained by interspecific crossing were analysed through microsatellite markers in order to determine their genetic identity as well as the presence of resistance-related alleles linked to Rpv3, Rpv10 and Rpv12 loci. The levels of resistance of the investigated cultivars to downy mildew were assessed by leaf disk assay and were scored according to OIV descriptor 452. Seven isolates of Plasmopara viticola collected in different regions of Bulgaria were characterized and discriminated with seven microsatellite markers and were used in the assays. Analysis of variance showed significant differences in resistance to downy mildew among the analysed varieties. A clear correlation between the presence of particular Rpvs and the level of resistance was determined. Five cultivars, ‘Droujba’, ‘Garant’ and ‘Plevenska rosa’ harbouring Rpv3 and Rpv12, ‘Kajlashki rubin’ harbouring Rpv12 and ‘Slava’, Rpv10 and Rpv3, showed high level of resistance with mean OIV452 values ranging from 6.5 to 8.25 over all seven P. viticola isolates. The level of resistance of the remaining 14 cultivars, which carry only Rpv3 inherited from cultivar Villard Blanc, was found to vary significantly among the isolates with mean OIV452 values between 1.75 and 4.8 over all used P. viticola isolates. The results obtained in this study favoured five genotypes remarkable for their high resistance to downy mildew and very good quality of grape and wine. These varieties represent a valuable material for pyramiding of resistance through marker-assisted selection.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/QRI8DN5N/Kosev et al. - 2017 - Phenotypic and molecular characterization of 18 Bu.pdf}
}

@article{kramerPlantingMolecularFunctions2015,
  title = {Planting Molecular Functions in an Ecological Context with {{Arabidopsis}} Thaliana},
  author = {Krämer, Ute},
  date = {2015-03-25},
  journaltitle = {eLife},
  volume = {4},
  pages = {e06100},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.06100},
  url = {https://doi.org/10.7554/eLife.06100},
  urldate = {2023-05-03},
  abstract = {The vascular plant Arabidopsis thaliana is a central genetic model and universal reference organism in plant and crop science. The successful integration of different fields of research in the study of A. thaliana has made a large contribution to our molecular understanding of key concepts in biology. The availability and active development of experimental tools and resources, in combination with the accessibility of a wealth of cumulatively acquired knowledge about this plant, support the most advanced systems biology approaches among all land plants. Research in molecular ecology and evolution has also brought the natural history of A. thaliana into the limelight. This article showcases our current knowledge of the natural history of A. thaliana from the perspective of the most closely related plant species, providing an evolutionary framework for interpreting novel findings and for developing new hypotheses based on our knowledge of this plant.},
  keywords = {Arabidopsis relative,natural history,the natural history of model organism},
  file = {/Users/mavi/Zotero/storage/V9AHTMCB/Krämer_2015_Planting molecular functions in an ecological context with Arabidopsis thaliana.pdf}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2022-10-22},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  keywords = {⛔ No DOI found},
  file = {/Users/mavi/Zotero/storage/FIV3NRC2/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2022-05-16},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/92A4PU9E/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{kulkarniPlantDiseaseDetection,
  title = {Plant {{Disease Detection Using Image Processing}} and {{Machine Learning}}},
  author = {Kulkarni, Pranesh and Karwande, Atharva and Kolhe, Tejas and Kamble, Soham and Wyawahare, Medha},
  pages = {13},
  abstract = {One of the important and tedious task in agricultural practices is detection of disease on crops. It requires huge time as well as skilled labor. This paper proposes a smart and efficient technique for detection of crop disease which uses computer vision and machine learning techniques. The proposed system is able to detect 20 different diseases of 5 common plants with 93\% accuracy.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/MJNXX42Q/Kulkarni et al. - Plant Disease Detection Using Image Processing and.pdf}
}

@online{kumarVariationalInferenceDisentangled2018,
  title = {Variational {{Inference}} of {{Disentangled Latent Concepts}} from {{Unlabeled Observations}}},
  author = {Kumar, Abhishek and Sattigeri, Prasanna and Balakrishnan, Avinash},
  date = {2018-12-27},
  eprint = {1711.00848},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1711.00848},
  url = {http://arxiv.org/abs/1711.00848},
  urldate = {2022-12-25},
  abstract = {Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We also propose a new disentanglement metric which is better aligned with the qualitative disentanglement observed in the decoder's output. We empirically observe significant improvement over existing methods in terms of both disentanglement and data likelihood (reconstruction quality).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/JSCBXAAH/Kumar et al. - 2018 - Variational Inference of Disentangled Latent Conce.pdf;/Users/mavi/Zotero/storage/J68T8IK4/1711.html}
}

@article{kwabenapatrickCapsuleNetworksSurvey2022,
  title = {Capsule {{Networks}} – {{A}} Survey},
  author = {Kwabena Patrick, Mensah and Felix Adekoya, Adebayo and Abra Mighty, Ayidzoe and Edward, Baagyire Y.},
  date = {2022-01},
  journaltitle = {Journal of King Saud University - Computer and Information Sciences},
  shortjournal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {34},
  number = {1},
  pages = {1295--1310},
  issn = {13191578},
  doi = {10.1016/j.jksuci.2019.09.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1319157819309322},
  urldate = {2022-05-25},
  abstract = {Modern day computer vision tasks requires efficient solution to problems such as image recognition, natural language processing, object detection, object segmentation and language translation. Symbolic Artificial Intelligence with its hard coding rules is incapable of solving these complex problems resulting in the introduction of Deep Learning (DL) models such as Recurrent Neural Networks and Convolutional Neural Networks (CNN). However, CNNs require lots of training data and are incapable of recognizing pose and deformation of objects leading to the introduction of Capsule Networks. Capsule Networks are the new sensation in Deep Learning. They have lived to this expectation as their performance in relation to the above problems has been better than Convolutional Neural Networks. Even with this promise in performance, lack of architectural knowledge and inner workings of Capsules serves as a hindrance for researchers to take full advantage of this breakthrough. In this paper, we provide a comprehensive review of the state of the art architectures, tools and methodologies in existing implementations of capsule networks. We highlight the successes, failures and opportunities for further research to serve as a motivation to researchers and industry players to exploit the full potential of this new field. The main contribution of this survey article is that it explains and summarizes significant current state of the art Capsule Network architectures and implementations.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/FKXN4IH7/Kwabena Patrick et al. - 2022 - Capsule Networks – A survey.pdf}
}

@online{lanchantinGeneralMultilabelImage2020,
  title = {General {{Multi-label Image Classification}} with {{Transformers}}},
  author = {Lanchantin, Jack and Wang, Tianlu and Ordonez, Vicente and Qi, Yanjun},
  date = {2020-11-27},
  eprint = {2011.14027},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.14027},
  urldate = {2023-03-14},
  abstract = {Multi-label image classification is the task of predicting a set of labels corresponding to objects, attributes or other entities present in an image. In this work we propose the Classification Transformer (C-Tran), a general framework for multi-label image classification that leverages Transformers to exploit the complex dependencies among visual features and labels. Our approach consists of a Transformer encoder trained to predict a set of target labels given an input set of masked labels, and visual features from a convolutional neural network. A key ingredient of our method is a label mask training objective that uses a ternary encoding scheme to represent the state of the labels as positive, negative, or unknown during training. Our model shows state-of-the-art performance on challenging datasets such as COCO and Visual Genome. Moreover, because our model explicitly represents the uncertainty of labels during training, it is more general by allowing us to produce improved results for images with partial or extra label annotations during inference. We demonstrate this additional capability in the COCO, Visual Genome, News500, and CUB image datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/NN6N53GL/Lanchantin et al_2020_General Multi-label Image Classification with Transformers.pdf;/Users/mavi/Zotero/storage/IP7Q8GMP/2011.html}
}

@article{lathuiliereComprehensiveAnalysisDeep2020,
  title = {A {{Comprehensive Analysis}} of {{Deep Regression}}},
  author = {Lathuilière, Stéphane and Mesejo, Pablo and Alameda-Pineda, Xavier and Horaud, Radu},
  date = {2020-09-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {42},
  number = {9},
  eprint = {1803.08450},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2065--2081},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2019.2910523},
  url = {http://arxiv.org/abs/1803.08450},
  urldate = {2023-06-19},
  abstract = {Deep learning revolutionized data science, and recently its popularity has grown exponentially, as did the amount of papers employing deep networks. Vision tasks, such as human pose estimation, did not escape from this trend. There is a large number of deep models, where small changes in the network architecture, or in the data pre-processing, together with the stochastic nature of the optimization procedures, produce notably different results, making extremely difficult to sift methods that significantly outperform others. This situation motivates the current study, in which we perform a systematic evaluation and statistical analysis of vanilla deep regression, i.e. convolutional neural networks with a linear regression top layer. This is the first comprehensive analysis of deep regression techniques. We perform experiments on four vision problems, and report confidence intervals for the median performance as well as the statistical significance of the results, if any. Surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modifications in the network architecture. Our results reinforce the hypothesis according to which, in general, a general-purpose network (e.g. VGG-16 or ResNet-50) adequately tuned can yield results close to the state-of-the-art without having to resort to more complex and ad-hoc regression models.},
  keywords = {\_tablet,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/EPBA3NNI/Lathuilière et al_2020_A Comprehensive Analysis of Deep Regression.pdf;/Users/mavi/Zotero/storage/25AIBQM3/1803.html}
}

@article{lawtonLossPeriurbanAgricultural2022,
  title = {The Loss of Peri-Urban Agricultural Land and the State-Local Tensions in Managing Its Demise: {{The}} Case of {{Greater Western Sydney}}, {{Australia}}},
  shorttitle = {The Loss of Peri-Urban Agricultural Land and the State-Local Tensions in Managing Its Demise},
  author = {Lawton, Amy and Morrison, Nicky},
  date = {2022-09-01},
  journaltitle = {Land Use Policy},
  shortjournal = {Land Use Policy},
  volume = {120},
  pages = {106265},
  issn = {0264-8377},
  doi = {10.1016/j.landusepol.2022.106265},
  url = {https://www.sciencedirect.com/science/article/pii/S0264837722002927},
  urldate = {2022-09-29},
  abstract = {Cities globally have witnessed rapid growth, with the world’s population becoming more urban than rural over the last decade. In Australia, this has led to rapid urban expansion into the peri-urban fringes to provide housing and services for its population. The subsequent loss of agricultural land is largely seen as a market driven process, however increasing attention has been given to the contributory role of land use planning policy, systems, and public actors in specific contexts. This study examines the drivers of peri-urban agricultural land loss in Greater Western Sydney (GWS), one of the fastest growing peri-urban regions in Australia, with a particular attention on the role of government agencies. Drawing together a comprehensive set of quantitative and qualitative data, we chart the various tensions in managing population growth and housing pressures whilst at the same time seeking to protect existing peri-urban agricultural land. The research sheds light on the size, value, and extent of peri-urban agricultural activity in GWS. It then shows the extent to which such land is being converted to more profitable end uses, like housing. It contends that competing housing priorities and private market interests will continue to take precedence, unless the current pro-urbanisation narrative and associated political priorities change. To this end, the study also found evidence that the rapid succession of shocks and stresses experienced in city regions such as Sydney has now led to some fruitful questioning of that agenda, and the placing of urban growth imperatives above the preservation of agricultural and rural lands, and other natural resources.},
  langid = {english},
  keywords = {City region food systems,Government policy,Housing,Land use,Peri-urban agriculture,Urban planning},
  file = {/Users/mavi/Zotero/storage/MAW9BBI5/Lawton and Morrison - 2022 - The loss of peri-urban agricultural land and the s.pdf;/Users/mavi/Zotero/storage/Z5D653M4/S0264837722002927.html}
}

@article{leeAttentionBasedRecurrentNeural2020,
  title = {Attention-{{Based Recurrent Neural Network}} for {{Plant Disease Classification}}},
  author = {Lee, Sue Han and Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
  date = {2020-12-14},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {11},
  pages = {601250},
  issn = {1664-462X},
  doi = {10.3389/fpls.2020.601250},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2020.601250/full},
  urldate = {2022-05-16},
  abstract = {Plant diseases have a significant impact on global food security and the world's agricultural economy. Their early detection and classification increase the chances of setting up effective control measures, which is why the search for automatic systems that allow this is of major interest to our society. Several recent studies have reported promising results in the classification of plant diseases from RGB images on the basis of Convolutional Neural Networks (CNN). These studies have been successfully experimented on a large number of crops and symptoms, and they have shown significant advantages in the support of human expertise. However, the CNN models still have limitations. In particular, CNN models do not necessarily focus on the visible parts affected by a plant disease to allow their classification, and they can sometimes take into account irrelevant backgrounds or healthy plant parts. In this paper, we therefore develop a new technique based on a Recurrent Neural Network (RNN) to automatically locate infected regions and extract relevant features for disease classification. We show experimentally that our RNN-based approach is more robust and has a greater ability to generalize to unseen infected crop species as well as to different plant disease domain images compared to classical CNN approaches. We also analyze the focus of attention as learned by our RNN and show that our approach is capable of accurately locating infectious diseases in plants. Our approach, which has been tested on a large number of plant species, should thus contribute to the development of more effective means of detecting and classifying crop pathogens in the near future.},
  langid = {english},
  keywords = {read,RNN},
  file = {/Users/mavi/Zotero/storage/Q9G5BPCZ/Lee et al. - 2020 - Attention-Based Recurrent Neural Network for Plant.pdf}
}

@inproceedings{leeConditionalMultiTaskLearning2021,
  title = {Conditional {{Multi-Task}} Learning for {{Plant Disease Identification}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Lee, Sue Han and Goeau, Herve and Bonnet, Pierre and Joly, Alexis},
  date = {2021-01-10},
  pages = {3320--3327},
  publisher = {{IEEE}},
  location = {{Milan, Italy}},
  doi = {10.1109/ICPR48806.2021.9412643},
  url = {https://ieeexplore.ieee.org/document/9412643/},
  urldate = {2022-05-16},
  abstract = {Several recent studies have proposed an automatic plant disease identification system based on deep learning. However, these approaches are generally based on learned classification models with target classes of joint host species-disease pairs that may not allow optimal use of the available information. This is because these approaches require distinguishing between similar host species or diseases, and more importantly, they have limited scalability due to the size of a network gradually increases as new classes are added, despite the fact that information on host species or diseases is already available. This constraint is all the more important as it can be difficult to collect/establish a specific list of all diseases for each host plant species in an actual application. In this paper, we address the problems by proposing a new conditional multi-task learning (CMTL) approach which allows the distribution of host species and disease characteristics learned simultaneously with a conditional link between them. This conditioning is formed in such a way that the knowledge to infer the prediction of one concept (the diseases) depends on the other concept (the host species), which corresponds to the way plant pathologists used to infer the diseases of the host species. We show that our approach can improve the performance of plant disease identification compared to the usual species-disease pair modeling in the previous studies. Meanwhile, we also compose a new dataset on plant disease identification that could serve as an important benchmark in this field.},
  eventtitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  isbn = {978-1-72818-808-9},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/NC9BDT5Z/Lee et al. - 2021 - Conditional Multi-Task learning for Plant Disease .pdf}
}

@article{leeMultiOrganPlantClassification2018,
  title = {Multi-{{Organ Plant Classification Based}} on {{Convolutional}} and {{Recurrent Neural Networks}}},
  author = {Lee, Sue Han and Chan, Chee Seng and Remagnino, Paolo},
  date = {2018-09},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {27},
  number = {9},
  pages = {4287--4301},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2018.2836321},
  url = {https://ieeexplore.ieee.org/document/8359391/},
  urldate = {2022-05-19},
  abstract = {Classification of plants based on a multi-organ approach is very challenging. Although additional data provide more information that might help to disambiguate between species, the variability in shape and appearance in plant organs also raises the degree of complexity of the problem. Despite promising solutions built using deep learning enable representative features to be learned for plant images, the existing approaches focus mainly on generic features for species classification, disregarding the features representing plant organs. In fact, plants are complex living organisms sustained by a number of organ systems. In our approach, we introduce a hybrid generic-organ convolutional neural network (HGO-CNN), which takes into account both organ and generic information, combining them using a new feature fusion scheme for species classification. Next, instead of using a CNN-based method to operate on one image with a single organ, we extend our approach. We propose a new framework for plant structural learning using the recurrent neural network-based method. This novel approach supports classification based on a varying number of plant views, capturing one or more organs of a plant, by optimizing the contextual dependencies between them. We also present the qualitative results of our proposed models based on feature visualization techniques and show that the outcomes of visualizations depict our hypothesis and expectation. Finally, we show that by leveraging and combining the aforementioned techniques, our best network outperforms the state of the art on the PlantClef2015 benchmark. The source code and models are available at https://github.com/cs-chan/Deep-Plant.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/5QEKJK7G/Lee et al. - 2018 - Multi-Organ Plant Classification Based on Convolut.pdf}
}

@article{leeNewPerspectivesPlant2020,
  title = {New Perspectives on Plant Disease Characterization Based on Deep Learning},
  author = {Lee, Sue Han and Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
  date = {2020-03},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {170},
  pages = {105220},
  issn = {01681699},
  doi = {10.1016/j.compag.2020.105220},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169919300560},
  urldate = {2022-05-16},
  abstract = {The control of plant diseases is a major challenge to ensure global food security and sustainable agriculture. Several recent studies have proposed to improve existing procedures for early detection of plant diseases through modern automatic image recognition systems based on deep learning. In this article, we study these methods in detail, especially those based on convolutional neural networks. We first examine whether it is more relevant to fine-tune a pre-trained model on a plant identification task rather than a general object recognition task. In particular, we show, through visualization techniques, that the characteristics learned differ according to the approach adopted and that they do not necessarily focus on the part affected by the disease. Therefore, we introduce a more intuitive method that considers diseases independently of crops, and we show that it is more effective than the classic crop-disease pair approach, especially when dealing with disease involving crops that are not illustrated in the training database. This finding therefore encourages future research to rethink the current de facto paradigm of crop disease categorization.},
  langid = {english},
  keywords = {CNN,Disease detection,Image processing,Montpelliier,read},
  file = {/Users/mavi/Zotero/storage/5HESHXAS/Lee et al. - 2020 - New perspectives on plant disease characterization.pdf}
}

@article{leeTemplateTransformerNetworks,
  title = {Template {{Transformer Networks}} for {{Image Segmentation}}},
  author = {Lee, Matthew Chung Hai and Petersen, Kersten and Pawlowski, Nick and Glocker, Ben and Schaap, Michiel},
  pages = {4},
  abstract = {In this paper we introduce and compare different approaches for incorporating shape prior information into neural network based image segmentation. Specifically, we introduce the concept of template transformer networks where a shape template is deformed to match the underlying structure of interest through an end-to-end trained spatial transformer network. This has the advantage of explicitly enforcing shape priors and is free of discretisation artefacts by providing a soft partial volume segmentation. We also introduce a simple yet effective way of incorporating priors in state-of-the-art pixel-wise binary classification methods such as fully convolutional networks and U-net. Here, the template shape is given as an additional input channel, incorporating this information significantly reduces false positives. We report results on sub-voxel segmentation of coronary lumen structures in cardiac computed tomography showing the benefit of incorporating priors in neural network based image segmentation.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/RRW4BX6D/Lee et al. - Template Transformer Networks for Image Segmentati.pdf}
}

@article{leiAutomaticDetectionCounting2018,
  title = {Automatic Detection and Counting of Urediniospores of {{Puccinia}} Striiformis f. Sp. Tritici Using Spore Traps and Image Processing},
  author = {Lei, Yu and Yao, Zhifeng and He, Dongjian},
  date = {2018-12},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {8},
  number = {1},
  pages = {13647},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-31899-0},
  url = {http://www.nature.com/articles/s41598-018-31899-0},
  urldate = {2022-06-23},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/C6WGNZ76/Lei et al. - 2018 - Automatic detection and counting of urediniospores.pdf}
}

@article{letzgusExplainableAIRegression2022,
  title = {Toward {{Explainable AI}} for {{Regression Models}}},
  author = {Letzgus, Simon and Wagner, Patrick and Lederer, Jonas and Samek, Wojciech and Müller, Klaus-Robert and Montavon, Gregoire},
  date = {2022-07},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {39},
  number = {4},
  eprint = {2112.11407},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {40--58},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2022.3153277},
  url = {http://arxiv.org/abs/2112.11407},
  urldate = {2023-06-19},
  abstract = {In addition to the impressive predictive power of machine learning (ML) models, more recently, explanation methods have emerged that enable an interpretation of complex non-linear learning models such as deep neural networks. Gaining a better understanding is especially important e.g. for safety-critical ML applications or medical diagnostics etc. While such Explainable AI (XAI) techniques have reached significant popularity for classifiers, so far little attention has been devoted to XAI for regression models (XAIR). In this review, we clarify the fundamental conceptual differences of XAI for regression and classification tasks, establish novel theoretical insights and analysis for XAIR, provide demonstrations of XAIR on genuine practical regression problems, and finally discuss the challenges remaining for the field.},
  keywords = {\_tablet,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/DJXJIPU9/Letzgus et al_2022_Toward Explainable AI for Regression Models.pdf;/Users/mavi/Zotero/storage/GNE7AIA4/2112.html}
}

@article{lifei-feiOneshotLearningObject2006,
  title = {One-Shot Learning of Object Categories},
  author = {{Li Fei-Fei} and Fergus, R. and Perona, P.},
  date = {2006-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
  volume = {28},
  number = {4},
  pages = {594--611},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2006.79},
  url = {http://ieeexplore.ieee.org/document/1597116/},
  urldate = {2022-05-20},
  abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by Maximum Likelihood (ML) and Maximum A Posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/F7GQMAFC/Li Fei-Fei et al. - 2006 - One-shot learning of object categories.pdf}
}

@online{liFullyConvolutionalNetworks2021,
  title = {Fully {{Convolutional Networks}} for {{Panoptic Segmentation}} with {{Point-based Supervision}}},
  author = {Li, Yanwei and Zhao, Hengshuang and Qi, Xiaojuan and Chen, Yukang and Qi, Lu and Wang, Liwei and Li, Zeming and Sun, Jian and Jia, Jiaya},
  date = {2021-08-17},
  eprint = {2108.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.07682},
  urldate = {2022-05-17},
  abstract = {In this paper, we present a conceptually simple, strong, and efficient framework for fully- and weakly-supervised panoptic segmentation, called Panoptic FCN. Our approach aims to represent and predict foreground things and background stuff in a unified fully convolutional pipeline, which can be optimized with point-based fully or weak supervision. In particular, Panoptic FCN encodes each object instance or stuff category with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent properties for things and stuff can be respectively satisfied in a simple generate-kernel-then-segment workflow. Without extra boxes for localization or instance separation, the proposed approach outperforms the previous box-based and -free models with high efficiency. Furthermore, we propose a new form of pointbased annotation for weakly-supervised panoptic segmentation. It only needs several random points for both things and stuff, which dramatically reduces the annotation cost of human. The proposed Panoptic FCN is also proved to have much superior performance in this weakly-supervised setting, which achieves 82\% of the fully-supervised performance with only 20 randomly annotated points per instance. Extensive experiments demonstrate the effectiveness and efficiency of Panoptic FCN on COCO, VOC 2012, Cityscapes, and Mapillary Vistas datasets. And it sets up a new leading benchmark for both fully- and weakly-supervised panoptic segmentation. Our code and models are made publicly available at https://github.com/dvlab-research/PanopticFCN.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/K7PK3YGJ/Li et al. - 2021 - Fully Convolutional Networks for Panoptic Segmenta.pdf}
}

@online{LightningModulePyTorchLightning,
  title = {{{LightningModule}} — {{PyTorch Lightning}} 1.7.7 Documentation},
  url = {https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html},
  urldate = {2022-10-04}
}

@article{liLeafSegmentationDense2018,
  title = {Leaf {{Segmentation}} on {{Dense Plant Point Clouds}} with {{Facet Region Growing}}},
  author = {Li, Dawei and Cao, Yan and Tang, Xue-song and Yan, Siyuan and Cai, Xin},
  date = {2018-10-25},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {18},
  number = {11},
  pages = {3625},
  issn = {1424-8220},
  doi = {10.3390/s18113625},
  url = {http://www.mdpi.com/1424-8220/18/11/3625},
  urldate = {2022-05-16},
  abstract = {Leaves account for the largest proportion of all organ areas for most kinds of plants, and are comprise the main part of the photosynthetically active material in a plant. Observation of individual leaves can help to recognize their growth status and measure complex phenotypic traits. Current image-based leaf segmentation methods have problems due to highly restricted species and vulnerability toward canopy occlusion. In this work, we propose an individual leaf segmentation approach for dense plant point clouds using facet over-segmentation and facet region growing. The approach can be divided into three steps: (1) point cloud pre-processing, (2) facet over-segmentation, and (3) facet region growing for individual leaf segmentation. The experimental results show that the proposed method is effective and efficient in segmenting individual leaves from 3D point clouds of greenhouse ornamentals such as Epipremnum aureum, Monstera deliciosa, and Calathea makoyana, and the average precision and recall are both above 90\%. The results also reveal the wide applicability of the proposed methodology for point clouds scanned from different kinds of 3D imaging systems, such as stereo vision and Kinect v2. Moreover, our method is potentially applicable in a broad range of applications that aim at segmenting regular surfaces and objects from a point cloud.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/7DI7PPCX/Li et al. - 2018 - Leaf Segmentation on Dense Plant Point Clouds with.pdf}
}

@article{liMachineLearningBased2021,
  title = {Machine Learning‐based Automated Fungal Cell Counting under a Complicated Background with Ilastik and {{ImageJ}}},
  author = {Li, Chenxi and Ma, Xiaoyu and Deng, Jing and Li, Jiajia and Liu, Yanjie and Zhu, Xudong and Liu, Jin and Zhang, Ping},
  date = {2021-11},
  journaltitle = {Engineering in Life Sciences},
  shortjournal = {Eng. Life Sci.},
  volume = {21},
  number = {11},
  pages = {769--777},
  issn = {1618-0240, 1618-2863},
  doi = {10.1002/elsc.202100055},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/elsc.202100055},
  urldate = {2022-05-20},
  abstract = {Measuring the concentration and viability of fungal cells is an important and fundamental procedure in scientific research and industrial fermentation. In consideration of the drawbacks of manual cell counting, large quantities of fungal cells require methods that provide easy, objective and reproducible highthroughput calculations, especially for samples in complicated backgrounds. To answer this challenge, we explored and developed an easy-to-use fungal cell counting pipeline that combined the machine learning-based ilastik tool with the freeware ImageJ, as well as a conventional photomicroscope. Briefly, learning from labels provided by the user, ilastik performs segmentation and classification automatically in batch processing mode and thus discriminates fungal cells from complex backgrounds. The files processed through ilastik can be recognized by ImageJ, which can compute the numeric results with the macro ‘Fungal Cell Counter’. Taking the yeast Cryptococccus deneoformans and the filamentous fungus Pestalotiopsis microspora as examples, we observed that the customizable software algorithm reduced inter-operator errors significantly and achieved accurate and objective results, while manual counting with a haemocytometer exhibited some errors between repeats and required more time. In summary, a convenient, rapid, reproducible and extremely low-cost method to count yeast cells and fungal spores is described here, which can be applied to multiple kinds of eucaryotic microorganisms in genetics, cell biology and industrial fermentation.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/GWCP97RF/Li et al. - 2021 - Machine learning‐based automated fungal cell count.pdf}
}

@online{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-20},
  eprint = {1405.0312},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2022-05-20},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/LL9WEBVZ/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf}
}

@article{linSelfSupervisedLeafSegmentation2023,
  title = {Self-{{Supervised Leaf Segmentation}} under {{Complex Lighting Conditions}}},
  author = {Lin, Xufeng and Li, Chang-Tsun and Adams, Scott and Kouzani, Abbas Z. and Jiang, Richard and He, Ligang and Hu, Yongjian and Vernon, Michael and Doeven, Egan and Webb, Lawrence and Mcclellan, Todd and Guskich, Adam},
  date = {2023-03-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {135},
  pages = {109021},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2022.109021},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320322005015},
  urldate = {2023-05-03},
  abstract = {As an essential prerequisite task in image-based plant phenotyping, leaf segmentation has garnered increasing attention in recent years. While self-supervised learning is emerging as an effective alternative to various computer vision tasks, its adaptation for image-based plant phenotyping remains rather unexplored. In this work, we present a self-supervised leaf segmentation framework consisting of a self-supervised semantic segmentation model, a color-based leaf segmentation algorithm, and a self-supervised color correction model. The self-supervised semantic segmentation model groups the semantically similar pixels by iteratively referring to the self-contained information, allowing the pixels of the same semantic object to be jointly considered by the color-based leaf segmentation algorithm for identifying the leaf regions. Additionally, we propose to use a self-supervised color correction model for images taken under complex illumination conditions. Experimental results on datasets of different plant species demonstrate the potential of the proposed self-supervised framework in achieving effective and generalizable leaf segmentation.},
  langid = {english},
  keywords = {\_tablet,Cannabis,Color correction,Convolutional neural networks,Image-based plant phenotyping,Leaf segmentation,Self-supervised learning},
  file = {/Users/mavi/Zotero/storage/SXADFLAT/Lin et al_2023_Self-Supervised Leaf Segmentation under Complex Lighting Conditions.pdf;/Users/mavi/Zotero/storage/3T686KFM/S0031320322005015.html}
}

@online{liuConvNet2020s2022,
  title = {A {{ConvNet}} for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  date = {2022-03-02},
  eprint = {2201.03545},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.03545},
  url = {http://arxiv.org/abs/2201.03545},
  urldate = {2023-04-18},
  abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read},
  file = {/Users/mavi/Zotero/storage/C2QLTZ33/Liu et al_2022_A ConvNet for the 2020s.pdf;/Users/mavi/Zotero/storage/QB6XYNHS/2201.html}
}

@article{liuIdentificationAppleLeaf2017,
  title = {Identification of {{Apple Leaf Diseases Based}} on {{Deep Convolutional Neural Networks}}},
  author = {Liu, Bin and Zhang, Yun and He, DongJian and Li, Yuxiang},
  date = {2017-12-29},
  journaltitle = {Symmetry},
  shortjournal = {Symmetry},
  volume = {10},
  number = {1},
  pages = {11},
  issn = {2073-8994},
  doi = {10.3390/sym10010011},
  url = {http://www.mdpi.com/2073-8994/10/1/11},
  urldate = {2022-05-16},
  abstract = {Mosaic, Rust, Brown spot, and Alternaria leaf spot are the four common types of apple leaf diseases. Early diagnosis and accurate identification of apple leaf diseases can control the spread of infection and ensure the healthy development of the apple industry. The existing research uses complex image preprocessing and cannot guarantee high recognition rates for apple leaf diseases. This paper proposes an accurate identifying approach for apple leaf diseases based on deep convolutional neural networks. It includes generating sufficient pathological images and designing a novel architecture of a deep convolutional neural network based on AlexNet to detect apple leaf diseases. Using a dataset of 13,689 images of diseased apple leaves, the proposed deep convolutional neural network model is trained to identify the four common apple leaf diseases. Under the hold-out test set, the experimental results show that the proposed disease identification approach based on the convolutional neural network achieves an overall accuracy of 97.62\%, the model parameters are reduced by 51,206,928 compared with those in the standard AlexNet model, and the accuracy of the proposed model with generated pathological images obtains an improvement of 10.83\%. This research indicates that the proposed deep learning model provides a better solution in disease control for apple leaf diseases with high accuracy and a faster convergence rate, and that the image generation technique proposed in this paper can enhance the robustness of the convolutional neural network model.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/SB83HEJG/Liu et al. - 2017 - Identification of Apple Leaf Diseases Based on Dee.pdf}
}

@online{liuPaddleSegHighEfficientDevelopment2021,
  title = {{{PaddleSeg}}: {{A High-Efficient Development Toolkit}} for {{Image Segmentation}}},
  shorttitle = {{{PaddleSeg}}},
  author = {Liu, Yi and Chu, Lutao and Chen, Guowei and Wu, Zewu and Chen, Zeyu and Lai, Baohua and Hao, Yuying},
  date = {2021-01-15},
  eprint = {2101.06175},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2101.06175},
  url = {http://arxiv.org/abs/2101.06175},
  urldate = {2022-10-17},
  abstract = {Image Segmentation plays an essential role in computer vision and image processing with various applications from medical diagnosis to autonomous car driving. A lot of segmentation algorithms have been proposed for addressing specific problems. In recent years, the success of deep learning techniques has tremendously influenced a wide range of computer vision areas, and the modern approaches of image segmentation based on deep learning are becoming prevalent. In this article, we introduce a high-efficient development toolkit for image segmentation, named PaddleSeg. The toolkit aims to help both developers and researchers in the whole process of designing segmentation models, training models, optimizing performance and inference speed, and deploying models. Currently, PaddleSeg supports around 20 popular segmentation models and more than 50 pre-trained models from real-time and high-accuracy levels. With modular components and backbone networks, users can easily build over one hundred models for different requirements. Furthermore, we provide comprehensive benchmarks and evaluations to show that these segmentation algorithms trained on our toolkit have more competitive accuracy. Also, we provide various real industrial applications and practical cases based on PaddleSeg. All codes and examples of PaddleSeg are available at https://github.com/PaddlePaddle/PaddleSeg.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/UWZW65FG/Liu et al. - 2021 - PaddleSeg A High-Efficient Development Toolkit fo.pdf;/Users/mavi/Zotero/storage/IH2T8VSC/2101.html}
}

@article{liuPanopticFeatureFusion2021,
  title = {Panoptic {{Feature Fusion Net}}: {{A Novel Instance Segmentation Paradigm}} for {{Biomedical}} and {{Biological Images}}},
  shorttitle = {Panoptic {{Feature Fusion Net}}},
  author = {Liu, Dongnan and Zhang, Donghao and Song, Yang and Huang, Heng and Cai, Weidong},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {30},
  eprint = {2002.06345},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2045--2059},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2021.3050668},
  url = {http://arxiv.org/abs/2002.06345},
  urldate = {2022-05-17},
  abstract = {Instance segmentation is an important task for biomedical and biological image analysis. Due to the complicated background components, the high variability of object appearances, numerous overlapping objects, and ambiguous object boundaries, this task still remains challenging. Recently, deep learning based methods have been widely employed to solve these problems and can be categorized into proposal-free and proposalbased methods. However, both proposal-free and proposal-based methods suffer from information loss, as they focus on either global-level semantic or local-level instance features. To tackle this issue, we present a Panoptic Feature Fusion Net (PFFNet) that unifies the semantic and instance features in this work. Specifically, our proposed PFFNet contains a residual attention feature fusion mechanism to incorporate the instance prediction with the semantic features, in order to facilitate the semantic contextual information learning in the instance branch. Then, a mask quality sub-branch is designed to align the confidence score of each object with the quality of the mask prediction. Furthermore, a consistency regularization mechanism is designed between the semantic segmentation tasks in the semantic and instance branches, for the robust learning of both tasks. Extensive experiments demonstrate the effectiveness of our proposed PFFNet, which outperforms several state-of-the-art methods on various biomedical and biological datasets.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/QWQGQGFG/Liu et al. - 2021 - Panoptic Feature Fusion Net A Novel Instance Segm.pdf}
}

@article{liuPlantDiseasesPests2021,
  title = {Plant Diseases and Pests Detection Based on Deep Learning: A Review},
  shorttitle = {Plant Diseases and Pests Detection Based on Deep Learning},
  author = {Liu, Jun and Wang, Xuewei},
  date = {2021-12},
  journaltitle = {Plant Methods},
  shortjournal = {Plant Methods},
  volume = {17},
  number = {1},
  pages = {22},
  issn = {1746-4811},
  doi = {10.1186/s13007-021-00722-9},
  url = {https://plantmethods.biomedcentral.com/articles/10.1186/s13007-021-00722-9},
  urldate = {2022-05-16},
  abstract = {Plant diseases and pests are important factors determining the yield and quality of plants. Plant diseases and pests identification can be carried out by means of digital image processing. In recent years, deep learning has made breakthroughs in the field of digital image processing, far superior to traditional methods. How to use deep learning technology to study plant diseases and pests identification has become a research issue of great concern to researchers. This review provides a definition of plant diseases and pests detection problem, puts forward a comparison with traditional plant diseases and pests detection methods. According to the difference of network structure, this study outlines the research on plant diseases and pests detection based on deep learning in recent years from three aspects of classification network, detection network and segmentation network, and the advantages and disadvantages of each method are summarized. Common datasets are introduced, and the performance of existing studies is compared. On this basis, this study discusses possible challenges in practical applications of plant diseases and pests detection based on deep learning. In addition, possible solutions and research ideas are proposed for the challenges, and several suggestions are given. Finally, this study gives the analysis and prospect of the future trend of plant diseases and pests detection based on deep learning.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/VACZCRG3/Liu and Wang - 2021 - Plant diseases and pests detection based on deep l.pdf}
}

@incollection{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  date = {2016},
  volume = {9905},
  eprint = {1512.02325},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  url = {http://arxiv.org/abs/1512.02325},
  urldate = {2023-01-23},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300\textbackslash times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500\textbackslash times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/6AKGL727/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;/Users/mavi/Zotero/storage/9GTRLTGB/1512.html}
}

@online{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-08-17},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.14030},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2023-04-28},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read},
  file = {/Users/mavi/Zotero/storage/VFQS6PTU/Liu et al_2021_Swin Transformer.pdf;/Users/mavi/Zotero/storage/PMMEGZV6/2103.html}
}

@article{liWeReallyNeed2020,
  title = {Do We Really Need Deep {{CNN}} for Plant Diseases Identification?},
  author = {Li, Yang and Nie, Jing and Chao, Xuewei},
  date = {2020-11},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {178},
  pages = {105803},
  issn = {01681699},
  doi = {10.1016/j.compag.2020.105803},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S016816992031190X},
  urldate = {2022-06-03},
  abstract = {Timely identification of plant diseases plays crucial roles in the management and decision-making to protect the agricultural yield and quality. In this research field, there have been so many efforts focused on deep learning, namely deep CNN. The CNN is powerful and essential for image processing; however, do we really need deep CNN for plant diseases identification, cannot the shallow CNN extract enough information? We proposed two methods namely SCNN-KSVM (Shallow CNN with Kernel SVM) and SCNN-RF (Shallow CNN with Random Forest) to solve this confusion. The comparison experiments with other deep learning models were carried out on three different datasets. The results show that the SCNN-KSVM and SCNN-RF outperform other pretrained deep models on the indicators of precision, recall, and F1-score, with fewer parameters. The combination of shallow CNN and classic machine learning classification algorithm is a positive attempt to deal with the plant diseases identification in a simple manner.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/HTGFDHM8/Li et al. - 2020 - Do we really need deep CNN for plant diseases iden.pdf}
}

@article{lobetImageAnalysisPlant2017,
  title = {Image {{Analysis}} in {{Plant Sciences}}: {{Publish Then Perish}}},
  shorttitle = {Image {{Analysis}} in {{Plant Sciences}}},
  author = {Lobet, Guillaume},
  date = {2017-07},
  journaltitle = {Trends in Plant Science},
  shortjournal = {Trends Plant Sci},
  volume = {22},
  number = {7},
  eprint = {28571940},
  eprinttype = {pmid},
  pages = {559--566},
  issn = {1878-4372},
  doi = {10.1016/j.tplants.2017.05.002},
  abstract = {Image analysis has become a powerful technique for most plant scientists. In recent years dozens of image analysis tools have been published in plant science journals. These tools cover the full spectrum of plant scales, from single cells to organs and canopies. However, the field of plant image analysis remains in its infancy. It still has to overcome important challenges, such as the lack of robust validation practices or the absence of long-term support. In this Opinion article, I: (i) present the current state of the field, based on data from the plant-image-analysis.org database; (ii) identify the challenges faced by its community; and (iii) propose workable ways of improvement.},
  langid = {english},
  keywords = {{Databases, Factual},image analysis,maintenance,open data,open science,Plants,validation}
}

@article{luksicIdentificationPowderyMildew2022,
  title = {Identification of Powdery Mildew Resistance in Wild Grapevine ({{Vitis}} Vinifera Subsp. Sylvestris {{Gmel Hegi}}) from {{Croatia}} and {{Bosnia}} and {{Herzegovina}}},
  author = {Lukšić, Katarina and Zdunić, Goran and Katarina, Hančević and Žulj Mihaljević, Maja and Mucalo, Ana and Maul, Erika and Riaz, Summaira and Pejic, Ivan},
  date = {2022-02-08},
  journaltitle = {Scientific Reports},
  shortjournal = {Scientific Reports},
  volume = {12},
  doi = {10.1038/s41598-022-06037-6},
  abstract = {Wild grapevine ( Vitis vinifera subsp. sylvestris ) is widely recognized as an important source of resistance or tolerance genes for diseases and environmental stresses. Recent studies revealed partial resistance to powdery mildew ( Erysiphe necator, PM) in V. sylvestris from Central Asia. Here, we report resistance to PM of V. sylvestris collected from different regions of Croatia and in seedling populations established from in situ V. sylvestris accessions. Ninety-one in situ individuals and 67 V. sylvestris seedlings were evaluated for PM resistance according to OIV 455 descriptor. Three SSR markers (SC47-18, SC8-071-0014, and UDV-124) linked to PM resistance locus Ren1 were used to decipher allelic structure. Nine seedlings showed resistance in in vivo evaluations while leaf disk assays revealed three PM-resistant accessions. One V. vinifera cultivar used as a control for PM evaluations also showed high phenotypic resistance. Based on the presence of one or two resistance alleles that are linked to the Ren1 locus, 32 resistant seedlings and 41 resistant in situ genotypes were identified in the investigated set. Eight seedlings showed consistent phenotypic PM resistance, of which seven carried one or two alleles at the tested markers. This study provides the first evidence of PM resistance present within the eastern Adriatic V. sylvestris germplasm.},
  file = {/Users/mavi/Zotero/storage/5MHK96C7/Lukšić et al_2022_Identification of powdery mildew resistance in wild grapevine (Vitis vinifera.pdf}
}

@inproceedings{luoCoarsetoFineAnnotationEnrichment2018,
  title = {Coarse-to-{{Fine Annotation Enrichment}} for {{Semantic Segmentation Learning}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Luo, Yadan and Wang, Ziwei and Huang, Zi and Yang, Yang and Zhao, Cong},
  date = {2018-10-17},
  eprint = {1808.07209},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {237--246},
  doi = {10.1145/3269206.3271672},
  url = {http://arxiv.org/abs/1808.07209},
  urldate = {2023-05-05},
  abstract = {Rich high-quality annotated data is critical for semantic segmentation learning, yet acquiring dense and pixel-wise ground-truth is both labor- and time-consuming. Coarse annotations (e.g., scribbles, coarse polygons) offer an economical alternative, with which training phase could hardly generate satisfactory performance unfortunately. In order to generate high-quality annotated data with a low time cost for accurate segmentation, in this paper, we propose a novel annotation enrichment strategy, which expands existing coarse annotations of training data to a finer scale. Extensive experiments on the Cityscapes and PASCAL VOC 2012 benchmarks have shown that the neural networks trained with the enriched annotations from our framework yield a significant improvement over that trained with the original coarse labels. It is highly competitive to the performance obtained by using human annotated dense annotations. The proposed method also outperforms among other state-of-the-art weakly-supervised segmentation methods.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/XCIVACEH/Luo et al_2018_Coarse-to-Fine Annotation Enrichment for Semantic Segmentation Learning.pdf;/Users/mavi/Zotero/storage/8YSNMRFH/1808.html}
}

@article{luReviewConvolutionalNeural2021,
  title = {Review on {{Convolutional Neural Network}} ({{CNN}}) {{Applied}} to {{Plant Leaf Disease Classification}}},
  author = {Lu, Jinzhu and Tan, Lijuan and Jiang, Huanyu},
  date = {2021-07-27},
  journaltitle = {Agriculture},
  shortjournal = {Agriculture},
  volume = {11},
  number = {8},
  pages = {707},
  issn = {2077-0472},
  doi = {10.3390/agriculture11080707},
  url = {https://www.mdpi.com/2077-0472/11/8/707},
  urldate = {2022-06-03},
  abstract = {Crop production can be greatly reduced due to various diseases, which seriously endangers food security. Thus, detecting plant diseases accurately is necessary and urgent. Traditional classification methods, such as naked-eye observation and laboratory tests, have many limitations, such as being time consuming and subjective. Currently, deep learning (DL) methods, especially those based on convolutional neural network (CNN), have gained widespread application in plant disease classification. They have solved or partially solved the problems of traditional classification methods and represent state-of-the-art technology in this field. In this work, we reviewed the latest CNN networks pertinent to plant leaf disease classification. We summarized DL principles involved in plant disease classification. Additionally, we summarized the main problems and corresponding solutions of CNN used for plant disease classification. Furthermore, we discussed the future development direction in plant disease classification.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/HE57GU24/Lu et al. - 2021 - Review on Convolutional Neural Network (CNN) Appli.pdf}
}

@article{luSurveyPublicDatasets2020,
  title = {A Survey of Public Datasets for Computer Vision Tasks in Precision Agriculture},
  author = {Lu, Yuzhen and Young, Sierra},
  date = {2020-11},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {178},
  pages = {105760},
  issn = {01681699},
  doi = {10.1016/j.compag.2020.105760},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169920312709},
  urldate = {2022-05-20},
  abstract = {Computer vision technologies have attracted significant interest in precision agriculture in recent years. At the core of robotics and artificial intelligence, computer vision enables various tasks from planting to harvesting in the crop production cycle to be performed automatically and efficiently. However, the scarcity of public image datasets remains a crucial bottleneck for fast prototyping and evaluation of computer vision and machine learning algorithms for the targeted tasks. Since 2015, a number of image datasets have been established and made publicly available to alleviate this bottleneck. Despite this progress, a dedicated survey on these datasets is still lacking. To fill this gap, this paper makes the first comprehensive but not exhaustive review of the public image datasets collected under field conditions for facilitating precision agriculture, which include 15 datasets on weed control, 10 datasets on fruit detection, and 9 datasets on miscellaneous applications. We survey the main characteristics and applications of these datasets, and discuss the key considerations for creating high-quality public image datasets. This survey paper will be valuable for the research community on the selection of suitable image datasets for algorithm development and identification of where creation of new image datasets is needed to support precision agriculture.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/F9GSH5ZV/Lu and Young - 2020 - A survey of public datasets for computer vision ta.pdf}
}

@article{MaladiesRavageursDiagnostiquer,
  title = {Maladies et ravageurs : diagnostiquer le plus tôt possible},
  pages = {3},
  langid = {french},
  file = {/Users/mavi/Zotero/storage/UWGALKRZ/Maladies et ravageurs  diagnostiquer le plus tôt .pdf}
}

@article{martinelliAdvancedMethodsPlant2015,
  title = {Advanced Methods of Plant Disease Detection. {{A}} Review},
  author = {Martinelli, Federico and Scalenghe, Riccardo and Davino, Salvatore and Panno, Stefano and Scuderi, Giuseppe and Ruisi, Paolo and Villa, Paolo and Stroppiana, Daniela and Boschetti, Mirco and Goulart, Luiz R. and Davis, Cristina E. and Dandekar, Abhaya M.},
  date = {2015-01},
  journaltitle = {Agronomy for Sustainable Development},
  shortjournal = {Agron. Sustain. Dev.},
  volume = {35},
  number = {1},
  pages = {1--25},
  issn = {1774-0746, 1773-0155},
  doi = {10.1007/s13593-014-0246-1},
  url = {http://link.springer.com/10.1007/s13593-014-0246-1},
  urldate = {2022-05-16},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/BB48I5UK/Martinelli et al. - 2015 - Advanced methods of plant disease detection. A rev.pdf}
}

@article{matamandaInterplayUrbanAgriculture2022,
  title = {The Interplay between Urban Agriculture and Spatial ({{In}}) Justice: {{Case}} Study Analysis of {{Harare}}, {{Zimbabwe}}},
  shorttitle = {The Interplay between Urban Agriculture and Spatial ({{In}}) Justice},
  author = {Matamanda, Abraham R. and Mandebvu-Chaora, Chipo and Rammile, Siphokazi},
  date = {2022-04-01},
  journaltitle = {Land Use Policy},
  shortjournal = {Land Use Policy},
  volume = {115},
  pages = {106029},
  issn = {0264-8377},
  doi = {10.1016/j.landusepol.2022.106029},
  url = {https://www.sciencedirect.com/science/article/pii/S0264837722000564},
  urldate = {2022-09-29},
  abstract = {The debate on urban agriculture in African cities has focused on the environmental consequences and the livelihood support to the urban poor. These debates are primarily centered on the urban policies and plans that largely fail to integrate urban agriculture, albeit its importance. This study expands on this scholarship but focuses on the interplay of urban agriculture and spatial justice in Harare. We argue that urban planning in Harare is premised on classism and perpetuates spatial segregation, which manifests through different land uses in residential suburbs for the low- and high-income suburbs. Using an exploratory phenomenological approach, we interrogate the interplay through in-depth interviews with respondents from three study contexts: Hopley Farm Settlement and two suburbs, Hatfield and Glen-View. The primary data were triangulated with secondary data to increase the validity of the study. The findings challenge the common assumption that urban agriculture is essentially a survival strategy that the urban poor engage in to support their livelihoods. Rather, beyond the subsistence nature of urban agriculture practices among the poor, the current practice in Harare is characterized by a new group of elites who have commodified urban agriculture. Unlike the authorities' restrictive policies and strategies in poor neighborhoods, urban agriculture has been integrated into the official land use plans, becoming an integral land use activity in the affluent suburbs. This study draws attention to a largely overlooked aspect in literature on spatial (in) justice in the Zimbabwean context.},
  langid = {english},
  keywords = {Harare,land use planning,spatial justice,urban agriculture},
  file = {/Users/mavi/Zotero/storage/K2JNM3R4/Matamanda et al. - 2022 - The interplay between urban agriculture and spatia.pdf;/Users/mavi/Zotero/storage/HQ8YV53B/S0264837722000564.html}
}

@article{mauludReviewLinearRegression2020,
  title = {A {{Review}} on {{Linear Regression Comprehensive}} in {{Machine Learning}}},
  author = {Maulud, Dastan and Abdulazeez, Adnan M.},
  date = {2020-12-31},
  journaltitle = {Journal of Applied Science and Technology Trends},
  shortjournal = {JASTT},
  volume = {1},
  number = {4},
  pages = {140--147},
  issn = {2708-0757},
  doi = {10.38094/jastt1457},
  url = {https://www.jastt.org/index.php/jasttpath/article/view/57},
  urldate = {2023-06-19},
  abstract = {Perhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear regression is used to find a linear relationship between one or more predictors. The linear regression has two types: simple regression and multiple regression (MLR). This paper discusses various works by different researchers on linear regression and polynomial regression and compares their performance using the best approach to optimize prediction and precision. Almost all of the articles analyzed in this review is focused on datasets; in order to determine a model's efficiency, it must be correlated with the actual values obtained for the explanatory variables.},
  langid = {english},
  keywords = {\_tablet},
  file = {/Users/mavi/Zotero/storage/YZ7L5DLR/Maulud_Abdulazeez_2020_A Review on Linear Regression Comprehensive in Machine Learning.pdf}
}

@report{mazo-molinaPtr1LocusSolanum2019,
  type = {preprint},
  title = {The {{{\emph{Ptr1}}}} Locus of {{{\emph{Solanum}}}}{\emph{ Lycopersicoide}} s Confers Resistance to Race 1 Strains of {{{\emph{Pseudomonas}}}}{\emph{ Syringae}} Pv. Tomato and to {{{\emph{Ralstonia}}}}{\emph{ Pseudosolanacearum}} by Recognizing the Type {{III}} Effectors {{AvrRpt2}}/{{RipBN}}: {{Suplemental Information}}},
  shorttitle = {The {{{\emph{Ptr1}}}} Locus of {{{\emph{Solanum}}}}{\emph{ Lycopersicoide}} s Confers Resistance to Race 1 Strains of {{{\emph{Pseudomonas}}}}{\emph{ Syringae}} Pv. Tomato and to {{{\emph{Ralstonia}}}}{\emph{ Pseudosolanacearum}} by Recognizing the Type {{III}} Effectors {{AvrRpt2}}/{{RipBN}}},
  author = {Mazo-Molina, Carolina and Mainiero, Samantha and Hind, Sarah R and Kraus, Christine M and Vachev, Mishi and Maviane Macia, Felicià and Lindeberg, Magdalen and Saha, Surya and Strickler, Susan R and Feder, Ari and Giovannoni, James J and Smart, Christine D and Peeters, Nemo and Martin, Gregory B},
  date = {2019-01-11},
  institution = {{Plant Biology}},
  doi = {10.1101/518399},
  url = {http://biorxiv.org/lookup/doi/10.1101/518399},
  urldate = {2019-09-19},
  abstract = {Race 1 strains of             Pseudomonas syringae             pv. tomato, which causes bacterial speck disease of tomato, are becoming increasingly common and no simply-inherited genetic resistance to such strains is known. We discovered that a locus in             Solanum lycopersicoides             , termed             Pseudomonas tomato race 1 (Ptr1)             , confers resistance to race 1             Pst             strains by recognizing the type III effector AvrRpt2. In Arabidopsis, AvrRpt2 degrades the RIN4 protein thereby activating             RPS2             -mediated immunity.             Ptr1             also recognized homologs of AvrRpt2 from diverse bacteria including one in             Ralstonia pseudosolanacearum             and this correlated with the ability of AvrRpt2 to degrade RIN4. Using site-directed mutagenesis of AvrRpt2 we found that             Ptr1             and             RPS2             recognize identical features of AvrRpt2. However, the genome sequence of             S. lycopersicoides             revealed no             RPS2             homolog in the             Ptr1             region.             Ptr1             could play an important role in controlling bacterial speck disease and its future cloning may shed light on an example of convergent evolution for recognition of a widespread type III effector.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/Y9S9Y2VL/Mazo-Molina et al. - 2019 - The iPtr1i locus of iSolanum lycopersicoide.pdf}
}

@article{melkiExploratoryAnalysisPixelwise2022,
  title = {Exploratory {{Analysis}} on {{Pixelwise Image Segmentation Metrics}} with an {{Application}} in {{Proximal Sensing}}},
  author = {Melki, Paul and Bombrun, Lionel and Millet, Estelle and Diallo, Boubacar and ElChaoui ElGhor, Hakim and Da Costa, Jean-Pierre},
  date = {2022-02-18},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {14},
  number = {4},
  pages = {996},
  issn = {2072-4292},
  doi = {10.3390/rs14040996},
  url = {https://www.mdpi.com/2072-4292/14/4/996},
  urldate = {2022-05-16},
  abstract = {A considerable number of metrics can be used to evaluate the performance of machine learning algorithms. While much work is dedicated to the study and improvement of data quality and models’ performance, much less research is focused on the study of these evaluation metrics, their intrinsic relationship, the interplay of the influence among the metrics, the models, the data, and the environments and conditions in which they are to be applied. While some works have been conducted on general machine learning tasks such as classification, fewer efforts have been dedicated to more complex problems such as object detection and image segmentation, in which the evaluation of performance can vary drastically depending on the objectives and domains of application. Working in an agricultural context, specifically on the problem of the automatic detection of plants in proximal sensing images, we studied twelve evaluation metrics that we used to evaluate three image segmentation models recently presented in the literature. After a unified presentation of these metrics, we carried out an exploratory analysis of their relationships using a correlation analysis, a clustering of variables, and two factorial analyses (namely principal component analysis and multiple factorial analysis). We distinguished three groups of highly linked metrics and, through visual inspection of the representative images of each group, identified the aspects of segmentation that each group evaluates. The aim of this exploratory analysis was to provide some clues to practitioners for understanding and choosing the metrics that are most relevant to their agricultural task.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/XBQU5M8B/Melki et al. - 2022 - Exploratory Analysis on Pixelwise Image Segmentati.pdf}
}

@article{mendozabeltranMappingDirectN2O2022,
  title = {Mapping Direct {{N2O}} Emissions from Peri-Urban Agriculture: {{The}} Case of the {{Metropolitan Area}} of {{Barcelona}}},
  shorttitle = {Mapping Direct {{N2O}} Emissions from Peri-Urban Agriculture},
  author = {Mendoza Beltran, Angelica and Jepsen, Kelzy and Rufí-Salís, Martí and Ventura, Sergi and Madrid Lopez, Cristina and Villalba, Gara},
  date = {2022-05-20},
  journaltitle = {Science of The Total Environment},
  shortjournal = {Science of The Total Environment},
  volume = {822},
  pages = {153514},
  issn = {0048-9697},
  doi = {10.1016/j.scitotenv.2022.153514},
  url = {https://www.sciencedirect.com/science/article/pii/S0048969722006064},
  urldate = {2022-09-29},
  abstract = {Geographically explicit datasets reflecting local management of crops are needed to help improve direct nitrous oxide (N2O) emission inventories. Yet, the lack of geographically explicit datasets of relevant factors influencing the emissions make it difficult to estimate them in such way. Particularly, for local peri-urban agriculture, spatially explicit datasets of crop type, fertilizer use, irrigation, and emission factors (EFs) are hard to find, yet necessary for evaluating and promoting urban self-sufficiency, resilience, and circularity. We spatially distribute these factors for the peri-urban agriculture in the Metropolitan Area of Barcelona (AMB) and create N2O emissions maps using crop-specific EFs as well as Tier 1 IPCC EFs for comparison. Further, the role of the soil types is qualitatively assessed. When compared to Tier 1 IPCC EFs, we find 15\% more emissions (i.e. 7718 kg N2O-N year−1) than those estimated with the crop-specific EFs (i.e. 6533 kg N2O-N year−1) for the entire AMB. Emissions for most rainfed crop areas like cereals (e.g. oat and barley) and non-citric fruits (e.g. cherries and peaches), which cover 24\% and 13\% of AMB's peri-urban agricultural area respectively, are higher with Tier 1 EF. Conversely, crop-specific EFs estimate higher emissions for irrigated horticultural crops (e.g. tomato, artichoke) which cover 33\% of AMB's peri-urban agricultural area and make up 70\% of the total N2O emissions (4588 kg N2O-N year−1 using crop-specific EFs). Mapping the emissions helps evaluate spatial variability of key factors such as fertilizer use and irrigation of crops but carry uncertainties due to downscaling regional data to represent urban level data gaps. It also highlighted core emitting areas. Further the usefulness of the outputs on mitigation, sustainability and circularity studies are briefly discussed.},
  langid = {english},
  keywords = {Cities,GHG emissions maps,GIS,Local food,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/R685PIPS/Mendoza Beltran et al. - 2022 - Mapping direct N2O emissions from peri-urban agric.pdf;/Users/mavi/Zotero/storage/AXBDE9R3/S0048969722006064.html}
}

@online{miliotoRealtimeSemanticSegmentation2018,
  title = {Real-Time {{Semantic Segmentation}} of {{Crop}} and {{Weed}} for {{Precision Agriculture Robots Leveraging Background Knowledge}} in {{CNNs}}},
  author = {Milioto, Andres and Lottes, Philipp and Stachniss, Cyrill},
  date = {2018-03-02},
  eprint = {1709.06764},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1709.06764},
  urldate = {2022-05-16},
  abstract = {Precision farming robots, which target to reduce the amount of herbicides that need to be brought out in the fields, must have the ability to identify crops and weeds in real time to trigger weeding actions. In this paper, we address the problem of CNN-based semantic segmentation of crop fields separating sugar beet plants, weeds, and background solely based on RGB data. We propose a CNN that exploits existing vegetation indexes and provides a classification in real time. Furthermore, it can be effectively re-trained to so far unseen fields with a comparably small amount of training data. We implemented and thoroughly evaluated our system on a real agricultural robot operating in different fields in Germany and Switzerland. The results show that our system generalizes well, can operate at around 20 Hz, and is suitable for online operation in the fields.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/mavi/Zotero/storage/573BI2BZ/Milioto et al. - 2018 - Real-time Semantic Segmentation of Crop and Weed f.pdf}
}

@article{mohantyUsingDeepLearning2016,
  title = {Using {{Deep Learning}} for {{Image-Based Plant Disease Detection}}},
  author = {Mohanty, Sharada P. and Hughes, David P. and Salathé, Marcel},
  date = {2016-09-22},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {7},
  pages = {1419},
  issn = {1664-462X},
  doi = {10.3389/fpls.2016.01419},
  url = {http://journal.frontiersin.org/article/10.3389/fpls.2016.01419/full},
  urldate = {2022-07-04},
  abstract = {Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35\% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/R29SFBP2/Mohanty et al. - 2016 - Using Deep Learning for Image-Based Plant Disease .pdf}
}

@online{monnierDeepTransformationInvariantClustering2020,
  title = {Deep {{Transformation-Invariant Clustering}}},
  author = {Monnier, Tom and Groueix, Thibault and Aubry, Mathieu},
  date = {2020-10-27},
  eprint = {2006.11132},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.11132},
  urldate = {2023-02-22},
  abstract = {Recent advances in image clustering typically focus on learning better deep representations. In contrast, we present an orthogonal approach that does not rely on abstract features but instead learns to predict image transformations and performs clustering directly in image space. This learning process naturally fits in the gradient-based training of K-means and Gaussian mixture model, without requiring any additional loss or hyper-parameters. It leads us to two new deep transformation-invariant clustering frameworks, which jointly learn prototypes and transformations. More specifically, we use deep learning modules that enable us to resolve invariance to spatial, color and morphological transformations. Our approach is conceptually simple and comes with several advantages, including the possibility to easily adapt the desired invariance to the task and a strong interpretability of both cluster centers and assignments to clusters. We demonstrate that our novel approach yields competitive and highly promising results on standard image clustering benchmarks. Finally, we showcase its robustness and the advantages of its improved interpretability by visualizing clustering results over real photograph collections.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/79PEQA5P/Monnier et al_2020_Deep Transformation-Invariant Clustering.pdf;/Users/mavi/Zotero/storage/WIZAL7K7/2006.html}
}

@online{nagLookaheadOptimizerImproves2020,
  title = {Lookahead Optimizer Improves the Performance of {{Convolutional Autoencoders}} for Reconstruction of Natural Images},
  author = {Nag, Sayan},
  date = {2020-12-02},
  eprint = {2012.05694},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/2012.05694},
  urldate = {2023-02-01},
  abstract = {Autoencoders are a class of artificial neural networks which have gained a lot of attention in the recent past. Using the encoder block of an autoencoder the input image can be compressed into a meaningful representation. Then a decoder is employed to reconstruct the compressed representation back to a version which looks like the input image. It has plenty of applications in the field of data compression and denoising. Another version of Autoencoders (AE) exist, called Variational AE (VAE) which acts as a generative model like GAN. Recently, an optimizer was introduced which is known as lookahead optimizer which significantly enhances the performances of Adam as well as SGD. In this paper, we implement Convolutional Autoencoders (CAE) and Convolutional Variational Autoencoders (CVAE) with lookahead optimizer (with Adam) and compare them with the Adam (only) optimizer counterparts. For this purpose, we have used a movie dataset comprising of natural images for the former case and CIFAR100 for the latter case. We show that lookahead optimizer (with Adam) improves the performance of CAEs for reconstruction of natural images.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,{Physics - Data Analysis, Statistics and Probability}},
  file = {/Users/mavi/Zotero/storage/68YMY7ZU/Nag - 2020 - Lookahead optimizer improves the performance of Co.pdf;/Users/mavi/Zotero/storage/LIKPSJ9E/2012.html}
}

@article{nagrechaHydraOptimizedData,
  title = {Hydra: {{An Optimized Data Systemfor Large Multi-Model Deep Learning}}},
  author = {Nagrecha, Kabir and Kumar, Arun},
  abstract = {In many deep learning (DL) applications, the desire for ever higher accuracy and the new ubiquity of transfer learning has led to a marked increase in the size and depth of model architectures. Thus, the memory capacity of GPUs is often a bottleneck for DL practitioners. Existing techniques that rely on partitioning the model architecture across a network of GPUs suffer from substantial underutilization and busy waiting due to sequential dependencies in most large-scale model architectures (Transformers, CNNs). We observe that almost all such prior large-model systems focus on training only one model at a time, but in reality DL practitioners often train many models in bulk due to model selection needs, e.g., hyper-parameter tuning, architecture finetuning, etc. This gap leads to significant system inefficiency. We approach this problem from first principles and propose a new information system architecture for scalable multi-model training that adapts and blends ideas from classical RDBMS design with task parallelism from the ML world. We propose a suite of techniques to optimize system efficiency holistically, including a highly general parameter-spilling design that enables large models to be trained even with a single GPU, a novel multi-query optimization scheme that blends model execution schedules efficiently and maximizes GPU utilization, and a double buffering idea to hide latency. We prototype our ideas on top of PyTorch to build a system we call Hydra. Experiments with real benchmark large-scale multi-model DL workloads show that Hydra is over 7x faster than regular model parallelism and 1.8-4.5X faster than state-of-the-art industrial tools for large-scale model training.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/Users/mavi/Zotero/storage/2ZQ68CFP/Nagrecha and Kumar - Hydra An Optimized Data Systemfor Large Multi-Mod.pdf}
}

@article{newellEcosystemServicesUrban2022,
  title = {Ecosystem Services of Urban Agriculture and Prospects for Scaling up Production: {{A}} Study of {{Detroit}}},
  shorttitle = {Ecosystem Services of Urban Agriculture and Prospects for Scaling up Production},
  author = {Newell, Joshua P. and Foster, Alec and Borgman, Mariel and Meerow, Sara},
  date = {2022-06-01},
  journaltitle = {Cities},
  shortjournal = {Cities},
  volume = {125},
  pages = {103664},
  issn = {0264-2751},
  doi = {10.1016/j.cities.2022.103664},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275122001032},
  urldate = {2022-09-29},
  abstract = {Urban agriculture provides a range of ecosystem services (as well as potential disservices). This study examines the spatial extent, physical characteristics, and residents' perceptions of community and private gardens in Detroit, a city that has high potential for agricultural development given its abundant vacant and abandoned land. Despite popular narratives of Detroit as a mecca for urban agriculture, spatial analysis of the city's Lower Eastside (\textasciitilde 15 sq. miles) reveals that gardens cover less than 1\% of the vacant land and are often an ephemeral form of land use. Interviews indicate that residents plant gardens primarily for the cultural ecosystem services (e.g. social cohesion, community building) they provide and secondarily for provisioning services (i.e. food production). Uncertainty over land tenure, legacy environmental pollutants, unknowns regarding potential ecosystem disservices, and lack of government support and capital investment are the primary obstacles to scaling up urban agriculture in Detroit and other cities and will need to be addressed. To facilitate its expansion, we propose that urban agriculture be framed as a form of multifunctional green infrastructure. We conducted GIS-based analysis to identify suitable parcels for scaling up agriculture in the study area. To maximize the distribution of ecosystem service benefits, our modeling recommends dispersing rather than clustering gardens in the urban landscape. This strategy would provide more benefits to more people while countering the gentrification effects that may occur when cities expand green space.},
  langid = {english},
  keywords = {Detroit,Ecosystem services,Environmental justice,Equity,Green infrastructure,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/PCDVI5ZI/Newell et al. - 2022 - Ecosystem services of urban agriculture and prospe.pdf;/Users/mavi/Zotero/storage/FRB6L7SH/S0264275122001032.html}
}

@article{nicollAnchorageConiferousTrees2006,
  title = {Anchorage of Coniferous Trees in Relation to Species, Soil Type, and Rooting Depth},
  author = {Nicoll, Bruce C and Gardiner, Barry A and Rayner, Bill and Peace, Andrew J},
  date = {2006-07-01},
  journaltitle = {Canadian Journal of Forest Research},
  shortjournal = {Can. J. For. Res.},
  volume = {36},
  number = {7},
  pages = {1871--1883},
  issn = {0045-5067, 1208-6037},
  doi = {10.1139/x06-072},
  url = {http://www.nrcresearchpress.com/doi/10.1139/x06-072},
  urldate = {2022-09-30},
  abstract = {A database was constructed of tree-anchorage measurements from almost 2000 trees from 12 conifer species that were mechanically overturned on 34 sites in the United Kingdom between 1960 and 2000. Anchorage was compared among species, soil groups (freely-draining mineral, gleyed mineral, peaty mineral, and deep peat) and root depth classes (shallow, {$<$}40 cm; medium, 40–80 cm; and deep, {$>$}80 cm) using regressions of critical turning moment against stem mass. Sitka spruce (Picea sitchensis (Bong.) Carr.) was used as a benchmark because it formed the largest part of the database and was the only species with all soil-group and depth-class combinations. Anchorage of Sitka spruce was strongest on peat and poorest on gleyed mineral soils. Deep rooting increased critical turning moments by 10\%–15\% compared with trees of equivalent mass with shallower roots. Significantly better anchorage than Sitka spruce was found for grand fir (Abies grandis (Dougl. ex D. Don) Lindl.), with various rooting depths on freely draining and gleyed mineral soils and for Douglas-fir (Pseudotsuga menziesii (Mirb.) Franco) on medium-depth mineral soil. Lodgepole pine (Pinus contorta Dougl. ex Loud.) had poorer anchorage than Sitka spruce over a range of soil groups and root depth classes. Norway spruce (Picea abies (L.) Karst.) on shallow gleyed mineral soil, and Corsican pine (Pinus nigra subsp. laricio (Poir.) Maire) on medium depth mineral soil, also had poorer anchorage. Other combinations had similar anchorage to the equivalent Sitka spruce. These results are discussed with respect to the development of forest wind-risk models.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/NJ3WIH4T/Nicoll et al. - 2006 - Anchorage of coniferous trees in relation to speci.pdf}
}

@article{noonUseDeepLearning2020,
  title = {Use of Deep Learning Techniques for Identification of Plant Leaf Stresses: {{A}} Review},
  shorttitle = {Use of Deep Learning Techniques for Identification of Plant Leaf Stresses},
  author = {Noon, Serosh Karim and Amjad, Muhammad and Qureshi, Muhammad Ali and Mannan, Abdul},
  date = {2020-12},
  journaltitle = {Sustainable Computing: Informatics and Systems},
  shortjournal = {Sustainable Computing: Informatics and Systems},
  volume = {28},
  pages = {100443},
  issn = {22105379},
  doi = {10.1016/j.suscom.2020.100443},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2210537920301700},
  urldate = {2022-05-16},
  abstract = {The use of deep networks in agriculture has increased enormously in the last decade including their use to classify different plant leaf stresses. More recently, a large number of deep learning-based approaches for plant leaf stress identification have been proposed in literature but there are only a few partial efforts to summarize different contributions. Hence, there is a dire need of a detailed survey compiling techniques used for identifi­ cation of leaf stresses found in a variety of plants. This work presents a review of 45 deep learning-based techniques recently proposed for 33 different crops using 14 famous convolutional neural network architec­ tures. The techniques reviewed were divided in vegetables, fruits and other crops on the basis of stress type, size of dataset, training/test size and the deep network used. The effort will facilitate researchers especially those who are new in this field to get a quick introduction of the trend on using deep learning in plant leaf stress identification.},
  langid = {english},
  keywords = {read,REview},
  file = {/Users/mavi/Zotero/storage/Y6BIDSSU/Noon et al. - 2020 - Use of deep learning techniques for identification.pdf}
}

@article{ojeda-martinezSavingTimeMaintaining2020,
  title = {Saving Time Maintaining Reliability: A New Method for Quantification of {{Tetranychus}} Urticae Damage in {{Arabidopsis}} Whole Rosettes},
  shorttitle = {Saving Time Maintaining Reliability},
  author = {Ojeda-Martinez, Dairon and Martinez, Manuel and Diaz, Isabel and Santamaria, M. Estrella},
  date = {2020-08-27},
  journaltitle = {BMC Plant Biology},
  shortjournal = {BMC Plant Biology},
  volume = {20},
  number = {1},
  pages = {397},
  issn = {1471-2229},
  doi = {10.1186/s12870-020-02584-0},
  url = {https://doi.org/10.1186/s12870-020-02584-0},
  urldate = {2022-10-21},
  abstract = {The model species Tetranychus urticae produces important plant injury and economic losses in the field. The current accepted method for the quantification of the spider mite damage in Arabidopsis whole rosettes is time consuming and entails a bottleneck for large-scale studies such as mutant screening or quantitative genetic analyses. Here, we describe an improved version of the existing method by designing an automatic protocol. The accuracy, precision, reproducibility and concordance of the new enhanced approach are validated in two Arabidopsis accessions with opposite damage phenotypes. Results are compared to the currently available manual method.},
  keywords = {Arabidopsis thaliana,Assess,Chlorotic spots,CompuEye,Fiji,Ilastik,Machine learning,Photoshop,Plant damage quantification,Tetranychus urticae},
  file = {/Users/mavi/Zotero/storage/CSUA7PQH/Ojeda-Martinez et al. - 2020 - Saving time maintaining reliability a new method .pdf;/Users/mavi/Zotero/storage/YCB7EF84/s12870-020-02584-0.html}
}

@online{oordNeuralDiscreteRepresentation2018,
  title = {Neural {{Discrete Representation Learning}}},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=false and Vinyals, Oriol and Kavukcuoglu, Koray},
  date = {2018-05-30},
  eprint = {1711.00937},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.00937},
  url = {http://arxiv.org/abs/1711.00937},
  urldate = {2022-12-25},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/DJDQANNN/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf;/Users/mavi/Zotero/storage/UTIK5QT4/1711.html}
}

@article{ouyangLandSpaceOptimization2022,
  title = {Land Space Optimization of Urban-Agriculture-Ecological Functions in the {{Changsha-Zhuzhou-Xiangtan Urban Agglomeration}}, {{China}}},
  author = {Ouyang, Xiao and Xu, Jun and Li, Jiayu and Wei, Xiao and Li, Yonghui},
  date = {2022-06-01},
  journaltitle = {Land Use Policy},
  shortjournal = {Land Use Policy},
  volume = {117},
  pages = {106112},
  issn = {0264-8377},
  doi = {10.1016/j.landusepol.2022.106112},
  url = {https://www.sciencedirect.com/science/article/pii/S0264837722001399},
  urldate = {2022-09-29},
  abstract = {Evaluating urban-agriculture-ecological space is useful in optimizing land space and high-quality socio-economic development. This study evaluated urban development and ecological protection in an urban agglomeration and delineated the ecological security pattern for the sustainable use of land spaces. Using the Changsha-Zhuzhou-Xiangtan (CZT) urban agglomeration as study area, system dynamics (SD) model, cellular automaton (CA), and artificial neural network (ANN) were utilized to simulate the variation trends of urban development space, ecological space, and urban agricultural space for 2019–2035 under different development scenarios (status-quo, economic-centric, environment-centric, and coordinated equilibrium). The highlights of results are as follows: (1) The combined effects of the economy, nature, accessibility, and spillover effects exerted significant impact on urban land expansion. Socio-economic variables and spillover effect constitute the main controlling factors of land space expansion. (2) The urban construction space of the CZT urban agglomeration would vary for the different development scenarios. During the simulation period, urban construction space in the study area would be 1572.06~km2 for the status-quo scenario, 1763.27~km2 for the economic-centric scenario, 1634.13~km2 for the environment-centric scenario, and 1668.69~km2 for the coordinated equilibrium scenario. (3) The growth boundary of the urban agglomeration was demarcated based on the spatial attributes, land-use functions, and clustering. The rigid growth boundary was estimated at 3686~km2, while the flexible growth boundary was estimated to be 1794.53~km2. These values can be used to guide planning, policies, and strategies for the orderly expansion of urban land in future development.},
  langid = {english},
  keywords = {Ecological security pattern,Land space,Urban agglomeration,Urban-agriculture-ecological functions},
  file = {/Users/mavi/Zotero/storage/RCI7YSVG/Ouyang et al. - 2022 - Land space optimization of urban-agriculture-ecolo.pdf;/Users/mavi/Zotero/storage/HWFV3D2E/S0264837722001399.html}
}

@article{ozguvenAutomaticDetectionClassification2019,
  title = {Automatic Detection and Classification of Leaf Spot Disease in Sugar Beet Using Deep Learning Algorithms},
  author = {Ozguven, Mehmet Metin and Adem, Kemal},
  date = {2019-12},
  journaltitle = {Physica A: Statistical Mechanics and its Applications},
  shortjournal = {Physica A: Statistical Mechanics and its Applications},
  volume = {535},
  pages = {122537},
  issn = {03784371},
  doi = {10.1016/j.physa.2019.122537},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437119314529},
  urldate = {2022-05-17},
  abstract = {Depending on the severity of the leaf spot disease in the field, it can cause a loss in sugar yield by 10\% to 50\%. Therefore, disease symptoms should be detected ontime and relevant measures should be taken instantly to prevent further spread or progress of the disease. In this study, an Updated Faster R-CNN architecture developed by changing the parameters of a CNN model and a Faster R-CNN architecture for automatic detection of leaf spot disease (Cercospora beticola Sacc.) in sugar beet were proposed. The method, proposed for the detection of disease severity by imaging-based expert systems, was trained and tested with 155 images and according to the test results, the overall correct classification rate was found to be 95.48\%. In addition, the proposed approach showed that changes in CNN parameters according to the image and regions to be detected could increase the success of Faster R-CNN architecture. The proposed approach yielded better outcomes for relevant parameters than the modern methods specified in previous literature. Therefore, it is believed that the method will reduce the time spent in diagnosis of sugar beet leaf spot disease in the large production areas as well as reducing the human error and time to identify the severity and course of the disease.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/6JFJNAP9/Ozguven and Adem - 2019 - Automatic detection and classification of leaf spo.pdf}
}

@online{pandeyDiffuseVAEEfficientControllable2022,
  title = {{{DiffuseVAE}}: {{Efficient}}, {{Controllable}} and {{High-Fidelity Generation}} from {{Low-Dimensional Latents}}},
  shorttitle = {{{DiffuseVAE}}},
  author = {Pandey, Kushagra and Mukherjee, Avideep and Rai, Piyush and Kumar, Abhishek},
  date = {2022-11-29},
  eprint = {2201.00308},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.00308},
  urldate = {2023-01-13},
  abstract = {Diffusion probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, standard Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design novel conditional parameterizations for diffusion models. We show that the resulting model equips diffusion models with a low-dimensional VAE inferred latent code which can be used for downstream tasks like controllable synthesis. The proposed method also improves upon the speed vs quality tradeoff exhibited in standard unconditional DDPM/DDIM models (for instance, FID of 16.47 vs 34.36 using a standard DDIM on the CelebA-HQ-128 benchmark using T=10 reverse process steps) without having explicitly trained for such an objective. Furthermore, the proposed model exhibits synthesis quality comparable to state-of-the-art models on standard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent generalization to different types of noise in the conditioning signal. For reproducibility, our source code is publicly available at https://github.com/kpandey008/DiffuseVAE.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/UZ3ZTCNV/Pandey et al. - 2022 - DiffuseVAE Efficient, Controllable and High-Fidel.pdf;/Users/mavi/Zotero/storage/UIYT82YR/2201.html}
}

@article{pandeySharedUniqueResponses2015,
  title = {Shared and Unique Responses of Plants to Multiple Individual Stresses and Stress Combinations: Physiological and Molecular Mechanisms},
  shorttitle = {Shared and Unique Responses of Plants to Multiple Individual Stresses and Stress Combinations},
  author = {Pandey, Prachi and Ramegowda, Venkategowda and Senthil-Kumar, Muthappa},
  date = {2015},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {he},
  volume = {6},
  eprint = {26442037},
  eprinttype = {pmid},
  pages = {723},
  issn = {1664-462X},
  doi = {10.3389/fpls.2015.00723},
  abstract = {In field conditions, plants are often simultaneously exposed to multiple biotic and abiotic stresses resulting in substantial yield loss. Plants have evolved various physiological and molecular adaptations to protect themselves under stress combinations. Emerging evidences suggest that plant responses to a combination of stresses are unique from individual stress responses. In addition, plants exhibit shared responses which are common to individual stresses and stress combination. In this review, we provide an update on the current understanding of both unique and shared responses. Specific focus of this review is on heat-drought stress as a major abiotic stress combination and, drought-pathogen and heat-pathogen as examples of abiotic-biotic stress combinations. We also comprehend the current understanding of molecular mechanisms of cross talk in relation to shared and unique molecular responses for plant survival under stress combinations. Thus, the knowledge of shared responses of plants from individual stress studies and stress combinations can be utilized to develop varieties with broad spectrum stress tolerance.},
  langid = {english},
  pmcid = {PMC4584981},
  keywords = {concurrent stress,drought,heat,pathogen infection,tailored response,unique adaptation mechanisms},
  file = {/Users/mavi/Zotero/storage/DUH7IXP8/Pandey et al. - 2015 - Shared and unique responses of plants to multiple .pdf}
}

@online{PapersCodeGenerating,
  title = {Papers with {{Code}} - {{Generating Diverse High-Fidelity Images}} with {{VQ-VAE-2}}},
  url = {https://paperswithcode.com/paper/190600446},
  urldate = {2023-02-22},
  abstract = {Implemented in 15 code libraries.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/XL758L2M/190600446.html}
}

@misc{papertSummerVisionProject1966,
  title = {The {{Summer Vision Project}}},
  author = {Papert, Seymour},
  date = {1966},
  url = {https://people.csail.mit.edu/brooks/idocs/AIM-100.pdf},
  urldate = {2023-05-05},
  langid = {american},
  file = {/Users/mavi/Zotero/storage/7TRB2T8R/AIM-100.pdf}
}

@inproceedings{parkImprovingUnsupervisedImage2021,
  title = {Improving {{Unsupervised Image Clustering With Robust Learning}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Park, Sungwon and Han, Sungwon and Kim, Sundong and Kim, Danu and Park, Sungkyu and Hong, Seunghoon and Cha, Meeyoung},
  date = {2021-06},
  pages = {12273--12282},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01210},
  url = {https://ieeexplore.ieee.org/document/9578903/},
  urldate = {2022-10-27},
  abstract = {Unsupervised image clustering methods often introduce alternative objectives to indirectly train the model and are subject to faulty predictions and overconfident results. To overcome these challenges, the current research proposes an innovative model RUC that is inspired by robust learning. RUC’s novelty is at utilizing pseudo-labels of existing image clustering models as a noisy dataset that may include misclassified samples. Its retraining process can revise misaligned knowledge and alleviate the overconfidence problem in predictions. The model’s flexible structure makes it possible to be used as an add-on module to other clustering methods and helps them achieve better performance on multiple datasets. Extensive experiments show that the proposed model can adjust the model confidence with better calibration and gain additional robustness against adversarial noise.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/2ZHGUY8D/Park et al. - 2021 - Improving Unsupervised Image Clustering With Robus.pdf}
}

@inproceedings{passoMolecularGeneticsInteractions2016,
  title = {Molecular Genetics of Interactions between {{Xanthomonas}} Campestris Pv. Raphani and {{Arabidopsis}} Thaliana.},
  author = {Passo, V.},
  date = {2016-08-01},
  url = {https://www.semanticscholar.org/paper/Molecular-genetics-of-interactions-between-pv.-and-Passo/581e4020613d7356a9097f6c34151ce2f702f677},
  urldate = {2023-05-03},
  abstract = {The major aim of this research was to investigate interactions between the bacterial pathogen Xanthomonas campestris pv. raphani (Xcr) and Arabidopsis thaliana which are largely unexplored as a model pathosystem, and to identify genetic loci conferring resistance to this pathogen. Xcr is genetically close to X. campestris pv. campestris (Xcc) and both pathogens infect common Brassicaceae species including A. thaliana, but cause distinct diseases - leaf spot and black rot, respectively.    Phenotypic variation was identified among interactions between 22 A. thaliana accessions of wide geographic origin and Xcr and Xcc strains representing known host genotype specific races. Accessions were identified showing broad resistance and broad susceptibility to multiple Xcr and Xcc strains as well as accessions that differentiate between Xcr races.    Genetic mapping of resistance to a strain of Xcr race 2 in a multi-parent recombinant inbred population revealed two major effect loci: RXCR1 at the bottom arm of chromosome 3, and RXCR2 at the bottom arm of chromosome 5. RXCR1 was confirmed by fine-mapping and loss- and gain-of-function experiments, as a single gene encoding a kinase-like protein conferring resistance in the accession Columbia. Columbia is resistant to strains of all three known Xcr races, but RXCR1 is insufficient on its own to explain the broad resistance. A summary of phenotypic and genetic analyses of interactions between Xcr and A. thaliana is presented in a gene-for-gene model.    X. campestris strains associated with outbreaks of a leaf spot and blight disease of brassica crops in Mauritius, were characterized. These strains were similar to reference Xcc strains in pathogenicity tests and molecular analyses. The presence of Xcr in these outbreaks was not confirmed.    Whole-genome sequencing data was used to identify genes that may contribute to the distinct modes of pathogenesis of Xcr and Xcc as well as variation in host specific races within each pathovar. Genes differentially present/absent between nine Xcr and 23 Xcc strains were identified and include genes predicted/known to encode type III effectors some of which have been previously described to have an effect on Xcc pathogenicity. Candidate avirulence determinants of Xcr and Xcc races were also identified.},
  file = {/Users/mavi/Zotero/storage/YJD7S5YC/Passo_2016_Molecular genetics of interactions between Xanthomonas campestris pv.pdf}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate = {2023-04-28},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  keywords = {⛔ No DOI found,read},
  file = {/Users/mavi/Zotero/storage/C947VPGJ/Paszke et al_2019_PyTorch.pdf}
}

@article{pautassoTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Writing}} a {{Literature Review}}},
  author = {Pautasso, Marco},
  date = {2013-07-18},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {9},
  number = {7},
  pages = {e1003149},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003149},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003149},
  urldate = {2022-10-26},
  langid = {english},
  keywords = {Citation analysis,Computational biology,Database searching,Malaria,Obesity,Peer review,Scientists,Systematic reviews},
  file = {/Users/mavi/Zotero/storage/YAJPFDK2/Pautasso - 2013 - Ten Simple Rules for Writing a Literature Review.pdf;/Users/mavi/Zotero/storage/BZTEKH6W/article.html}
}

@article{pavicicImageBasedMethodsScore2021,
  title = {Image-{{Based Methods}} to {{Score Fungal Pathogen Symptom Progression}} and {{Severity}} in {{Excised Arabidopsis Leaves}}},
  author = {Pavicic, Mirko and Overmyer, Kirk and Rehman, Attiq and Jones, Piet and Jacobson, Dan and Himanen, Kristiina},
  date = {2021-01-15},
  journaltitle = {Plants},
  shortjournal = {Plants},
  volume = {10},
  pages = {158},
  doi = {10.3390/plants10010158},
  abstract = {Image-based symptom scoring of plant diseases is a powerful tool for associating disease resistance with plant genotypes. Advancements in technology have enabled new imaging and image processing strategies for statistical analysis of time-course experiments. There are several tools available for analyzing symptoms on leaves and fruits of crop plants, but only a few are available for the model plant Arabidopsis thaliana (Arabidopsis). Arabidopsis and the model fungus Botrytis cinerea (Botrytis) comprise a potent model pathosystem for the identification of signaling pathways conferring immunity against this broad host-range necrotrophic fungus. Here, we present two strategies to assess severity and symptom progression of Botrytis infection over time in Arabidopsis leaves. Thus, a pixel classification strategy using color hue values from red-green-blue (RGB) images and a random forest algorithm was used to establish necrotic, chlorotic, and healthy leaf areas. Secondly, using chlorophyll fluorescence (ChlFl) imaging, the maximum quantum yield of photosystem II (Fv/Fm) was determined to define diseased areas and their proportion per total leaf area. Both RGB and ChlFl imaging strategies were employed to track disease progression over time. This has provided a robust and sensitive method for detecting sensitive or resistant genetic backgrounds. A full methodological workflow, from plant culture to data analysis, is described.},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/HJPV9ZL9/Pavicic et al_2021_Image-Based Methods to Score Fungal Pathogen Symptom Progression and Severity.pdf}
}

@article{peltolaMechanisticModelAssessing1999,
  title = {A Mechanistic Model for Assessing the Risk of Wind and Snow Damage to Single Trees and Stands of {{Scots}} Pine, {{Norway}} Spruce, and Birch},
  author = {Peltola, H and Kellomäki, S and Väisänen, H and Ikonen, V -P},
  date = {1999-06-01},
  journaltitle = {Canadian Journal of Forest Research},
  shortjournal = {Can. J. For. Res.},
  volume = {29},
  number = {6},
  pages = {647--661},
  issn = {0045-5067, 1208-6037},
  doi = {10.1139/x99-029},
  url = {http://www.nrcresearchpress.com/doi/10.1139/x99-029},
  urldate = {2022-09-30},
  abstract = {A mechanistic model for assessing the risk of wind and snow damage to single trees and stands of Scots pine (Pinus sylvestris L.), Norway spruce (Picea abies (L.) Karst.), and birch (Betula spp.) is presented. The model predicts the critical turning moment and wind speed at which the trees will be uprooted or break at forest margins. The resistance to uprooting is predicted using the estimate of the root-soil plate weight to derive a resistive moment, while the resistance to stem breakage relies on values for the modulus of rupture determined for different species of timber. A tree is assumed to be uprooted if the total turning moment exceeds the support provided by the root-soil plate anchorage. Similarly, a tree is assumed to break if the breaking stress acting on the stem exceeds a critical value of the modulus of rupture. The model is in general quite sensitive to parameter changes, which partly results from the location in the forest to which it was designed to apply (the stand edge). The predictions of the critical turning moments needed to uproot and break trees nevertheless give a good agreement on average with the Finnish tree-pulling data for Scots pine, Norway spruce, and birch.},
  langid = {english}
}

@article{peltolaMechanisticModelCalculating1993,
  title = {A Mechanistic Model for Calculating Windthrow and Stem Breakage of {{Scots}} Pines at Stand Edge},
  author = {Peltola, H and Kellomaki, S},
  date = {1993},
  journaltitle = {Silva Fennica},
  volume = {27},
  pages = {99--111},
  doi = {10.14214/sf.a15665},
  file = {/Users/mavi/Zotero/storage/25SPQ84R/Peltola and Kellomaki - 1993 - A mechanistic model for calculating windthrow and .pdf}
}

@article{penaLifeCycleCost2022,
  title = {Life Cycle Cost Analysis of Tomato Production in Innovative Urban Agriculture Systems},
  author = {Peña, Alexandra and Rovira-Val, M. Rosa and Mendoza, Joan Manuel F.},
  date = {2022-09-20},
  journaltitle = {Journal of Cleaner Production},
  shortjournal = {Journal of Cleaner Production},
  volume = {367},
  pages = {133037},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2022.133037},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652622026294},
  urldate = {2022-09-29},
  abstract = {The construction of innovative urban agriculture systems in cities has increased due to food and environmental concerns. While the environmental performance of urban agriculture has been extensively studied, research on the life cycle costs of urban agriculture systems is still limited, which constraints sustainability-oriented decision-making processes. This paper analyses the economic viability of tomato production cycle in an innovative building with an integrated urban agriculture system in rooftop by applying the life cycle cost methodology. The data was collected from direct measurements and internal and external sources. To calculate labour costs, a customised data collection sheet was created. The results are presented by life cycle stage, cost category and type of cost (fixed \& variable). Results indicate that the main cost drivers for tomato production are labour (24.7\%), the rooftop greenhouse structure (15\%), external pest control (12.6\%), and rainwater consumption (9.5\%), accounting altogether for 61.8\% of the total costs. Accordingly, cost reduction solutions are evaluated through the development of sensitivity scenarios (rooftop greenhouse structure design, tap water use and rainwater tank size), including the consideration of another relevant aspect, such as the role of the production level output, as it can greatly influence the economic viability and profitability. Finally, the main environmental and social aspects of these urban production systems are also included.},
  langid = {english},
  keywords = {Economic viability,Food security,LCC,Sustainable cities,Urban agriculture,Urban food production},
  file = {/Users/mavi/Zotero/storage/B67K74SD/Peña et al. - 2022 - Life cycle cost analysis of tomato production in i.pdf;/Users/mavi/Zotero/storage/PSULQIEC/S0959652622026294.html}
}

@article{perez-valenciaTwostageApproachSpatiotemporal2022,
  title = {A Two-Stage Approach for the Spatio-Temporal Analysis of High-Throughput Phenotyping Data},
  author = {Pérez-Valencia, Diana M. and Rodríguez-Álvarez, María Xosé and Boer, Martin P. and Kronenberg, Lukas and Hund, Andreas and Cabrera-Bosquet, Llorenç and Millet, Emilie J. and family=Eeuwijk, given=Fred A., prefix=van, useprefix=false},
  date = {2022-12},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {3177},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-06935-9},
  url = {https://www.nature.com/articles/s41598-022-06935-9},
  urldate = {2022-05-31},
  abstract = {Abstract             High throughput phenotyping (HTP) platforms and devices are increasingly used for the characterization of growth and developmental processes for large sets of plant genotypes. Such HTP data require challenging statistical analyses in which longitudinal genetic signals need to be estimated against a background of spatio-temporal noise processes. We propose a two-stage approach for the analysis of such longitudinal HTP data. In a first stage, we correct for design features and spatial trends per time point. In a second stage, we focus on the longitudinal modelling of the spatially corrected data, thereby taking advantage of shared longitudinal features between genotypes and plants within genotypes. We propose a flexible hierarchical three-level P-spline growth curve model, with plants/plots nested in genotypes, and genotypes nested in populations. For selection of genotypes in a plant breeding context, we show how to extract new phenotypes, like growth rates, from the estimated genotypic growth curves and their first-order derivatives. We illustrate our approach on HTP data from the PhenoArch greenhouse platform at INRAE Montpellier and the outdoor Field Phenotyping platform at ETH Zürich.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/M88J2TLT/Pérez-Valencia et al. - 2022 - A two-stage approach for the spatio-temporal analy.pdf}
}

@online{perezFiLMVisualReasoning2017,
  title = {{{FiLM}}: {{Visual Reasoning}} with a {{General Conditioning Layer}}},
  shorttitle = {{{FiLM}}},
  author = {Perez, Ethan and Strub, Florian and family=Vries, given=Harm, prefix=de, useprefix=true and Dumoulin, Vincent and Courville, Aaron},
  date = {2017-12-18},
  eprint = {1709.07871},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1709.07871},
  urldate = {2022-05-19},
  abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning — answering image-related questions which require a multi-step, high-level process — a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-theart error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/GERRW5X9/Perez et al. - 2017 - FiLM Visual Reasoning with a General Conditioning.pdf}
}

@article{picekAutomaticFungiRecognition2022,
  title = {Automatic {{Fungi Recognition}}: {{Deep Learning Meets Mycology}}},
  shorttitle = {Automatic {{Fungi Recognition}}},
  author = {Picek, Lukáš and Šulc, Milan and Matas, Jiří and Heilmann-Clausen, Jacob and Jeppesen, Thomas S. and Lind, Emil},
  date = {2022-01-14},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {22},
  number = {2},
  pages = {633},
  issn = {1424-8220},
  doi = {10.3390/s22020633},
  url = {https://www.mdpi.com/1424-8220/22/2/633},
  urldate = {2022-05-25},
  abstract = {The article presents an AI-based fungi species recognition system for a citizen-science community. The system’s real-time identification too — FungiVision — with a mobile application front-end, led to increased public interest in fungi, quadrupling the number of citizens collecting data. FungiVision, deployed with a human-in-the-loop, reaches nearly 93\% accuracy. Using the collected data, we developed a novel fine-grained classification dataset — Danish Fungi 2020 (DF20) — with several unique characteristics: species-level labels, a small number of errors, and rich observation metadata. The dataset enables the testing of the ability to improve classification using metadata, e.g., time, location, habitat and substrate, facilitates classifier calibration testing and finally allows the study of the impact of the device settings on the classification performance. The continual flow of labelled data supports improvements of the online recognition system. Finally, we present a novel method for the fungi recognition service, based on a Vision Transformer architecture. Trained on DF20 and exploiting available metadata, it achieves a recognition error that is 46.75\% lower than the current system. By providing a stream of labeled data in one direction, and an accuracy increase in the other, the collaboration creates a virtuous cycle helping both communities.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/XC8QRCW6/Picek et al. - 2022 - Automatic Fungi Recognition Deep Learning Meets M.pdf}
}

@article{pingPotentialHealthRisk2022,
  title = {Potential Health Risk of Pesticide Residues in Greenhouse Vegetables under Modern Urban Agriculture: {{A}} Case Study in {{Beijing}}, {{China}}},
  shorttitle = {Potential Health Risk of Pesticide Residues in Greenhouse Vegetables under Modern Urban Agriculture},
  author = {Ping, Hua and Wang, Beihong and Li, Cheng and Li, Yang and Ha, Xuejiao and Jia, Wenshen and Li, Bingru and Ma, Zhihong},
  date = {2022-01-01},
  journaltitle = {Journal of Food Composition and Analysis},
  shortjournal = {Journal of Food Composition and Analysis},
  volume = {105},
  pages = {104222},
  issn = {0889-1575},
  doi = {10.1016/j.jfca.2021.104222},
  url = {https://www.sciencedirect.com/science/article/pii/S0889157521004221},
  urldate = {2022-09-29},
  abstract = {This study analyzed the residues of 24 pesticides in greenhouse vegetables grown in Beijing under modern urban agriculture and estimated the potential health risk of consuming these vegetables. The analysis was carried out using gas chromatography-mass spectrometry (GC–MS) and ultra-high-performance liquid chromatography-tandem mass spectrometry (UHPLC-MS/MS). 37.5 \% of the vegetables contained at least one pesticide and 1\% of the samples contained residues that exceeded the maximum residue limits set by the Chinese government. These values are lower than those in vegetables under traditional agriculture. The banned pesticides chlorpyrifos and phorate in Chinese cabbage and Chinese chive exceeded maximum residue limits, but the target hazard quotient and hazard index indicated that intake of these vegetables may not cause adverse health effects. Modern urban agriculture practices should focus on strict pesticide management to reduce the uptake of these chemicals by the plants. This study can provide basic data for the management of modern urban agriculture in China.},
  langid = {english},
  keywords = {Modern urban agriculture,Pesticide residues,QuEChERS,Target hazard quotient (THQ),Vegetable},
  file = {/Users/mavi/Zotero/storage/WMF8KX5I/Ping et al. - 2022 - Potential health risk of pesticide residues in gre.pdf;/Users/mavi/Zotero/storage/7LHQ65RC/S0889157521004221.html}
}

@article{possamaiPhenotypingQTLIdentification2022,
  title = {Phenotyping for {{QTL}} Identification: {{A}} Case Study of Resistance to {{Plasmopara}} Viticola and {{Erysiphe}} Necator in Grapevine},
  shorttitle = {Phenotyping for {{QTL}} Identification},
  author = {Possamai, Tyrone and Wiedemann-Merdinoglu, Sabine},
  date = {2022},
  journaltitle = {Frontiers in Plant Science},
  volume = {13},
  issn = {1664-462X},
  doi = {10.3389/fpls.2022.930954},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2022.930954},
  urldate = {2023-07-20},
  abstract = {Vitis vinifera is the most widely cultivated grapevine species. It is highly susceptible to Plasmopara viticola and Erysiphe necator, the causal agents of downy mildew (DM) and powdery mildew (PM), respectively. Current strategies to control DM and PM mainly rely on agrochemical applications that are potentially harmful to humans and the environment. Breeding for resistance to DM and PM in wine grape cultivars by introgressing resistance loci from wild Vitis spp. is a complementary and more sustainable solution to manage these two diseases. During the last two decades, 33 loci of resistance to P. viticola (Rpv) and 15 loci of resistance to E. necator (Ren and Run) have been identified. Phenotyping is salient for QTL characterization and understanding the genetic basis of resistant traits. However, phenotyping remains a major bottleneck for research on Rpv and Ren/Run loci and disease resistance evaluation. A thorough analysis of the literature on phenotyping methods used for DM and PM resistance evaluation highlighted phenotyping performed in the vineyard, greenhouse or laboratory with major sources of variation, such as environmental conditions, plant material (organ physiology and age), pathogen inoculum (genetic and origin), pathogen inoculation (natural or controlled), and disease assessment method (date, frequency, and method of scoring). All these factors affect resistance assessment and the quality of phenotyping data. We argue that the use of new technologies for disease symptom assessment, and the production and adoption of standardized experimental guidelines should enhance the accuracy and reliability of phenotyping data. This should contribute to a better replicability of resistance evaluation outputs, facilitate QTL identification, and contribute to streamline disease resistance breeding programs.},
  file = {/Users/mavi/Zotero/storage/BYAW8Y68/Possamai_Wiedemann-Merdinoglu_2022_Phenotyping for QTL identification.pdf}
}

@article{possamaiRpvMediatedDefense2020,
  title = {Rpv {{Mediated Defense Responses}} in {{Grapevine Offspring Resistant}} to {{Plasmopara}} Viticola},
  author = {Possamai, Tyrone and Migliaro, Daniele and Gardiman, Massimo and Velasco, Riccardo and De Nardi, Barbara},
  date = {2020-06},
  journaltitle = {Plants},
  volume = {9},
  number = {6},
  pages = {781},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2223-7747},
  doi = {10.3390/plants9060781},
  url = {https://www.mdpi.com/2223-7747/9/6/781},
  urldate = {2022-10-22},
  abstract = {Downy mildew, caused by the biotrophic oomycete Plasmopara viticola, is one of the most serious grapevine diseases. The development of new varieties, showing partial resistance to downy mildew, through traditional breeding provides a sustainable and effective solution for disease management. Marker-assisted-selection (MAS) provide fast and cost-effective genotyping methods, but phenotyping remains necessary to characterize the host–pathogen interaction and assess the effective resistance level of new varieties as well as to validate MAS selection. In this study, the Rpv mediated defense responses were investigated in 31 genotypes, encompassing susceptible and resistant varieties and 26 seedlings, following inoculation of leaf discs with P. viticola. The offspring differed in Rpv loci inherited (none, one or two): Rpv3-3 and Rpv10 from Solaris and Rpv3-1 and Rpv12 from Kozma 20-3. To improve the assessment of different resistance responses, pathogen reaction (sporulation) and host reaction (necrosis) were scored separately as independent features. They were differently expressed depending on Rpv locus: offspring carrying Rpv3-1 and Rpv12 loci showed the strongest resistance response (scarce sporulation and necrosis), those carrying Rpv3-3 locus showed the highest levels of necrosis while Rpv10 carrying genotypes showed intermediate levels of both sporulation and necrosis.},
  issue = {6},
  langid = {english},
  keywords = {\emph{Rpv},downy mildew,grape breeding,leaf discs,necrosis,pyramiding,sporulation},
  file = {/Users/mavi/Zotero/storage/HVTHABJ4/Possamai et al. - 2020 - Rpv Mediated Defense Responses in Grapevine Offspr.pdf;/Users/mavi/Zotero/storage/PF53P8FS/htm.html}
}

@online{preechakulDiffusionAutoencodersMeaningful2022,
  title = {Diffusion {{Autoencoders}}: {{Toward}} a {{Meaningful}} and {{Decodable Representation}}},
  shorttitle = {Diffusion {{Autoencoders}}},
  author = {Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},
  date = {2022-03-09},
  eprint = {2111.15640},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.15640},
  url = {http://arxiv.org/abs/2111.15640},
  urldate = {2023-01-06},
  abstract = {Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code, where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our project page: https://Diff-AE.github.io/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/ZI2X7KSQ/Preechakul et al. - 2022 - Diffusion Autoencoders Toward a Meaningful and De.pdf;/Users/mavi/Zotero/storage/ZRKJNZP7/2111.html}
}

@article{puppimdeoliveiraGovernanceUrbanAgriculture2021,
  title = {Governance of Urban Agriculture in {{African}} Cities: {{Gaps}} and Opportunities for Innovation in {{Accra}}, {{Ghana}}},
  shorttitle = {Governance of Urban Agriculture in {{African}} Cities},
  author = {Puppim de Oliveira, Jose A. and Ahmed, Abubakari},
  date = {2021-08-20},
  journaltitle = {Journal of Cleaner Production},
  shortjournal = {Journal of Cleaner Production},
  volume = {312},
  pages = {127730},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2021.127730},
  url = {https://www.sciencedirect.com/science/article/pii/S095965262101948X},
  urldate = {2022-09-29},
  abstract = {Urban agriculture provides one of the most promising areas for innovation in green and blue infrastructure in cities, particularly in developing countries. It can address multiple economic, social and local environmental benefits. Despite this critical role, urban agriculture often faces many challenges, including land competition, lack of urban policy directives, unfair land use planning and land tenure decisions. The presence of such barriers is indicative of critical issues of governance. This article examines various actors’ roles in different forms of governance in improving the sustainability benefits of urban agriculture. It draws policy lessons from Accra, Ghana, through empirical research conducted at two sites. The paper identifies the institutions and actors that govern urban agriculture and points to the problems and potential solutions to sustainable urban agriculture. These problems could be addressed by removing perverse incentives, conflicting regulations and unfair land management decision-making systems and providing more secure land tenure and large-scale technical support for agricultural and environmental management in a tropical urban environment.},
  langid = {english},
  keywords = {Accra,Ghana,Green infrastructure,Urban agriculture,Urban governance},
  file = {/Users/mavi/Zotero/storage/X3A4QHYV/Puppim de Oliveira and Ahmed - 2021 - Governance of urban agriculture in African cities.pdf;/Users/mavi/Zotero/storage/8UR7QR2A/S095965262101948X.html}
}

@online{qianWeNeedFully2020,
  title = {Do {{We Need Fully Connected Output Layers}} in {{Convolutional Networks}}?},
  author = {Qian, Zhongchao and Hayes, Tyler L. and Kafle, Kushal and Kanan, Christopher},
  date = {2020-04-28},
  eprint = {2004.13587},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2004.13587},
  urldate = {2022-12-26},
  abstract = {Traditionally, deep convolutional neural networks consist of a series of convolutional and pooling layers followed by one or more fully connected (FC) layers to perform the final classification. While this design has been successful, for datasets with a large number of categories, the fully connected layers often account for a large percentage of the network's parameters. For applications with memory constraints, such as mobile devices and embedded platforms, this is not ideal. Recently, a family of architectures that involve replacing the learned fully connected output layer with a fixed layer has been proposed as a way to achieve better efficiency. In this paper we examine this idea further and demonstrate that fixed classifiers offer no additional benefit compared to simply removing the output layer along with its parameters. We further demonstrate that the typical approach of having a fully connected final output layer is inefficient in terms of parameter count. We are able to achieve comparable performance to a traditionally learned fully connected classification output layer on the ImageNet-1K, CIFAR-100, Stanford Cars-196, and Oxford Flowers-102 datasets, while not having a fully connected output layer at all.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/PCDXPWD5/Qian et al. - 2020 - Do We Need Fully Connected Output Layers in Convol.pdf;/Users/mavi/Zotero/storage/2YZVG95R/2004.html}
}

@article{raddadStrategicPlanningIntegrate2022,
  title = {Strategic Planning to Integrate Urban Agriculture in {{Palestinian}} Urban Development under Conditions of Political Instability},
  author = {Raddad, By Samer Hatem},
  date = {2022-10-01},
  journaltitle = {Urban Forestry \& Urban Greening},
  shortjournal = {Urban Forestry \& Urban Greening},
  volume = {76},
  pages = {127734},
  issn = {1618-8667},
  doi = {10.1016/j.ufug.2022.127734},
  url = {https://www.sciencedirect.com/science/article/pii/S1618866722002771},
  urldate = {2022-09-29},
  abstract = {Today, urban agriculture, an element of sustainable urban development, is not a new idea in an urban planning context. However, not all urban planners and policymakers consider urban agriculture in their cities. Palestinian cities are subject to conditions of political instability that negatively impact urban food security. This study proposes a new strategic model to integrate urban agriculture into a Palestinian urban development framework based on an evaluation of the current status of urban agriculture in the Palestinian urban planning system. This study argues that urban agriculture is absent in urban development processes in Palestinian cities and claims presence of a clear gap between urban planning policies and urban agriculture in Palestine. Lack of knowledge and will among Palestinian urban planners and policymakers have led to the emergence of negative attitudes toward urban agricultural activities in the Palestinian urban planning system. This study proposes a strategic model to enhance knowledge and awareness about urban agricultural issues among Palestinian urban planners and policymakers to foster positive attitudes toward the combination of urban agriculture and the urban planning process in harmony with the Palestinian urban methods, laws, and strategic urban development plans for support of the urban agricultural while making Palestinian cities more sustainable, especially under conditions of political instability. Also, integration urban agriculture as a green area into the Palestinian urban environment would support urban ecosystem services to become more resilient and increase the multifunctional use of urban green spaces such as enhancement of urban food security, social and cultural interaction, and improvement of air quality and life in the Palestinian cities.},
  langid = {english},
  keywords = {Palestine,Political instability,Strategic Planning,Urban agriculture,Urban development},
  file = {/Users/mavi/Zotero/storage/LUFBHHLB/Raddad - 2022 - Strategic planning to integrate urban agriculture .pdf;/Users/mavi/Zotero/storage/GFXE7CZT/S1618866722002771.html}
}

@online{rainforthTighterVariationalBounds2019,
  title = {Tighter {{Variational Bounds}} Are {{Not Necessarily Better}}},
  author = {Rainforth, Tom and Kosiorek, Adam R. and Le, Tuan Anh and Maddison, Chris J. and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  date = {2019-03-05},
  eprint = {1802.04537},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1802.04537},
  url = {http://arxiv.org/abs/1802.04537},
  urldate = {2022-12-25},
  abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted auto-encoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/NMUGYMVD/Rainforth et al. - 2019 - Tighter Variational Bounds are Not Necessarily Bet.pdf;/Users/mavi/Zotero/storage/QKWAVDBC/1802.html}
}

@article{ranconComparisonSIFTEncoded2018,
  title = {Comparison of {{SIFT Encoded}} and {{Deep Learning Features}} for the {{Classification}} and {{Detection}} of {{Esca Disease}} in {{Bordeaux Vineyards}}},
  author = {Rançon, Florian and Bombrun, Lionel and Keresztes, Barna and Germain, Christian},
  date = {2018-12-20},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {11},
  number = {1},
  pages = {1},
  issn = {2072-4292},
  doi = {10.3390/rs11010001},
  url = {http://www.mdpi.com/2072-4292/11/1/1},
  urldate = {2022-05-16},
  abstract = {Grapevine wood fungal diseases such as esca are among the biggest threats in vineyards nowadays. The lack of very efficient preventive (best results using commercial products report 20\% efficiency) and curative means induces huge economic losses. The study presented in this paper is centered around the in-field detection of foliar esca symptoms during summer, exhibiting a typical “striped” pattern. Indeed, in-field disease detection has shown great potential for commercial applications and has been successfully used for other agricultural needs such as yield estimation. Differentiation with foliar symptoms caused by other diseases or abiotic stresses was also considered. Two vineyards from the Bordeaux region (France, Aquitaine) were chosen as the basis for the experiment. Pictures of diseased and healthy vine plants were acquired during summer 2017 and labeled at the leaf scale, resulting in a patch database of around 6000 images (224 × 224 pixels) divided into red cultivar and white cultivar samples. Then, we tackled the classification part of the problem comparing state-of-the-art SIFT encoding and pre-trained deep learning feature extractors for the classification of database patches. In the best case, 91\% overall accuracy was obtained using deep features extracted from MobileNet network trained on ImageNet database, demonstrating the efficiency of simple transfer learning approaches without the need to design an ad-hoc specific feature extractor. The third part aimed at disease detection (using bounding boxes) within full plant images. For this purpose, we integrated the deep learning base network within a “one-step” detection network (RetinaNet), allowing us to perform detection queries in real time (approximately six frames per second on GPU). Recall/Precision (RP) and Average Precision (AP) metrics then allowed us to evaluate the performance of the network on a 91-image (plants) validation database. Overall, 90\% precision for a 40\% recall was obtained while best esca AP was about 70\%. Good correlation between annotated and detected symptomatic surface per plant was also obtained, meaning slightly symptomatic plants can be efficiently separated from severely attacked plants.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/3RYPU2Q5/Rançon et al. - 2018 - Comparison of SIFT Encoded and Deep Learning Featu.pdf}
}

@article{raoCultivatingSustainableHealthy2022,
  title = {Cultivating Sustainable and Healthy Cities: {{A}} Systematic Literature Review of the Outcomes of Urban and Peri-Urban Agriculture},
  shorttitle = {Cultivating Sustainable and Healthy Cities},
  author = {Rao, Nitya and Patil, Sheetal and Singh, Chandni and Roy, Parama and Pryor, Charles and Poonacha, Prathigna and Genes, Mariam},
  date = {2022-10-01},
  journaltitle = {Sustainable Cities and Society},
  shortjournal = {Sustainable Cities and Society},
  volume = {85},
  pages = {104063},
  issn = {2210-6707},
  doi = {10.1016/j.scs.2022.104063},
  url = {https://www.sciencedirect.com/science/article/pii/S221067072200381X},
  urldate = {2022-09-29},
  abstract = {Despite considerable interest in urban and peri-urban agriculture (UPA) in recent decades, its contributions to urban sustainability and human wellbeing remain contested. This systematic literature review examines the geographical landscape of the peer-reviewed literature on UPA and assesses its reported outcomes on sustainability and wellbeing. Following systematic review protocols, we undertook a two-step literature screening and quality assessment process. From a total of 4029 articles, based inclusion-exclusion criteria, we filtered 320 articles for quantitative and 86 for qualitative assessment. Quantitative analysis confirmed an exponential increase in literature on UPA since 2015 and a regional bias towards the Global North. The qualitative analysis identified six thematic outcomes of UPA under three sustainability pillars - environmental sustainability; material well-being; labour and livelihoods; land tenure and urban planning; and food and nutritional security as part of economic sustainability; and subjective and relational wellbeing as well as gender and social differentiation as elements of social sustainability. Environmental sustainability was most discussed, followed by subjective wellbeing and food and nutritional security. Gender and social differentiation issues were least represented in the papers. There remain knowledge gaps around how urban policy and planning can recognise, leverage, and scale up the sustainability and wellbeing co-benefits of UPA.},
  langid = {english},
  keywords = {Sustainability,Urban and peri-urban agriculture,Urbanisation,Wellbeing},
  file = {/Users/mavi/Zotero/storage/YBID3Q82/Rao et al. - 2022 - Cultivating sustainable and healthy cities A syst.pdf;/Users/mavi/Zotero/storage/R5H8SUJC/S221067072200381X.html}
}

@article{raufCitrusFruitsLeaves2019,
  title = {A Citrus Fruits and Leaves Dataset for Detection and Classification of Citrus Diseases through Machine Learning},
  author = {Rauf, Hafiz Tayyab and Saleem, Basharat Ali and Lali, M. Ikram Ullah and Khan, Muhammad Attique and Sharif, Muhammad and Bukhari, Syed Ahmad Chan},
  date = {2019-10},
  journaltitle = {Data in Brief},
  shortjournal = {Data in Brief},
  volume = {26},
  pages = {104340},
  issn = {23523409},
  doi = {10.1016/j.dib.2019.104340},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2352340919306948},
  urldate = {2022-05-16},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/E556P66F/Rauf et al. - 2019 - A citrus fruits and leaves dataset for detection a.pdf}
}

@article{rautReviewLeafDisease,
  title = {Review {{On Leaf Disease Detection Using Image Processing Techniques}}},
  author = {Raut, Sandesh and Ingole, Kartik},
  volume = {04},
  number = {04},
  pages = {5},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/L2T9AAWH/Raut and Ingole - Review On Leaf Disease Detection Using Image Proce.pdf}
}

@article{riquelmeDeepLearningLung2020,
  title = {Deep {{Learning}} for {{Lung Cancer Nodules Detection}} and {{Classification}} in {{CT Scans}}},
  author = {Riquelme, Diego and Akhloufi, Moulay},
  date = {2020-01-08},
  journaltitle = {AI},
  shortjournal = {AI},
  volume = {1},
  number = {1},
  pages = {28--67},
  issn = {2673-2688},
  doi = {10.3390/ai1010003},
  url = {https://www.mdpi.com/2673-2688/1/1/3},
  urldate = {2022-05-31},
  abstract = {Detecting malignant lung nodules from computed tomography (CT) scans is a hard and time-consuming task for radiologists. To alleviate this burden, computer-aided diagnosis (CAD) systems have been proposed. In recent years, deep learning approaches have shown impressive results outperforming classical methods in various fields. Nowadays, researchers are trying different deep learning techniques to increase the performance of CAD systems in lung cancer screening with computed tomography. In this work, we review recent state-of-the-art deep learning algorithms and architectures proposed as CAD systems for lung cancer detection. They are divided into two categories—(1) Nodule detection systems, which from the original CT scan detect candidate nodules; and (2) False positive reduction systems, which from a set of given candidate nodules classify them into benign or malignant tumors. The main characteristics of the different techniques are presented, and their performance is analyzed. The CT lung datasets available for research are also introduced. Comparison between the different techniques is presented and discussed.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/FXGVIMT3/Riquelme and Akhloufi - 2020 - Deep Learning for Lung Cancer Nodules Detection an.pdf}
}

@thesis{robertsMachinePerceptionThreedimensional1963,
  type = {Thesis},
  title = {Machine Perception of Three-Dimensional Solids},
  author = {Roberts, Lawrence G.},
  date = {1963},
  institution = {{Massachusetts Institute of Technology}},
  url = {https://dspace.mit.edu/handle/1721.1/11589},
  urldate = {2023-05-05},
  abstract = {Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1963.},
  langid = {english},
  annotation = {Accepted: 2005-08-17T19:45:17Z},
  file = {/Users/mavi/Zotero/storage/KG3K825Y/Roberts_1963_Machine perception of three-dimensional solids.pdf}
}

@online{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015-05-18},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1505.04597},
  url = {http://arxiv.org/abs/1505.04597},
  urldate = {2023-05-08},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/XCEFSF4A/Ronneberger et al_2015_U-Net.pdf;/Users/mavi/Zotero/storage/7ZJM2DCB/1505.html}
}

@article{rousseauMultiscaleImagingPlants2015,
  title = {Multiscale Imaging of Plants: Current Approaches and Challenges},
  shorttitle = {Multiscale Imaging of Plants},
  author = {Rousseau, David and Chéné, Yann and Belin, Etienne and Semaan, Georges and Trigui, Ghassen and Boudehri, Karima and Franconi, Florence and Chapeau-Blondeau, François},
  date = {2015},
  journaltitle = {Plant Methods},
  shortjournal = {Plant Methods},
  volume = {11},
  number = {1},
  pages = {6},
  issn = {1746-4811},
  doi = {10.1186/s13007-015-0050-1},
  url = {http://www.plantmethods.com/content/11/1/6},
  urldate = {2022-05-16},
  abstract = {We review a set of recent multiscale imaging techniques, producing high-resolution images of interest for plant sciences. These techniques are promising because they match the multiscale structure of plants. However, the use of such high-resolution images is challenging in the perspective of their application to high-throughput phenotyping on large populations of plants, because of the memory cost for their data storage and the computational cost for their processing to extract information. We discuss how this renews the interest for multiscale image processing tools such as wavelets, fractals and recent variants to analyse such high-resolution images.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/LBVR33VK/Rousseau et al. - 2015 - Multiscale imaging of plants current approaches a.pdf}
}

@article{ruedenImageJ2ImageJNext2017,
  title = {{{ImageJ2}}: {{ImageJ}} for the next Generation of Scientific Image Data},
  author = {Rueden, C. T. and Schindelin, J. and family=Hiner, given=M. C., prefix=et al., useprefix=false},
  date = {2017},
  journaltitle = {BMC Bioinformatics},
  volume = {18},
  pages = {529},
  doi = {10.1186/s12859-017-1934-z},
  keywords = {read}
}

@article{sadeghi-tehranDeepCountInFieldAutomatic2019,
  title = {{{DeepCount}}: {{In-Field Automatic Quantification}} of {{Wheat Spikes Using Simple Linear Iterative Clustering}} and {{Deep Convolutional Neural Networks}}},
  author = {Sadeghi-Tehran, Pouria},
  date = {2019},
  journaltitle = {Frontiers in Plant Science},
  volume = {10},
  pages = {16},
  abstract = {Crop yield is an essential measure for breeders, researchers, and farmers and is composed of and may be calculated by the number of ears per square meter, grains per ear, and thousand grain weight. Manual wheat ear counting, required in breeding programs to evaluate crop yield potential, is labor-intensive and expensive; thus, the development of a real-time wheat head counting system would be a significant advancement. In this paper, we propose a computationally efficient system called DeepCount to automatically identify and count the number of wheat spikes in digital images taken under natural field conditions. The proposed method tackles wheat spike quantification by segmenting an image into superpixels using simple linear iterative clustering (SLIC), deriving canopy relevant features, and then constructing a rational feature model fed into the deep convolutional neural network (CNN) classification for semantic segmentation of wheat spikes. As the method is based on a deep learning model, it replaces hand-engineered features required for traditional machine learning methods with more efficient algorithms. The method is tested on digital images taken directly in the field at different stages of ear emergence/maturity (using visually different wheat varieties), with different canopy complexities (achieved through varying nitrogen inputs) and different heights above the canopy under varying environmental conditions. In addition, the proposed technique is compared with a wheat ear counting method based on a previously developed edge detection technique and morphological analysis. The proposed approach is validated with image-based ear counting and ground-based measurements. The results demonstrate that the DeepCount technique has a high level of robustness regardless of variables, such as growth stage and weather conditions, hence demonstrating the feasibility of the approach in real scenarios. The system is a leap toward a portable and smartphone-assisted wheat ear counting systems, results in reducing the labor involved, and is suitable for high-throughput analysis. It may also be adapted to work on Red; Green; Blue (RGB) images acquired from unmanned aerial vehicle (UAVs).},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/M7TJUKGB/Sadeghi-Tehran - 2019 - DeepCount In-Field Automatic Quantification of Wh.pdf}
}

@article{sagarFutureImpactsUrban2022,
  title = {Future Impacts of {{Urban}} and {{Peri-urban}} Agriculture on Carbon Stock and Land Surface Temperatures in {{India}}},
  author = {Sagar, U. Srilok and Singh, Yatharth and Mahalingam, Ashwin and Malladi, Teja},
  date = {2022-09-01},
  journaltitle = {Urban Climate},
  shortjournal = {Urban Climate},
  volume = {45},
  pages = {101267},
  issn = {2212-0955},
  doi = {10.1016/j.uclim.2022.101267},
  url = {https://www.sciencedirect.com/science/article/pii/S2212095522001857},
  urldate = {2022-09-29},
  abstract = {Over the last two decades, cities in India have seen significant urban growth accompanied by green cover loss. Recently however, there has been a growing interest in urban and peri-urban agriculture (UPA) in these urban areas. The extent to which UPA mitigates the effects of urbanization is unclear. The present study aims to quantify the impact of UPA in the cities of Chennai and Bengaluru in India. Past trends of urban growth and green cover depletion are used to predict how urbanized Chennai and Bengaluru will be in the future, using CA Markov techniques on GIS data. A survey was then carried out to understand the general perception and growth trends pertaining to UPA. This survey data was then combined with our land use model to predict the growth of UPA in Chennai and Bengaluru in the future. These ‘future maps’ were then used to quantify the impact of UPA on biomass and land surface temperatures. We find that UPA can play a small, but not insignificant role in augmenting carbon stock and bringing down land surface temperatures and propose that urban development policies consciously include the role of UPA.},
  langid = {english},
  keywords = {CA-Markov models,Carbon density,Land surface temperature,LULC analysis,Urban and Peri-urban agriculture},
  file = {/Users/mavi/Zotero/storage/EVP4P3QT/Sagar et al. - 2022 - Future impacts of Urban and Peri-urban agriculture.pdf;/Users/mavi/Zotero/storage/4KVFNNZM/S2212095522001857.html}
}

@article{sahuReviewLeafDisease2020,
  title = {A {{Review}} on {{Leaf Disease Detection}} Using {{Image Processing}}},
  author = {Sahu, Kajal and Tiwari, Shrikant and Mandal, Snehalata},
  date = {2020},
  volume = {07},
  number = {05},
  pages = {5},
  abstract = {Identification of the leaf diseases is the key to preventing the losses in the yield and quantity of the agricultural product. The studies of the leaf diseases mean the studies of visually observable patterns seen on the leaf. Health monitoring and disease detection on leaf is very critical for sustainable agriculture. It is very difficult to monitor the leaf diseases manually. It requires tremendous amount of work, expertize in the leaf diseases, and also require the excessive processing time. Hence, image processing is used for the detection of leaf diseases. Disease detection involves the steps like image acquisition, image pre-processing, image segmentation, feature extraction and classification. This paper discussed the methods used for the detection of leaf diseases using their leaves images.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/ABG9SRFW/Sahu et al. - 2020 - A Review on Leaf Disease Detection using Image Pro.pdf}
}

@article{saleemPlantDiseaseDetection2019,
  title = {Plant {{Disease Detection}} and {{Classification}} by {{Deep Learning}}},
  author = {{Saleem} and {Potgieter} and {Mahmood Arif}},
  date = {2019-10-31},
  journaltitle = {Plants},
  shortjournal = {Plants},
  volume = {8},
  number = {11},
  pages = {468},
  issn = {2223-7747},
  doi = {10.3390/plants8110468},
  url = {https://www.mdpi.com/2223-7747/8/11/468},
  urldate = {2022-05-19},
  abstract = {Plant diseases affect the growth of their respective species, therefore their early identification is very important. Many Machine Learning (ML) models have been employed for the detection and classification of plant diseases but, after the advancements in a subset of ML, that is, Deep Learning (DL), this area of research appears to have great potential in terms of increased accuracy. Many developed/modified DL architectures are implemented along with several visualization techniques to detect and classify the symptoms of plant diseases. Moreover, several performance metrics are used for the evaluation of these architectures/techniques. This review provides a comprehensive explanation of DL models used to visualize various plant diseases. In addition, some research gaps are identified from which to obtain greater transparency for detecting diseases in plants, even before their symptoms appear clearly.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/RYY6RU7N/Saleem et al. - 2019 - Plant Disease Detection and Classification by Deep.pdf}
}

@article{salomonHealthySoilsBackbone2022,
  title = {Healthy Soils: {{The}} Backbone of Productive, Safe and Sustainable Urban Agriculture},
  shorttitle = {Healthy Soils},
  author = {Salomon, Matthias J. and Cavagnaro, Timothy R.},
  date = {2022-03-20},
  journaltitle = {Journal of Cleaner Production},
  shortjournal = {Journal of Cleaner Production},
  volume = {341},
  pages = {130808},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2022.130808},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652622004462},
  urldate = {2022-09-29},
  abstract = {The increasing urbanization of an ever-growing global population is coinciding with major global challenges that are threatening our food security. Urban agriculture is as a multi-functional tool to improve urban living and to provide food security towards resilient communities. This review explores on the importance of urban agriculture and identifies several points that will help inform a shift towards actively managed urban agriculture soils. First, the importance of soil within urban agriculture systems is highlighted, followed by a review of common issues of soil health in urban agriculture. These issues are then addressed by providing management principles for increased soil functioning in urban agriculture systems. These principles focus on improved soil nutrient and carbon pools and acknowledge the importance of the soil microbial community. Soil contamination with metal, organic and microbial contaminants is then addressed through a discussion of options available to help mitigate potential risk factors. Together, this review summarises our current understanding of soil health in urban agriculture systems. Where issues have been identified, these have been addressed by suggesting sustainable management principles. These can be used as recommendations for urban farmers towards more sustainable food production. Finally, we propose educational and governance approaches to help implementing these measurements.},
  langid = {english},
  keywords = {Contamination,Fertility,Health,Soil,Sustainability,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/VCXVE9UF/Salomon and Cavagnaro - 2022 - Healthy soils The backbone of productive, safe an.pdf;/Users/mavi/Zotero/storage/35SXM28V/S0959652622004462.html}
}

@article{samieiDeepLearningbasedDetection2020,
  title = {Deep Learning-Based Detection of Seedling Development},
  author = {Samiei, Salma and Rasti, Pejman and Ly Vu, Joseph and Buitink, Julia and Rousseau, David},
  date = {2020-07-30},
  journaltitle = {Plant Methods},
  shortjournal = {Plant Methods},
  volume = {16},
  number = {1},
  pages = {103},
  issn = {1746-4811},
  doi = {10.1186/s13007-020-00647-9},
  url = {https://doi.org/10.1186/s13007-020-00647-9},
  urldate = {2023-07-21},
  abstract = {Monitoring the timing of seedling emergence and early development via high-throughput phenotyping with computer vision is a challenging topic of high interest in plant science. While most studies focus on the measurements of leaf area index or detection of specific events such as emergence, little attention has been put on the identification of kinetics of events of early seedling development on a seed to seed basis.},
  keywords = {Deep learning,Kinetic,Seedling development},
  file = {/Users/mavi/Zotero/storage/WGC8MZEE/Samiei et al_2020_Deep learning-based detection of seedling development.pdf;/Users/mavi/Zotero/storage/J7LSPYD4/s13007-020-00647-9.html}
}

@article{schelhaasIntroducingTreeInteractions2007,
  title = {Introducing Tree Interactions in Wind Damage Simulation},
  author = {Schelhaas, M.J. and Kramer, K. and Peltola, H. and family=Werf, given=D.C., prefix=van der, useprefix=true and Wijdeven, S.M.J.},
  date = {2007-10},
  journaltitle = {Ecological Modelling},
  shortjournal = {Ecological Modelling},
  volume = {207},
  number = {2-4},
  pages = {197--209},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2007.04.025},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380007002578},
  urldate = {2022-09-30},
  abstract = {Wind throw is an important risk factor in forest management in North-western Europe. In recent years, mechanistic models have been developed to estimate critical wind speeds needed to break or uproot the average tree of a forest stand. Based on these models, we developed a wind damage module for the individual tree model ForGEM (Forest Genetics, Ecology and Management). For a given wind speed this module assesses the forces on each individual tree, based on the tree dimensions, and support and sheltering provided by other trees. Due to this individual approach, irregular stands can also be assessed. The module is demonstrated on Douglas fir stands (Pseudotsuga menziesii (Mirb.) Franco) of different densities in the Netherlands. Patterns of damage are explained, both in freshly exposed stands as well as in sheltered stands. Wind speeds needed to cause damage approximated those of known wind throw events. The wind damage module proved to be very sensitive to simulated tree heights and diameters. Furthermore, the newly introduced support mechanism played an important role in the stability of trees and stands. Lower individual tree stability in dense stands was clearly compensated for by the support of other trees.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/L4U2GNEL/Schelhaas et al. - 2007 - Introducing tree interactions in wind damage simul.pdf}
}

@article{schelhaasNaturalDisturbancesEuropean2003,
  title = {Natural Disturbances in the {{European}} Forests in the 19th and 20th Centuries},
  author = {Schelhaas, Mart-Jan and Nabuurs, Gert-Jan and Schuck, Andreas},
  date = {2003},
  journaltitle = {Global Change Biology},
  volume = {9},
  number = {11},
  pages = {1620--1633},
  issn = {1354-1013 1365-2486},
  doi = {10.1046/j.1365-2486.2003.00684.x},
  file = {/Users/mavi/Zotero/storage/3NZHP2NW/Schelhaas et al. - 2003 - Natural disturbances in the European forests in th.pdf}
}

@article{schindelinFijiOpensourcePlatform2012,
  title = {Fiji: An Open-Source Platform for Biological-Image Analysis},
  shorttitle = {Fiji},
  author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
  date = {2012-07},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {9},
  number = {7},
  pages = {676--682},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2019},
  url = {https://www.nature.com/articles/nmeth.2019},
  urldate = {2023-05-07},
  abstract = {Presented is an overview of the image-analysis software platform Fiji, a distribution of ImageJ that updates the underlying ImageJ architecture and adds modern software design elements to expand the capabilities of the platform and facilitate collaboration between biologists and computer scientists.},
  issue = {7},
  langid = {english},
  keywords = {Imaging,read,Software},
  file = {/Users/mavi/Zotero/storage/RCR5FCNB/Schindelin et al_2012_Fiji.pdf}
}

@article{schmidtIRoCSToolbox3D2014,
  title = {The {{iRoCS Toolbox}} – {{3D}} Analysis of the Plant Root Apical Meristem at Cellular Resolution},
  author = {Schmidt, Thorsten and Pasternak, Taras and Liu, Kun and Blein, Thomas and Aubry-Hivet, Dorothée and Dovzhenko, Alexander and Duerr, Jasmin and Teale, William and Ditengou, Franck A. and Burkhardt, Hans and Ronneberger, Olaf and Palme, Klaus},
  date = {2014},
  journaltitle = {The Plant Journal},
  volume = {77},
  number = {5},
  pages = {806--814},
  issn = {1365-313X},
  doi = {10.1111/tpj.12429},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tpj.12429},
  urldate = {2023-05-08},
  abstract = {To achieve a detailed understanding of processes in biological systems, cellular features must be quantified in the three-dimensional (3D) context of cells and organs. We described use of the intrinsic root coordinate system (iRoCS) as a reference model for the root apical meristem of plants. iRoCS enables direct and quantitative comparison between the root tips of plant populations at single-cell resolution. The iRoCS Toolbox automatically fits standardized coordinates to raw 3D image data. It detects nuclei or segments cells, automatically fits the coordinate system, and groups the nuclei/cells into the root's tissue layers. The division status of each nucleus may also be determined. The only manual step required is to mark the quiescent centre. All intermediate outputs may be refined if necessary. The ability to learn the visual appearance of nuclei by example allows the iRoCS Toolbox to be easily adapted to various phenotypes. The iRoCS Toolbox is provided as an open-source software package, licensed under the GNU General Public License, to make it accessible to a broad community. To demonstrate the power of the technique, we measured subtle changes in cell division patterns caused by modified auxin flux within the Arabidopsis thaliana root apical meristem.},
  langid = {english},
  keywords = {Arabidopsis thaliana,automated image analysis,confocal microscopy,pin mutants,population studies,root modelling,technical advance},
  file = {/Users/mavi/Zotero/storage/I7NHULCB/Schmidt et al_2014_The iRoCS Toolbox – 3D analysis of the plant root apical meristem at cellular.pdf;/Users/mavi/Zotero/storage/ACLEZ8LJ/tpj.html}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2020-02},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  url = {http://arxiv.org/abs/1610.02391},
  urldate = {2022-11-16},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/ISKFU8P7/Selvaraju et al_2020_Grad-CAM.pdf;/Users/mavi/Zotero/storage/JLMSWQJ3/1610.html}
}

@article{sethyImageProcessingTechniques2020,
  title = {Image {{Processing Techniques}} for {{Diagnosing Rice Plant Disease}}: {{A Survey}}},
  shorttitle = {Image {{Processing Techniques}} for {{Diagnosing Rice Plant Disease}}},
  author = {Sethy, Prabira Kumar and Barpanda, Nalini Kanta and Rath, Amiya Kumar and Behera, Santi Kumari},
  date = {2020},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {167},
  pages = {516--530},
  issn = {18770509},
  doi = {10.1016/j.procs.2020.03.308},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050920307742},
  urldate = {2022-05-16},
  abstract = {Over the past decades, rice crops are crucially admitted as one of the powerful energy streams for the production of resources. RAibcsetrpalcatnt diseases are considered as a raising factor behind the agricultural, economic and communal loss in the upcoming development of the agricultural field. Since last 10 years diagnosis of plant disease in approach to image processing techniques hOavveer trheempaiansetddekceaednesa, rreicoefcrinotpesreasrte acmruocniagllythaedrmesitetaerdchaesro. nAe onfumthbeeproowferdfiusleaesneerdgeytesctrtieoanm, sidfoenr ttihfiecaptriodnucatnidonqoufanretisfoicuartcieosn. mRiectehopdlasnhtadviesebaeseens daerveecloopnesdidaenreddapaspliaedraiinsinagwfidacetovrarbieethyinodf cthroepasg. rTichuisltpuaraple,rerceovnieowmsicrealantdedcoremsmeaurcnhalplaopsesrsinfrothme tuhpecpomerinogd bdetvweleoepnm2e0n0t7ofanthde 2a0g1ri8cuwltiuthralaffieolcdu.sSoinncethleasdte1v0elyoepamrsendtiaogfnostsaisteoof fpltahnet adrits.eaTsheeinrealaptperdoasctuhdtioesimaraegecopmropcaersesdinbgatseecdhnimiqaugees sheagvme ernetmataioinne,dfekaeteunreareextorafcitniotenr,esfteaatmuroengsetlhecetiroensearncdhecr.laAssifnicuamtiboenr. oTf hdisisepaaspeerdeatelscotioonu, tliidneenstiftihceaticounrraendt aqcuhainetvifeimcaetniotsn, lmimetihtaotdiosnhsa,vaendbeseunggdeesvteiolonpsefdorafnudtuarpeprleiesdeairncha awssidoecivaaterdiewtyitohf tchreopdsia. gTnhoisispaopferricrevpileawntsdriesleaatesdesr.esearch papers from the period between 2007 and 2018 with a focus on the development of state of the art. The related studies are compared based image segmentation, feature extraction, feature selection and classification. This paper also outlines the current achievements, limitations, and suggestions for future research associated with the diagnosis of rice plant diseases.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/55TIN9YU/Sethy et al. - 2020 - Image Processing Techniques for Diagnosing Rice Pl.pdf}
}

@online{Setting,
  title = {Setting},
  url = {https://plotly.com/python/figure-labels/},
  urldate = {2023-01-16},
  abstract = {Detailed examples of Setting the Font, Title, Legend Entries, and Axis Titles including changing color, size, log axes, and more in Python.}
}

@article{sharmaMachineLearningDeep2021,
  title = {Machine {{Learning}} and {{Deep Learning Applications-A Vision}}},
  author = {Sharma, Neha and Sharma, Reecha and Jindal, Neeru},
  date = {2021-06-01},
  journaltitle = {Global Transitions Proceedings},
  shortjournal = {Global Transitions Proceedings},
  series = {1st {{International Conference}} on {{Advances}} in {{Information}}, {{Computing}} and {{Trends}} in {{Data Engineering}} ({{AICDE}} - 2020)},
  volume = {2},
  number = {1},
  pages = {24--28},
  issn = {2666-285X},
  doi = {10.1016/j.gltp.2021.01.004},
  url = {https://www.sciencedirect.com/science/article/pii/S2666285X21000042},
  urldate = {2023-06-19},
  abstract = {The application of artificial intelligence is machine learning which is one of the current topics in the computer field as well as for the new COVID-19 pandemic. Researchers have given a lot of input to enhance the precision of machine learning algorithms and lot of work is carried out rapidly to enhance the intelligence of machines. Learning, a natural process in human behaviour that also becomes a vital part of machines as well. Besides this, another concept of deep learning comes to play its major role. Deep neural network (deep learning) is a subgroup of machine learning. Deep learning had been analysed and implemented in various applications and had shown remarkable results thus this field needs wider exploration which can be helpful for further real-world applications. The main objective of this paper is to provide insight survey for machine learning along with deep learning applications in various domains. Also, some applications with new normal COVID-19 blues. A review on already present applications and currently going on applications in several domains, for machine learning along with deep neural learning are exemplified.},
  langid = {english},
  keywords = {\_tablet,Deep neural learning (DL),Machine intelligence (artificial intelligence-AL),Machine Learning (ML)},
  file = {/Users/mavi/Zotero/storage/82P48A2P/Sharma et al_2021_Machine Learning and Deep Learning Applications-A Vision.pdf;/Users/mavi/Zotero/storage/N4MJDCP4/S2666285X21000042.html}
}

@online{shiDeepNeuralNetworks2023,
  title = {Deep {{Neural Networks}} for {{Rank-Consistent Ordinal Regression Based On Conditional Probabilities}}},
  author = {Shi, Xintong and Cao, Wenzhi and Raschka, Sebastian},
  date = {2023-05-31},
  eprint = {2111.08851},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2111.08851},
  url = {http://arxiv.org/abs/2111.08851},
  urldate = {2023-06-22},
  abstract = {In recent times, deep neural networks achieved outstanding predictive performance on various classification and pattern recognition tasks. However, many real-world prediction problems have ordinal response variables, and this ordering information is ignored by conventional classification losses such as the multi-category cross-entropy. Ordinal regression methods for deep neural networks address this. One such method is the CORAL method, which is based on an earlier binary label extension framework and achieves rank consistency among its output layer tasks by imposing a weight-sharing constraint. However, while earlier experiments showed that CORAL's rank consistency is beneficial for performance, it is limited by a weight-sharing constraint in a neural network's fully connected output layer, which may restrict the expressiveness and capacity of a network trained using CORAL. We propose a new method for rank-consistent ordinal regression without this limitation. Our rank-consistent ordinal regression framework (CORN) achieves rank consistency by a novel training scheme. This training scheme uses conditional training sets to obtain the unconditional rank probabilities through applying the chain rule for conditional probability distributions. Experiments on various datasets demonstrate the efficacy of the proposed method to utilize the ordinal target information, and the absence of the weight-sharing restriction improves the performance substantially compared to the CORAL reference approach. Additionally, the suggested CORN method is not tied to any specific architecture and can be utilized with any deep neural network classifier to train it for ordinal regression tasks.},
  pubstate = {preprint},
  keywords = {\_tablet,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/ARN3UXJ5/Shi et al_2023_Deep Neural Networks for Rank-Consistent Ordinal Regression Based On.pdf;/Users/mavi/Zotero/storage/CANHTAIE/2111.html}
}

@online{shwartz-zivOpeningBlackBox2017,
  title = {Opening the {{Black Box}} of {{Deep Neural Networks}} via {{Information}}},
  author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
  date = {2017-04-29},
  eprint = {1703.00810},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.00810},
  urldate = {2022-06-23},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby and Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/4XYZ24AZ/Shwartz-Ziv and Tishby - 2017 - Opening the Black Box of Deep Neural Networks via .pdf}
}

@article{singhDeepLearningPlant2018,
  title = {Deep {{Learning}} for {{Plant Stress Phenotyping}}: {{Trends}} and {{Future Perspectives}}},
  shorttitle = {Deep {{Learning}} for {{Plant Stress Phenotyping}}},
  author = {Singh, Asheesh Kumar and Ganapathysubramanian, Baskar and Sarkar, Soumik and Singh, Arti},
  date = {2018-10},
  journaltitle = {Trends in Plant Science},
  shortjournal = {Trends in Plant Science},
  volume = {23},
  number = {10},
  pages = {883--898},
  issn = {13601385},
  doi = {10.1016/j.tplants.2018.07.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1360138518301572},
  urldate = {2022-05-25},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/GVCW3I45/Singh et al. - 2018 - Deep Learning for Plant Stress Phenotyping Trends.pdf}
}

@article{singhDetectionPlantLeaf2017,
  title = {Detection of Plant Leaf Diseases Using Image Segmentation and Soft Computing Techniques},
  author = {Singh, Vijai and Misra, A.K.},
  date = {2017-03},
  journaltitle = {Information Processing in Agriculture},
  shortjournal = {Information Processing in Agriculture},
  volume = {4},
  number = {1},
  pages = {41--49},
  issn = {22143173},
  doi = {10.1016/j.inpa.2016.10.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2214317316300154},
  urldate = {2022-05-16},
  abstract = {Agricultural productivity is something on which economy highly depends. This is the one of the reasons that disease detection in plants plays an important role in agriculture field, as having disease in plants are quite natural. If proper care is not taken in this area then it causes serious effects on plants and due to which respective product quality, quantity or productivity is affected. For instance a disease named little leaf disease is a hazardous disease found in pine trees in United States. Detection of plant disease through some automatic technique is beneficial as it reduces a large work of monitoring in big farms of crops, and at very early stage itself it detects the symptoms of diseases i.e. when they appear on plant leaves. This paper presents an algorithm for image segmentation technique which is used for automatic detection and classification of plant leaf diseases. It also covers survey on different diseases classification techniques that can be used for plant leaf disease detection. Image segmentation, which is an important aspect for disease detection in plant leaf disease, is done by using genetic algorithm.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/P3DQVL9M/Singh and Misra - 2017 - Detection of plant leaf diseases using image segme.pdf}
}

@article{singhHybridFeatureBasedDisease2022,
  title = {Hybrid {{Feature-Based Disease Detection}} in {{Plant Leaf Using Convolutional Neural Network}}, {{Bayesian Optimized SVM}}, and {{Random Forest Classifier}}},
  author = {Singh, Ashutosh Kumar and Sreenivasu, Svn and Mahalaxmi, U.S.B. K. and Sharma, Himanshu and Patil, Dinesh D. and Asenso, Evans},
  editor = {Khan, Rijwan},
  date = {2022-02-10},
  journaltitle = {Journal of Food Quality},
  shortjournal = {Journal of Food Quality},
  volume = {2022},
  pages = {1--16},
  issn = {1745-4557, 0146-9428},
  doi = {10.1155/2022/2845320},
  url = {https://www.hindawi.com/journals/jfq/2022/2845320/},
  urldate = {2022-05-16},
  abstract = {Plant diseases are unfavourable factors that cause a significant decrease in the quality and quantity of crops. Experienced biologists or farmers often observe plants with the naked eye for disease, but this method is often imprecise and can take a long time. In this study, we use artificial intelligence and computer vision techniques to achieve the goal of designing and developing an intelligent classification mechanism for leaf diseases. This paper follows two methodologies and their simulation outcomes are compared for performance evaluation. In the first part, data augmentation is performed on the PlantVillage data set images (for apple, corn, potato, tomato, and rice plants), and their deep features are extracted using convolutional neural network (CNN). These features are classified by a Bayesian optimized support vector machine classifier and the results attained in terms of precision, sensitivity, f-score, and accuracy. The above-said methodologies will enable farmers all over the world to take early action to prevent their crops from becoming irreversibly damaged, thereby saving the world and themselves from a potential economic crisis. The second part of the methodology starts with the preprocessing of data set images, and their texture and color features are extracted by histogram of oriented gradient (HoG), GLCM, and color moments. Here, the three types of features, that is, color, texture, and deep features, are combined to form hybrid features. The binary particle swarm optimization is applied for the selection of these hybrid features followed by the classification with random forest classifier to get the simulation results. Binary particle swarm optimization plays a crucial role in hybrid feature selection; the purpose of this Algorithm is to obtain the suitable output with the least features. The comparative analysis of both techniques is presented with the use of the above-mentioned evaluation parameters.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/BTEHHG9U/Singh et al. - 2022 - Hybrid Feature-Based Disease Detection in Plant Le.pdf}
}

@article{singhMachineLearningHighThroughput2016,
  title = {Machine {{Learning}} for {{High-Throughput Stress Phenotyping}} in {{Plants}}},
  author = {Singh, Arti and Ganapathysubramanian, Baskar and Singh, Asheesh Kumar and Sarkar, Soumik},
  date = {2016-02},
  journaltitle = {Trends in Plant Science},
  shortjournal = {Trends in Plant Science},
  volume = {21},
  number = {2},
  pages = {110--124},
  issn = {13601385},
  doi = {10.1016/j.tplants.2015.10.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1360138515002630},
  urldate = {2022-05-16},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/ISHIHEBW/Singh et al. - 2016 - Machine Learning for High-Throughput Stress Phenot.pdf}
}

@inproceedings{singhPlantDocDatasetVisual2020,
  title = {{{PlantDoc}}: {{A Dataset}} for {{Visual Plant Disease Detection}}},
  shorttitle = {{{PlantDoc}}},
  booktitle = {Proceedings of the 7th {{ACM IKDD CoDS}} and 25th {{COMAD}}},
  author = {Singh, Davinder and Jain, Naman and Jain, Pranjali and Kayal, Pratik and Kumawat, Sudhakar and Batra, Nipun},
  date = {2020-01-05},
  eprint = {1911.10317},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {249--253},
  doi = {10.1145/3371158.3371196},
  url = {http://arxiv.org/abs/1911.10317},
  urldate = {2022-07-04},
  abstract = {India loses 35\% of the annual crop yield due to plant diseases. Early detection of plant diseases remains difficult due to the lack of lab infrastructure and expertise. In this paper, we explore the possibility of computer vision approaches for scalable and early plant disease detection. The lack of availability of sufficiently large-scale non-lab data set remains a major challenge for enabling vision based plant disease detection. Against this background, we present PlantDoc: a dataset for visual plant disease detection. Our dataset contains 2,598 data points in total across 13 plant species and up to 17 classes of diseases, involving approximately 300 human hours of effort in annotating internet scraped images. To show the efficacy of our dataset, we learn 3 models for the task of plant disease classification. Our results show that modelling using our dataset can increase the classification accuracy by up to 31\%. We believe that our dataset can help reduce the entry barrier of computer vision techniques in plant disease detection.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/mavi/Zotero/storage/39VCIPZT/Singh et al. - 2020 - PlantDoc A Dataset for Visual Plant Disease Detec.pdf}
}

@online{smithCyclicalLearningRates2017,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  date = {2017-04-04},
  eprint = {1506.01186},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.01186},
  urldate = {2023-02-01},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/mavi/Zotero/storage/RB9JNAX5/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf;/Users/mavi/Zotero/storage/RT9UGWB8/1506.html}
}

@online{snellLearningGenerateImages2017,
  title = {Learning to {{Generate Images}} with {{Perceptual Similarity Metrics}}},
  author = {Snell, Jake and Ridgeway, Karl and Liao, Renjie and Roads, Brett D. and Mozer, Michael C. and Zemel, Richard S.},
  date = {2017-01-23},
  eprint = {1511.06409},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.06409},
  url = {http://arxiv.org/abs/1511.06409},
  urldate = {2022-12-25},
  abstract = {Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions and reconstructing an input image from a compact representation. Supervised training of image-synthesis networks typically uses a pixel-wise loss (PL) to indicate the mismatch between a generated image and its corresponding target image. We propose instead to use a loss function that is better calibrated to human perceptual judgments of image quality: the multiscale structural-similarity score (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using MS-SSIM versus PL loss on training deterministic and stochastic autoencoders. For three different architectures, we collected human judgments of the quality of image reconstructions. Observers reliably prefer images synthesized by MS-SSIM-optimized models over those synthesized by PL-optimized models, for two distinct PL measures (\$\textbackslash ell\_1\$ and \$\textbackslash ell\_2\$ distances). We also explore the effect of training objective on image encoding and analyze conditions under which perceptually-optimized representations yield better performance on image classification. Finally, we demonstrate the superiority of perceptually-optimized networks for super-resolution imaging. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/3X62CUYI/Snell et al. - 2017 - Learning to Generate Images with Perceptual Simila.pdf;/Users/mavi/Zotero/storage/X66IZ3D4/1511.html}
}

@article{soetedjoPlantLeafDetection2021,
  title = {Plant {{Leaf Detection}} and {{Counting}} in a {{Greenhouse}} during {{Day}} and {{Nighttime Using}} a {{Raspberry Pi NoIR Camera}}},
  author = {Soetedjo, Aryuanto and Hendriarianti, Evy},
  date = {2021-01},
  journaltitle = {Sensors},
  volume = {21},
  number = {19},
  pages = {6659},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s21196659},
  url = {https://www.mdpi.com/1424-8220/21/19/6659},
  urldate = {2023-05-17},
  abstract = {A non-destructive method using machine vision is an effective way to monitor plant growth. However, due to the lighting changes and complicated backgrounds in outdoor environments, this becomes a challenging task. In this paper, a low-cost camera system using an NoIR (no infrared filter) camera and a Raspberry Pi module is employed to detect and count the leaves of Ramie plants in a greenhouse. An infrared camera captures the images of leaves during the day and nighttime for a precise evaluation. The infrared images allow Otsu thresholding to be used for efficient leaf detection. A combination of numbers of thresholds is introduced to increase the detection performance. Two approaches, consisting of static images and image sequence methods are proposed. A watershed algorithm is then employed to separate the leaves of a plant. The experimental results show that the proposed leaf detection using static images achieves high recall, precision, and F1 score of 0.9310, 0.9053, and 0.9167, respectively, with an execution time of 551 ms. The strategy of using sequences of images increases the performances to 0.9619, 0.9505, and 0.9530, respectively, with an execution time of 516.30 ms. The proposed leaf counting achieves a difference in count (DiC) and absolute DiC (ABS\_DiC) of 2.02 and 2.23, respectively, with an execution time of 545.41 ms. Moreover, the proposed method is evaluated using the benchmark image datasets, and shows that the foreground–background dice (FBD), DiC, and ABS\_DIC are all within the average values of the existing techniques. The results suggest that the proposed system provides a promising method for real-time implementation.},
  issue = {19},
  langid = {english},
  keywords = {\_tablet,greenhouse,infrared camera,leaf counting,leaf detection,Raspberry Pi},
  file = {/Users/mavi/Zotero/storage/TG3WQJTY/Soetedjo_Hendriarianti_2021_Plant Leaf Detection and Counting in a Greenhouse during Day and Nighttime.pdf}
}

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
  urldate = {2022-12-25},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  keywords = {⛔ No DOI found},
  file = {/Users/mavi/Zotero/storage/P7T3PCKA/Sohn et al. - 2015 - Learning Structured Output Representation using De.pdf}
}

@inproceedings{songCRESTConvolutionalResidual2017,
  title = {{{CREST}}: {{Convolutional Residual Learning}} for {{Visual Tracking}}},
  shorttitle = {{{CREST}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Song, Yibing and Ma, Chao and Gong, Lijun and Zhang, Jiawei and Lau, Rynson W.H. and Yang, Ming-Hsuan},
  date = {2017-10},
  pages = {2574--2583},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.279},
  url = {http://ieeexplore.ieee.org/document/8237541/},
  urldate = {2022-05-19},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/S8YF4UYN/Song et al. - 2017 - CREST Convolutional Residual Learning for Visual .pdf}
}

@article{stambukScreeningCroatianNative2021,
  title = {Screening of {{Croatian Native Grapevine Varieties}} for {{Susceptibility}} to {{Plasmopara}} Viticola {{Using Leaf Disc Bioassay}}, {{Chlorophyll Fluorescence}}, and {{Multispectral Imaging}}},
  author = {Štambuk, Petra and Šikuten, Iva and Preiner, Darko and Nimac, Ana and Lazarević, Boris and Marković, Zvjezdana and Maletić, Edi and Kontić, Jasminka Karoglan and Tomaz, Ivana},
  date = {2021-03-30},
  journaltitle = {Plants},
  shortjournal = {Plants},
  volume = {10},
  number = {4},
  pages = {661},
  issn = {2223-7747},
  doi = {10.3390/plants10040661},
  url = {https://www.mdpi.com/2223-7747/10/4/661},
  urldate = {2022-07-18},
  abstract = {In the era of sustainable grapevine production, there is a growing demand to define differences between Vitis vinifera varieties in susceptibility to downy mildew. Croatia, as a country with a long tradition of grapevine cultivation, preserves a large number of native grapevine varieties. A leaf disc bioassay has been conducted on 25 of them to define their response to downy mildew, according to the International Organisation of Vine and Wine (OIV) descriptor 452-1, together with the stress response of the leaf discs using chlorophyll fluorescence and multispectral imaging with 11 parameters included. Time points of measurement were as follows: before treatment (T0), one day post-inoculation (dpi) (T1), two dpi (T2), three dpi (T3), four dpi (T4), six dpi (T5), and eight dpi (T6). Visible changes in form of developed Plasmopara viticola (P. viticola) sporulation were evaluated on the seventh day upon inoculation. Results show that methods applied here distinguish varieties of different responses to downy mildew. Based on the results obtained, a phenotyping model in the absence of the pathogen is proposed, which is required to confirm by conducting more extensive research.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/GGU34EMN/Štambuk et al. - 2021 - Screening of Croatian Native Grapevine Varieties f.pdf;/Users/mavi/Zotero/storage/HTUMDRBE/Štambuk et al. - 2021 - Screening of Croatian Native Grapevine Varieties f.pdf;/Users/mavi/Zotero/storage/UFD6PIJR/Štambuk et al. - 2021 - Screening of Croatian Native Grapevine Varieties f.pdf}
}

@article{stolzeAutomatedImageAnalysis2019,
  title = {Automated Image Analysis with {{ImageJ}} of Yeast Colony Forming Units from Cannabis Flowers},
  author = {Stolze, Nicholas and Bader, Carly and Henning, Christopher and Mastin, Jared and Holmes, Andrea E. and Sutlief, Arin L.},
  date = {2019-09},
  journaltitle = {Journal of Microbiological Methods},
  shortjournal = {Journal of Microbiological Methods},
  volume = {164},
  pages = {105681},
  issn = {01677012},
  doi = {10.1016/j.mimet.2019.105681},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167701219305949},
  urldate = {2022-05-31},
  abstract = {Currently, in the state of Colorado and all other states within the United States of America with legalized marijuana programs, testing is required for bacteria, yeast, and mold on marijuana products. The Code of Colorado Regulations, 1 CCR 212–1, considers a passing result when a 1 g sample contains {$<$} 104 colony forming units (CFU) for the total yeast and mold count (TYMC). These measurements are usually obtained by manually counting colonies on petri-dishes or 3 M™ Petrifilms™, which is a time consuming and user subjective process. Therefore, an automated counting method utilizing ImageJ has been developed for CFU analysis of TYMC on Petrifilms. The performance of this colony counting method was demonstrated by comparing manual and automated counts from marijuana flower samples containing spikes of Candida albicans as well as samples that tested positive for the presence of yeast and mold. Fifteen images of Petrifilms showing various concentrations of colonies were studied by fifteen users at two institutions using both the automated and manual counting methods. All counts from the automated ImageJ procedure were within 12\% of those obtained manually. In twelve out of fifteen Petrifilms, the average count of the automated method was statistically similar to the manual counts. The statistical differences of the other three samples were observed to be random and caused by user errors. The automated counting method could be used to quickly count numbers that are as high as 400 CFUs, reducing time of analysis with improved documentation because the images and the electronic colony counts can be saved on a computer or cloud for long term storage and data access.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/TSAJJLCZ/Stolze et al. - 2019 - Automated image analysis with ImageJ of yeast colo.pdf}
}

@article{suchaLandAccessMechanisms2022,
  title = {Land Access Mechanisms of {{Soweto}} Farmers: {{Moving}} beyond Legal Land Tenure for Urban Agriculture},
  shorttitle = {Land Access Mechanisms of {{Soweto}} Farmers},
  author = {Suchá, Lenka and Dušková, Lenka},
  date = {2022-08-01},
  journaltitle = {Land Use Policy},
  shortjournal = {Land Use Policy},
  volume = {119},
  pages = {106169},
  issn = {0264-8377},
  doi = {10.1016/j.landusepol.2022.106169},
  url = {https://www.sciencedirect.com/science/article/pii/S026483772200196X},
  urldate = {2022-09-29},
  abstract = {Legal land tenure is often understood as an essential asset underpinning urban agriculture. However, the global land rights discussion recently moved away from a strict emphasis on legality towards a wider acknowledgement of the multidimensionality of land rights and land tenure. Based on the semi-structured interviews with farmers of Soweto at gardens in institutions and open-space gardens, and key informants we explore the mechanisms by which farmers gain, control, and maintain access to land with the aim to extend the evidence on the importance of social relations and their role in land tenure of small-scale urban farmers. Results of an in-depth qualitative analysis of structural and relational mechanisms of access to land show that farmers’ identity and ability to create and navigate through the complex web of social relations represents a vital formative force for land tenure. Building on our findings, we invite policy makers to enhance the agenda on land allocation for urban agriculture by preserving and fortifying the existing social networks and relationships. Such an approach allows for enlarging the spectrum of benefits provided by farmers to their community and vice versa, as well as for strengthening farmers’ self-esteem and internal motivation for engagement in urban agriculture. Therefore, our paper supports moving beyond the narrow notion of legal tenure for urban agriculture and embracing its more inclusive understanding by acknowledging social relations and their importance for farmers’ own perception of their land tenure.},
  langid = {english},
  keywords = {Access analysis,Land access,Social relations,South Africa,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/X2GUQ8TY/Suchá and Dušková - 2022 - Land access mechanisms of Soweto farmers Moving b.pdf;/Users/mavi/Zotero/storage/LMCRC5QV/S026483772200196X.html}
}

@article{sujathaPerformanceDeepLearning2021,
  title = {Performance of Deep Learning vs Machine Learning in Plant Leaf Disease Detection},
  author = {Sujatha, R. and Chatterjee, Jyotir Moy and Jhanjhi, Nz and Brohi, Sarfraz Nawaz},
  date = {2021-02},
  journaltitle = {Microprocessors and Microsystems},
  shortjournal = {Microprocessors and Microsystems},
  volume = {80},
  pages = {103615},
  issn = {01419331},
  doi = {10.1016/j.micpro.2020.103615},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0141933120307626},
  urldate = {2022-05-16},
  abstract = {Plants are recognized as essential as they are the primary source of humanity’s energy production since they are having nutritious, medicinal, etc. values. At any time between crop farming, plant diseases can affect the leaf, resulting in enormous crop production damages and economic market value. Therefore, in the farming industry, identification of leaf disease plays a crucial role. It needs, however, enormous labor, greater preparation time, and comprehensive plant pathogen knowledge. For the identification of plant disease detection various machine learning (ML) as well as deep learning (DL) methods are developed \& examined by various researchers, and many of the times they also got significant results in both cases. Motivated by those existing works, here in this article we are comparing the performance of ML (Support Vector Machine (SVM), Random Forest (RF), Sto­ chastic Gradient Descent (SGD)) \& DL (Inception-v3, VGG-16, VGG-19) in terms of citrus plant disease detection. The disease classification accuracy (CA) we received by experimentation is quite impressive as DL methods perform better than that of ML methods in case of disease detection as follows: RF-76.8\% {$>$} SGD-86.5\% {$>$} SVM87\% {$>$} VGG-19–87.4\% {$>$} Inception-v3–89\% {$>$} VGG-16–89.5\%. From the result, we can tell that RF is giving the least CA whereas VGG-16 is giving the best in terms of CA.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/PVL39K88/Sujatha et al. - 2021 - Performance of deep learning vs machine learning i.pdf}
}

@article{tanComparisonRetinaNetSSD2021,
  title = {Comparison of {{RetinaNet}}, {{SSD}}, and {{YOLO}} v3 for Real-Time Pill Identification},
  author = {Tan, Lu and Huangfu, Tianran and Wu, Liyao and Chen, Wenying},
  date = {2021-11-22},
  journaltitle = {BMC Medical Informatics and Decision Making},
  shortjournal = {BMC Medical Informatics and Decision Making},
  volume = {21},
  number = {1},
  pages = {324},
  issn = {1472-6947},
  doi = {10.1186/s12911-021-01691-8},
  url = {https://doi.org/10.1186/s12911-021-01691-8},
  urldate = {2022-10-26},
  abstract = {The correct identification of pills is very important to ensure the safe administration of drugs to patients. Here, we use three current mainstream object detection models, namely RetinaNet, Single Shot Multi-Box Detector (SSD), and You Only Look Once v3(YOLO v3), to identify pills and compare the associated performance.},
  keywords = {Convolutional neural network,Pill identification,RetinaNet,SSD,YOLO v3},
  file = {/Users/mavi/Zotero/storage/26UT3YNR/Tan et al. - 2021 - Comparison of RetinaNet, SSD, and YOLO v3 for real.pdf;/Users/mavi/Zotero/storage/YMNL7ABX/s12911-021-01691-8.html}
}

@online{tanEfficientDetScalableEfficient2020,
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  date = {2020-07-27},
  eprint = {1911.09070},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.1911.09070},
  url = {http://arxiv.org/abs/1911.09070},
  urldate = {2023-01-17},
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/mavi/Zotero/storage/JV2FYTWC/Tan et al. - 2020 - EfficientDet Scalable and Efficient Object Detect.pdf;/Users/mavi/Zotero/storage/DWS89EWY/1911.html}
}

@article{tangGrapeDiseaseImage2020,
  title = {Grape Disease Image Classification Based on Lightweight Convolution Neural Networks and Channelwise Attention},
  author = {Tang, Zhe and Yang, Jialing and Li, Zhe and Qi, Fang},
  date = {2020-11},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {178},
  pages = {105735},
  issn = {01681699},
  doi = {10.1016/j.compag.2020.105735},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169919319908},
  urldate = {2022-05-16},
  abstract = {In this paper, a lightweight convolution neural network model is proposed to diagnose grape diseases, including black rot, black measles and leaf blight. Focusing on small and low-latency models carried on mobile devices, a novel method is proposed based on lightweight convolution neural networks applying the channelwise attention (CA) mechanism. ShuffleNet V1 and V2 are chosen as the backbones, and squeeze-and-excitation (SE) blocks are considered as a CA mechanism to improve the ShuffleNet architecture. The proposed model is verified by an open dataset which includes 4,062 grape leaf images from four classes, including 3 diseased classes and 1 healthy class. The results of the experiments indicate the effectiveness of the proposed method. The best trained model accuracy is 99.14\%, and the model size is compressed from 227.5 MB (AlexNet) to 4.2 MB.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/AACFNLK3/Tang et al. - 2020 - Grape disease image classification based on lightw.pdf}
}

@article{tapiaMonitoringContributionUrban2021,
  title = {Monitoring the Contribution of Urban Agriculture to Urban Sustainability: An Indicator-Based Framework},
  shorttitle = {Monitoring the Contribution of Urban Agriculture to Urban Sustainability},
  author = {Tapia, Carlos and Randall, Linda and Wang, Shinan and Aguiar Borges, Luciane},
  date = {2021-11-01},
  journaltitle = {Sustainable Cities and Society},
  shortjournal = {Sustainable Cities and Society},
  volume = {74},
  pages = {103130},
  issn = {2210-6707},
  doi = {10.1016/j.scs.2021.103130},
  url = {https://www.sciencedirect.com/science/article/pii/S2210670721004121},
  urldate = {2022-09-29},
  abstract = {In an increasingly urbanized world, urban agriculture and community gardening are promoted as lever for sustainable urban development. Urban agriculture contributes to food security, provides health benefits for the population, fosters social inclusion and enhances perceived wellbeing. At the same time, from a planning perspective, urban agriculture also provides a valuable resource for urban regeneration. However, depending on prevalent farming practices urban agriculture may also have social and environmental externalities. While several of these aspects have been extensively tackled in the literature, others, in particular governance and planning aspects, are still unaddressed. Moreover, a comprehensive outline for the evaluation of urban agriculture performance from an urban sustainability perspective is still lacking. In this work we present a novel indicator-based evaluation framework for urban agriculture that captures the contribution of gardening practices to urban sustainability in a consistent, transparent and systematic way. We further illustrate the usability of our framework by testing it in Fællesgartneriet Brabrand, a community garden located in the city of Arhus, Denmark.},
  langid = {english},
  keywords = {Community gardening,Multi-criteria analysis,Sustainability indicators,Urban agriculture,Urban sustainability},
  file = {/Users/mavi/Zotero/storage/HLR9Q7M3/Tapia et al. - 2021 - Monitoring the contribution of urban agriculture t.pdf;/Users/mavi/Zotero/storage/4CAIJIUD/S2210670721004121.html}
}

@online{tolstikhinWassersteinAutoEncoders2019,
  title = {Wasserstein {{Auto-Encoders}}},
  author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  date = {2019-12-05},
  eprint = {1711.01558},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1711.01558},
  url = {http://arxiv.org/abs/1711.01558},
  urldate = {2022-12-25},
  abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/L83DU3W5/Tolstikhin et al. - 2019 - Wasserstein Auto-Encoders.pdf;/Users/mavi/Zotero/storage/6CSANCP5/1711.html}
}

@article{torunoPlantPathogenEffectorsCellular2016,
  title = {Plant-{{Pathogen Effectors}}: {{Cellular Probes Interfering}} with {{Plant Defenses}} in {{Spatial}} and {{Temporal Manners}}},
  shorttitle = {Plant-{{Pathogen Effectors}}},
  author = {Toruño, Tania Y. and Stergiopoulos, Ioannis and Coaker, Gitta},
  date = {2016-08-04},
  journaltitle = {Annual review of phytopathology},
  shortjournal = {Annu Rev Phytopathol},
  volume = {54},
  eprint = {27359369},
  eprinttype = {pmid},
  pages = {419--441},
  issn = {0066-4286},
  doi = {10.1146/annurev-phyto-080615-100204},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5283857/},
  urldate = {2023-05-03},
  abstract = {Plants possess large arsenals of immune receptors capable of recognizing all pathogen classes. To cause disease, pathogenic organisms must be able to overcome physical barriers, suppress or evade immune perception, and derive nutrients from host tissues. Consequently, to facilitate some of these processes, pathogens secrete effector proteins that promote colonization. This review covers recent advances in the field of effector biology, focusing on conserved cellular processes targeted by effectors from diverse pathogens. The ability of effectors to facilitate pathogen entry into the host interior, suppress plant immune perception, and alter host physiology for pathogen benefit is discussed. Pathogens also deploy effectors in a spatial and temporal manner, depending on infection stage. Recent advances have also enhanced our understanding of effectors acting in specific plant organs and tissues. Effectors are excellent cellular probes that facilitate insight into biological processes as well as key points of vulnerability in plant immune signaling networks.},
  pmcid = {PMC5283857},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/UU5GNPVM/Toruño et al_2016_Plant-Pathogen Effectors.pdf}
}

@online{touvronTrainingDataefficientImage2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
  date = {2021-01-15},
  eprint = {2012.12877},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.12877},
  url = {http://arxiv.org/abs/2012.12877},
  urldate = {2023-05-16},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/LTDYXLSP/Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf;/Users/mavi/Zotero/storage/6GZPE3IJ/2012.html}
}

@article{tovarRaspberryPipoweredImaging2018,
  title = {Raspberry {{Pi-powered}} Imaging for Plant Phenotyping},
  author = {Tovar, José and Hoyer, J. and Lin, Andy and Tielking, Allison and Callen, Steven and Castillo, S. and Miller, Michael and Tessman, Monica and Fahlgren, Noah and Carrington, James and Nusinow, Dmitri and Gehan (Dong), Malia},
  date = {2018-03-31},
  journaltitle = {Applications in Plant Sciences},
  shortjournal = {Applications in Plant Sciences},
  volume = {6},
  pages = {e01031},
  doi = {10.1002/aps3.1031},
  abstract = {Premise of the Study Image‐based phenomics is a powerful approach to capture and quantify plant diversity. However, commercial platforms that make consistent image acquisition easy are often cost‐prohibitive. To make high‐throughput phenotyping methods more accessible, low‐cost microcomputers and cameras can be used to acquire plant image data. Methods and Results We used low‐cost Raspberry Pi computers and cameras to manage and capture plant image data. Detailed here are three different applications of Raspberry Pi–controlled imaging platforms for seed and shoot imaging. Images obtained from each platform were suitable for extracting quantifiable plant traits (e.g., shape, area, height, color) en masse using open‐source image processing software such as PlantCV. Conclusions This protocol describes three low‐cost platforms for image acquisition that are useful for quantifying plant diversity. When coupled with open‐source image processing tools, these imaging platforms provide viable low‐cost solutions for incorporating high‐throughput phenomics into a wide range of research programs.},
  keywords = {\_tablet},
  file = {/Users/mavi/Zotero/storage/8VA2D93N/Tovar et al_2018_Raspberry Pi-powered imaging for plant phenotyping.pdf}
}

@article{trivediEarlyDetectionClassification2021,
  title = {Early {{Detection}} and {{Classification}} of {{Tomato Leaf Disease Using High-Performance Deep Neural Network}}},
  author = {Trivedi, Naresh K. and Gautam, Vinay and Anand, Abhineet and Aljahdali, Hani Moaiteq and Villar, Santos Gracia and Anand, Divya and Goyal, Nitin and Kadry, Seifedine},
  date = {2021-11-30},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {21},
  number = {23},
  pages = {7987},
  issn = {1424-8220},
  doi = {10.3390/s21237987},
  url = {https://www.mdpi.com/1424-8220/21/23/7987},
  urldate = {2022-05-16},
  abstract = {Tomato is one of the most essential and consumable crops in the world. Tomatoes differ in quantity depending on how they are fertilized. Leaf disease is the primary factor impacting the amount and quality of crop yield. As a result, it is critical to diagnose and classify these disorders appropriately. Different kinds of diseases influence the production of tomatoes. Earlier identification of these diseases would reduce the disease’s effect on tomato plants and enhance good crop yield. Different innovative ways of identifying and classifying certain diseases have been used extensively. The motive of work is to support farmers in identifying early-stage diseases accurately and informing them about these diseases. The Convolutional Neural Network (CNN) is used to effectively define and classify tomato diseases. Google Colab is used to conduct the complete experiment with a dataset containing 3000 images of tomato leaves affected by nine different diseases and a healthy leaf. The complete process is described: Firstly, the input images are preprocessed, and the targeted area of images are segmented from the original images. Secondly, the images are further processed with varying hyper-parameters of the CNN model. Finally, CNN extracts other characteristics from pictures like colors, texture, and edges, etc. The findings demonstrate that the proposed model predictions are 98.49\% accurate.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/J7AUFTVL/Trivedi et al. - 2021 - Early Detection and Classification of Tomato Leaf .pdf}
}

@online{turnerBayesianOptimizationSuperior2021,
  title = {Bayesian {{Optimization}} Is {{Superior}} to {{Random Search}} for {{Machine Learning Hyperparameter Tuning}}: {{Analysis}} of the {{Black-Box Optimization Challenge}} 2020},
  shorttitle = {Bayesian {{Optimization}} Is {{Superior}} to {{Random Search}} for {{Machine Learning Hyperparameter Tuning}}},
  author = {Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
  date = {2021-08-31},
  eprint = {2104.10201},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2104.10201},
  urldate = {2022-06-23},
  abstract = {This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS 2020 which ran from July–October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the first black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The final leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open-source black-box optimization packages as well as random search.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/25IBTBND/Turner et al. - 2021 - Bayesian Optimization is Superior to Random Search.pdf}
}

@article{tutzOrdinalRegressionReview2022,
  title = {Ordinal Regression: {{A}} Review and a Taxonomy of Models},
  shorttitle = {Ordinal Regression},
  author = {Tutz, Gerhard},
  date = {2022},
  journaltitle = {WIREs Computational Statistics},
  volume = {14},
  number = {2},
  pages = {e1545},
  issn = {1939-0068},
  doi = {10.1002/wics.1545},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1545},
  urldate = {2023-06-22},
  abstract = {Ordinal models can be seen as being composed from simpler, in particular binary models. This view on ordinal models allows to derive a taxonomy of models that includes basic ordinal regression models, models with more complex parameterizations, the class of hierarchically structured models, and the more recently developed finite mixture models. The structured overview that is given covers existing models and shows how models can be extended to account for further effects of explanatory variables. Particular attention is given to the modeling of additional heterogeneity as, for example, dispersion effects. The modeling is embedded into the framework of response styles and the exact meaning of heterogeneity terms in ordinal models is investigated. It is shown that the meaning of terms is crucially determined by the type of model that is used. Moreover, it is demonstrated how models with a complex category-specific effect structure can be simplified to obtain simpler models that fit sufficiently well. The fitting of models is illustrated by use of a real data set, and a short overview of existing software is given. This article is categorized under: Statistical Models {$>$} Fitting Models Data: Types and Structure {$>$} Categorical Data Statistical Models {$>$} Generalized Linear Models},
  langid = {english},
  keywords = {\_tablet,adjacent categories model,cumulative model,hierarchically structured models,ordinal regression,proportional odds model,sequential model},
  file = {/Users/mavi/Zotero/storage/VC9JCJK7/Tutz_2022_Ordinal regression.pdf;/Users/mavi/Zotero/storage/2A2T864A/wics.html}
}

@article{ubbensDeepPlantPhenomics2017,
  title = {Deep {{Plant Phenomics}}: {{A Deep Learning Platform}} for {{Complex Plant Phenotyping Tasks}}},
  shorttitle = {Deep {{Plant Phenomics}}},
  author = {Ubbens, Jordan R. and Stavness, Ian},
  date = {2017-07-07},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {8},
  pages = {1190},
  issn = {1664-462X},
  doi = {10.3389/fpls.2017.01190},
  url = {http://journal.frontiersin.org/article/10.3389/fpls.2017.01190/full},
  urldate = {2022-05-19},
  abstract = {Plant phenomics has received increasing interest in recent years in an attempt to bridge the genotype-to-phenotype knowledge gap. There is a need for expanded high-throughput phenotyping capabilities to keep up with an increasing amount of data from high-dimensional imaging sensors and the desire to measure more complex phenotypic traits (Knecht et al., 2016). In this paper, we introduce an open-source deep learning tool called Deep Plant Phenomics. This tool provides pre-trained neural networks for several common plant phenotyping tasks, as well as an easy platform that can be used by plant scientists to train models for their own phenotyping applications. We report performance results on three plant phenotyping benchmarks from the literature, including state of the art performance on leaf counting, as well as the first published results for the mutant classification and age regression tasks for Arabidopsis thaliana.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/AGHUW3SR/Ubbens and Stavness - 2017 - Deep Plant Phenomics A Deep Learning Platform for.pdf}
}

@article{valenciaIntegratingSustainabilityIndicators2022,
  title = {Integrating Sustainability Indicators and Governance Structures via Clustering Analysis and Multicriteria Decision Making for an Urban Agriculture Network},
  author = {Valencia, Andrea and Qiu, Jiangxiao and Chang, Ni-Bin},
  date = {2022-09-01},
  journaltitle = {Ecological Indicators},
  shortjournal = {Ecological Indicators},
  volume = {142},
  pages = {109237},
  issn = {1470-160X},
  doi = {10.1016/j.ecolind.2022.109237},
  url = {https://www.sciencedirect.com/science/article/pii/S1470160X22007099},
  urldate = {2022-09-29},
  abstract = {Environmental, social, and economic sustainability patterns interact in various dimensions of an urban environment. Exacerbated population growth triggers an emphasis on better resource management strategies addressing the balance of supply and demand over food, energy, and water sectors while considering social and economic development. Promoting sustainable development goals requires governance structures and functions within and across the food, energy, and water sectors, specifically due to polycentric urban development. This study emphasizes food security via an urban agriculture network in the greater Miami metropolitan area, encompassing the three counties of Palm-Beach, Broward, and Miami-Dade. Given the existing governance structure, we quantified several sustainability indices for clustering analysis to agglomerate urban agricultural sites (UASs) and to help identify the priority of clusters in terms of vulnerability or risk level according to their priority index in multicriteria decision-making. The cases of eight clusters were selected for the visualization of the UASs ranked by multicriteria decision-making based on scenarios prioritized for governance under the impacts of climate change, social equity, and economic development. The role of governance structure was highlighted for signifying the incentive programs to enhance the overall sustainability performance of UASs in an urban food–energy–water nexus.},
  langid = {english},
  keywords = {Clustering,Food–Energy–Water Nexus,Governance,Urban Agriculture,Urban Sustainability},
  file = {/Users/mavi/Zotero/storage/CWLPN4E6/Valencia et al. - 2022 - Integrating sustainability indicators and governan.pdf;/Users/mavi/Zotero/storage/4QZNKGZC/S1470160X22007099.html}
}

@article{vallePYMNewAffordable2017,
  title = {{{PYM}}: A New, Affordable, Image-Based Method Using a {{Raspberry Pi}} to Phenotype Plant Leaf Area in a Wide Diversity of Environments},
  shorttitle = {{{PYM}}},
  author = {Valle, Benoît and Simonneau, Thierry and Boulord, Romain and Sourd, Francis and Frisson, Thibault and Ryckewaert, Maxime and Hamard, Philippe and Brichet, Nicolas and Dauzat, Myriam and Christophe, Angélique},
  date = {2017-11-08},
  journaltitle = {Plant Methods},
  shortjournal = {Plant Methods},
  volume = {13},
  number = {1},
  pages = {98},
  issn = {1746-4811},
  doi = {10.1186/s13007-017-0248-5},
  url = {https://doi.org/10.1186/s13007-017-0248-5},
  urldate = {2023-05-17},
  abstract = {Plant science uses increasing amounts of phenotypic data to unravel the complex interactions between biological systems and their variable environments. Originally, phenotyping approaches were limited by manual, often destructive operations, causing large errors. Plant imaging emerged as a viable alternative allowing non-invasive and automated data acquisition. Several procedures based on image analysis were developed to monitor leaf growth as a major phenotyping target. However, in most proposals, a time-consuming parameterization of the analysis pipeline is required to handle variable conditions between images, particularly in the field due to unstable light and interferences with soil surface or weeds. To cope with these difficulties, we developed a low-cost, 2D imaging method, hereafter called PYM. The method is based on plant leaf ability to absorb blue light while reflecting infrared wavelengths. PYM consists of a Raspberry Pi computer equipped with an infrared camera and a blue filter and is associated with scripts that compute projected leaf area. This new method was tested on diverse species placed in contrasting conditions. Application to field conditions was evaluated on lettuces grown under photovoltaic panels. The objective was to look for possible acclimation of leaf expansion under photovoltaic panels to optimise the use of solar radiation per unit soil area.},
  keywords = {\_tablet,Field phenotyping,Image analysis,Infra-red camera,Leaf area measurement,Low cost phenotyping,PYM (raspberry Pi pYthon iMaging),Raspberry Pi},
  file = {/Users/mavi/Zotero/storage/CGII3898/Valle et al_2017_PYM.pdf;/Users/mavi/Zotero/storage/T7ZLAXZP/s13007-017-0248-5.html}
}

@article{vanderwaltScikitimageImageProcessing2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  author = {Van der Walt, Stefan and Schönberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, François and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  date = {2014},
  journaltitle = {PeerJ},
  volume = {2},
  pages = {e453},
  keywords = {read}
}

@online{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-05-20},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/mavi/Zotero/storage/VDPHXM79/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@article{vezzulliDownyMildewResistance2018,
  title = {Downy Mildew Resistance Evaluation in 28 Grapevine Hybrids Promising for Breeding Programs in {{Trentino}} Region ({{Italy}})},
  author = {Vezzulli, Silvia and Vecchione, Antonella and Stefanini, Marco and Zulini, Luca},
  date = {2018-02},
  journaltitle = {European Journal of Plant Pathology},
  shortjournal = {Eur J Plant Pathol},
  volume = {150},
  number = {2},
  pages = {485--495},
  issn = {0929-1873, 1573-8469},
  doi = {10.1007/s10658-017-1298-2},
  url = {http://link.springer.com/10.1007/s10658-017-1298-2},
  urldate = {2022-10-21},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/EE3QRJ5H/Vezzulli et al. - 2018 - Downy mildew resistance evaluation in 28 grapevine.pdf}
}

@article{viaudLeafSegmentationTracking2017,
  title = {Leaf {{Segmentation}} and {{Tracking}} in {{Arabidopsis}} Thaliana {{Combined}} to an {{Organ-Scale Plant Model}} for {{Genotypic Differentiation}}},
  author = {Viaud, Gautier and Loudet, Olivier and Cournède, Paul-Henry},
  date = {2017-01-11},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {7},
  issn = {1664-462X},
  doi = {10.3389/fpls.2016.02057},
  url = {http://journal.frontiersin.org/article/10.3389/fpls.2016.02057/full},
  urldate = {2022-05-16},
  abstract = {A promising method for characterizing the phenotype of a plant as an interaction between its genotype and its environment is to use refined organ-scale plant growth models that use the observation of architectural traits, such as leaf area, containing a lot of information on the whole history of the functioning of the plant. The Phenoscope, a high-throughput automated platform, allowed the acquisition of zenithal images of Arabidopsis thaliana over twenty one days for 4 different genotypes. A novel image processing algorithm involving both segmentation and tracking of the plant leaves allows to extract areas of the latter. First, all the images in the series are segmented independently using a watershed-based approach. A second step based on ellipsoid-shaped leaves is then applied on the segments found to refine the segmentation. Taking into account all the segments at every time, the whole history of each leaf is reconstructed by choosing recursively through time the most probable segment achieving the best score, computed using some characteristics of the segment such as its orientation, its distance to the plant mass center and its area. These results are compared to manually extracted segments, showing a very good accordance in leaf rank and that they therefore provide low-biased data in large quantity for leaf areas. Such data can therefore be exploited to design an organ-scale plant model adapted from the existing GreenLab model for A. thaliana and subsequently parameterize it. This calibration of the model parameters should pave the way for differentiation between the Arabidopsis genotypes.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/M22F236G/Viaud et al_2017_Leaf Segmentation and Tracking in Arabidopsis thaliana Combined to an.pdf}
}

@article{wadumestrigedonaSustainablePathwayPrioritization2022,
  title = {A Sustainable Pathway towards Prioritization of Multifunctional Benefits in Urban Agriculture Contributing to Shrinking Cities in Developing Countries - {{An}} Empirical Case of {{Sri Lanka}}},
  author = {Wadumestrige Dona, Chethika Thamarasi Gunasiri and Mohan, Geetha and Fukushi, Kensuke},
  date = {2022-01-01},
  journaltitle = {Current Research in Environmental Sustainability},
  shortjournal = {Current Research in Environmental Sustainability},
  volume = {4},
  pages = {100183},
  issn = {2666-0490},
  doi = {10.1016/j.crsust.2022.100183},
  url = {https://www.sciencedirect.com/science/article/pii/S2666049022000615},
  urldate = {2022-09-29},
  abstract = {Even though currently at the growth stage, many developing countries are projected to experience urban shrinkage in the future, which demands long-term actions to improve urban sustainability. The study's objective is to investigate the feasibility of using urban agriculture as an open space management strategy under future shrinking and ageing scenarios in developing countries from the urban planning perspective. The study set a future urban shrinking and ageing scenario for the Colombo District, Sri Lanka, for the year 2100. It assessed the perceptions of 93 urban planning professionals about using urban agriculture as a strategy to manage open spaces in the study area in the given future scenarios. These respondents were assumed to be future urban planning professionals living in the year 2100. Results revealed that urban planning professionals highly accept that the multifunctionality of urban agriculture is vital for urban sustainability in future urban shrinkage, and higher preferences were shown for the non-cash benefits, including educational, health, and social benefits. The study also identified the sub-benefits that need to be prioritized under each benefit category and the inter-relationships between them. Finally, it was also found that when these multifunctional benefits are integrated into the urban planning under the future urban shrinkage and ageing scenarios, farmlands that are {$<$}1 acre in size, home gardens, community gardens, and privately owned lands are the most suitable land-use characteristics to be considered.},
  langid = {english},
  keywords = {Developing countries,Multifunctional benefits,Shrinking cities,Urban agriculture},
  file = {/Users/mavi/Zotero/storage/R7HUQEYN/Wadumestrige Dona et al. - 2022 - A sustainable pathway towards prioritization of mu.pdf;/Users/mavi/Zotero/storage/LKHIMSNL/S2666049022000615.html}
}

@article{walshDOMERecommendationsSupervised2021,
  title = {{{DOME}}: Recommendations for Supervised Machine Learning Validation in Biology},
  shorttitle = {{{DOME}}},
  author = {Walsh, Ian and Fishman, Dmytro and Garcia-Gasulla, Dario and Titma, Tiina and Pollastri, Gianluca and Harrow, Jennifer and Psomopoulos, Fotis E. and Tosatto, Silvio C. E.},
  date = {2021-10},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {18},
  number = {10},
  pages = {1122--1127},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01205-4},
  url = {https://www.nature.com/articles/s41592-021-01205-4},
  urldate = {2022-11-17},
  abstract = {DOME is a set of community-wide recommendations for reporting supervised machine learning–based analyses applied to biological studies. Broad adoption of these recommendations will help improve machine learning assessment and reproducibility.},
  issue = {10},
  langid = {english},
  keywords = {Machine learning,Software,Standards},
  file = {/Users/mavi/Zotero/storage/9KGUBS5L/Walsh et al. - 2021 - DOME recommendations for supervised machine learn.pdf;/Users/mavi/Zotero/storage/8WXCI7JQ/s41592-021-01205-4.html}
}

@article{wangAssessingSustainableUrban2022,
  title = {Assessing Sustainable Urban Development Based on Functional Spatial Differentiation of Urban Agriculture in {{Wuhan}}, {{China}}},
  author = {Wang, Min and Yuan, Mengchu and Han, Pingyang and Wang, Dan},
  date = {2022-04-01},
  journaltitle = {Land Use Policy},
  shortjournal = {Land Use Policy},
  volume = {115},
  pages = {105999},
  issn = {0264-8377},
  doi = {10.1016/j.landusepol.2022.105999},
  url = {https://www.sciencedirect.com/science/article/pii/S0264837722000266},
  urldate = {2022-09-29},
  abstract = {Harmonized development of urban agricultural functions is necessary to ensure sustainable urban development, in which rational utilization of land plays a key role. With rapid urbanization, urban agricultural land has been encroached, the development of urban agriculture functions has become unbalanced, and differences between land, agriculture, and urban systems have become more evident. In this study, we analyzed the characteristics of land use structure and the functional spatial differentiation of urban agriculture in Wuhan and studied the quantitative relationship and influence mechanisms between them. We found that the spatial differentiation of urban agricultural function in Wuhan was closely related to the urban developmental structure, comprehensive index of land use degree, and information entropy of land use structure; moreover, urban agricultural land rate showed different effects on urban agricultural functions of different clusters. Based on these results, we proposed optimization strategies for streets and towns with different development levels of urban agricultural functions to promote sustainable development of both urban agriculture and cities through the rational use of land.},
  langid = {english},
  keywords = {Land use,Spatial differentiation,Sustainable urban development,Urban agriculture function},
  file = {/Users/mavi/Zotero/storage/Z9QLU8BM/Wang et al. - 2022 - Assessing sustainable urban development based on f.pdf;/Users/mavi/Zotero/storage/DNLQXFQB/S0264837722000266.html}
}

@article{wangRapidDetectionMethod2022,
  title = {A {{Rapid Detection Method}} for {{Fungal Spores}} from {{Greenhouse Crops Based}} on {{CMOS Image Sensors}} and {{Diffraction Fingerprint Feature Processing}}},
  author = {Wang, Yafei and Mao, Hanping and Xu, Guilin and Zhang, Xiaodong and Zhang, Yakun},
  date = {2022-04-06},
  journaltitle = {Journal of Fungi},
  shortjournal = {JoF},
  volume = {8},
  number = {4},
  pages = {374},
  issn = {2309-608X},
  doi = {10.3390/jof8040374},
  url = {https://www.mdpi.com/2309-608X/8/4/374},
  urldate = {2022-06-23},
  abstract = {The detection and control of fungal spores in greenhouse crops are important for stabilizing and increasing crop yield. At present, the detection of fungal spores mainly adopts the method of combining portable volumetric spore traps and microscope image processing. This method is problematic as it is limited by the small field of view of the microscope and has low efficiency. This study proposes a rapid detection method for fungal spores from greenhouse crops based on CMOS image sensors and diffraction fingerprint feature processing. We built a diffraction fingerprint image acquisition system for fungal spores of greenhouse crops and collected diffraction fingerprint images of three kinds of fungal spores. A total of 13 diffraction fingerprint features were selected for the classification of fungal spores. These 13 characteristic values were divided into 3 categories, main bright fringe, main dark fringe, and center fringe. Then, these three features were calculated to obtain the Peak to Center ratio (PCR), Valley to Center ratio, and Peak to Valley ratio (PVR). Based on these features, logistics regression (LR), K nearest neighbor (KNN), random forest (RF), and support vector machine (SVM) classification models were built. The test results show that the SVM model has a better overall classification performance than the LR, KNN, and RF models. The average accuracy rate of the recognition of three kinds of fungal spores from greenhouse crops under the SVM model was 92.72\%, while the accuracy rates of the LR, KNN, and RF models were 84.97\%, 87.44\%, and 88.72\%, respectively. The F1-Score value of the SVM model was higher, and the overall average value reached 89.41\%, which was 11.12\%, 7.18\%, and 5.57\% higher than the LR, KNN, and RF models, respectively. Therefore, the method proposed in this study can be used for the remote identification of three fungal spores which can provide a reference for the identification of fungal spores in greenhouse crops and has the advantages of low cost and portability.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/KGNS7INC/Wang et al. - 2022 - A Rapid Detection Method for Fungal Spores from Gr.pdf}
}

@article{wangReviewDeepLearning2022,
  title = {A {{Review}} of {{Deep Learning}} in {{Multiscale Agricultural Sensing}}},
  author = {Wang, Dashuai and Cao, Wujing and Zhang, Fan and Li, Zhuolin and Xu, Sheng and Wu, Xinyu},
  date = {2022-01-25},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {14},
  number = {3},
  pages = {559},
  issn = {2072-4292},
  doi = {10.3390/rs14030559},
  url = {https://www.mdpi.com/2072-4292/14/3/559},
  urldate = {2022-05-16},
  abstract = {Population growth, climate change, and the worldwide COVID-19 pandemic are imposing increasing pressure on global agricultural production. The challenge of increasing crop yield while ensuring sustainable development of environmentally friendly agriculture is a common issue throughout the world. Autonomous systems, sensing technologies, and artificial intelligence offer great opportunities to tackle this issue. In precision agriculture (PA), non-destructive and non-invasive remote and proximal sensing methods have been widely used to observe crops in visible and invisible spectra. Nowadays, the integration of high-performance imagery sensors (e.g., RGB, multispectral, hyperspectral, thermal, and SAR) and unmanned mobile platforms (e.g., satellites, UAVs, and terrestrial agricultural robots) are yielding a huge number of high-resolution farmland images, in which rich crop information is compressed. However, this has been accompanied by challenges, i.e., ways to swiftly and efficiently making full use of these images, and then, to perform fine crop management based on information-supported decision making. In the past few years, deep learning (DL) has shown great potential to reshape many industries because of its powerful capabilities of feature learning from massive datasets, and the agriculture industry is no exception. More and more agricultural scientists are paying attention to applications of deep learning in image-based farmland observations, such as land mapping, crop classification, biotic/abiotic stress monitoring, and yield prediction. To provide an update on these studies, we conducted a comprehensive investigation with a special emphasis on deep learning in multiscale agricultural remote and proximal sensing. Specifically, the applications of convolutional neural network-based supervised learning (CNN-SL), transfer learning (TL), and few-shot learning (FSL) in crop sensing at land, field, canopy, and leaf scales are the focus of this review. We hope that this work can act as a reference for the global agricultural community regarding DL in PA and can inspire deeper and broader research to promote the evolution of modern agriculture.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/9MTW272G/Wang et al. - 2022 - A Review of Deep Learning in Multiscale Agricultur.pdf}
}

@article{wangSpatiotemporalEvolutionUrbanagriculturalecological2022,
  title = {Spatiotemporal Evolution of Urban-Agricultural-Ecological Space in {{China}} and Its Driving Mechanism},
  author = {Wang, Di and Fu, Jingying and Xie, Xiaolan and Ding, Fangyu and Jiang, Dong},
  date = {2022-10-15},
  journaltitle = {Journal of Cleaner Production},
  shortjournal = {Journal of Cleaner Production},
  volume = {371},
  pages = {133684},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2022.133684},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652622032619},
  urldate = {2022-09-29},
  abstract = {Coordinating urban development, food security, and ecological protection is a prerequisite for sustainable development. However, with a large population and rapid urbanization, there are still many challenges to optimize the urban-agricultural-ecological space. Here, taking China as a case in point, we explored the dynamic pattern of urban-agricultural-ecological space from 2000 to 2020. The driving mechanism of urban-agricultural-ecological space was simulated with the Maximum Entropy (MaxEnt) Model at both the national and regional scales. The results indicated that about 1/8 of urban-agricultural-ecological space types in mainland China have been changed from 2000 to 2020, most of which is the transformation between agricultural space and ecological space. According to the main type of urban-agricultural-ecological space change, there could be classified into four different zones from the east coast to the west inland in mainland China. As a whole, socioeconomic factors have more contributions to the urban-agricultural-ecological space changes, especially the GDP density for the transformation between agricultural space and ecological space (the relative importance was 61.0\%), the proportion of residential area for the transformation between agricultural space and urban space (the relative importance was 38.1\%), and the initial land-use type for the transformation between urban space and ecological space (the relative importance was 30.9\%). These findings may support the scientifically delineation of urban space, agricultural space, and ecological space (three districts and three lines) in China.},
  langid = {english},
  keywords = {Driving mechanism,Machine learning method,Maximum entropy (MaxEnt) model,Spatiotemporal evolution,Sustainable development goals (SDGs),Urban-agricultural-ecological space},
  file = {/Users/mavi/Zotero/storage/VUQ28AH9/Wang et al. - 2022 - Spatiotemporal evolution of urban-agricultural-eco.pdf;/Users/mavi/Zotero/storage/U8CPL759/S0959652622032619.html}
}

@article{wardDeepLeafSegmentation,
  title = {Deep {{Leaf Segmentation Using Synthetic Data}}},
  author = {Ward, Daniel and Ward, Daniel and Moghadam, Peyman and Moghadam, Peyman and Hudson, Nicolas and Hudson, Nicolas},
  pages = {13},
  abstract = {Automated segmentation of individual leaves of a plant in an image is a prerequisite to measure more complex phenotypic traits in high-throughput phenotyping. Applying state-of-the-art machine learning approaches to tackle leaf instance segmentation requires a large amount of manually annotated training data. Currently, the benchmark datasets for leaf segmentation contain only a few hundred labeled training images. In this paper, we propose a framework for leaf instance segmentation by augmenting real plant datasets with generated synthetic images of plants inspired by domain randomisation. We train a state-of-the-art deep learning segmentation architecture (Mask-RCNN) with a combination of real and synthetic images of Arabidopsis plants. Our proposed approach achieves 90\% leaf segmentation score on the A1 test set outperforming the-state-of-theart approaches for the CVPPP Leaf Segmentation Challenge (LSC). Our approach also achieves 81\% mean performance over all five test datasets.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/BEB5R4EG/Ward et al_Deep Leaf Segmentation Using Synthetic Data.pdf}
}

@article{weiEmergentUrbanAgricultural2022,
  title = {Emergent Urban Agricultural Practices and Attitudes in the Residential Area in {{China}}},
  author = {Wei, Yuan and Jones, Paul},
  date = {2022-03-01},
  journaltitle = {Urban Forestry \& Urban Greening},
  shortjournal = {Urban Forestry \& Urban Greening},
  volume = {69},
  pages = {127491},
  issn = {1618-8667},
  doi = {10.1016/j.ufug.2022.127491},
  url = {https://www.sciencedirect.com/science/article/pii/S1618866722000346},
  urldate = {2022-09-29},
  abstract = {This paper explores the emergence of urban agriculture and its changing nature and role in the urbanization process in China. With an increase in urban agricultural pursuits in both planned and unplanned residential areas, the focus of the paper is primarily on better understanding the nature of emerging ‘informal’ practices in built and unbuilt spaces. By using a planned residential area in Kunming, Yunnan Province as a case study, this paper identifies the physical and spatial expressions of urban agriculture practices as well as the motivations and attitudes of stakeholders. The results show that over half of the participants had experience in undertaking urban agricultural practices, with the predominant reason being to fulfill personal interests, including meeting household needs. When undertaking urban agricultural practices, local residents demonstrate their divergent abilities and skills in negotiating, adapting, and managing both private and increasingly incursions into the public domain. Analysis of the empirical results indicates that urban agricultural pursuits are a viable practice which supports the livelihoods and wellbeing of the residents by using innovative, creative and emergent forms in both private and public spaces.},
  langid = {english},
  keywords = {Adaptation,China,Emergent practices,Urban agriculture,Urban planning,Urbanization},
  file = {/Users/mavi/Zotero/storage/VWS8R9RQ/Wei and Jones - 2022 - Emergent urban agricultural practices and attitude.pdf;/Users/mavi/Zotero/storage/5838E4BF/S1618866722000346.html}
}

@article{weiGeneticUNetAutomatically2021,
  title = {Genetic {{U-Net}}: {{Automatically Designed Deep Networks}} for {{Retinal Vessel Segmentation Using}} a {{Genetic Algorithm}}},
  shorttitle = {Genetic {{U-Net}}},
  author = {Wei, Jiahong and Zhu, Guijie and Fan, Zhun and Liu, Jinchao and Yibiao, Rong and Mo, Jiajie and Li, Wenji and Chen, Xinjian},
  date = {2021-09-10},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Transactions on Medical Imaging},
  volume = {PP},
  pages = {1--1},
  doi = {10.1109/TMI.2021.3111679},
  abstract = {Recently, many methods based on hand-designed convolutional neural networks (CNNs) have achieved promising results in automatic retinal vessel segmentation. However, these CNNs remain constrained in capturing retinal vessels in complex fundus images. To improve their segmentation performance, these CNNs tend to have many parameters, which may lead to overfitting and high computational complexity. Moreover, the manual design of competitive CNNs is time-consuming and requires extensive empirical knowledge. Herein, a novel automated design method, called Genetic U-Net, is proposed to generate a U-shaped CNN that can achieve better retinal vessel segmentation but with fewer architecture-based parameters, thereby addressing the above issues. First, we devised a condensed but flexible search space based on a U-shaped encoder-decoder. Then, we used an improved genetic algorithm to identify better-performing architectures in the search space and investigated the possibility of finding a superior network architecture with fewer parameters. The experimental results show that the architecture obtained using the proposed method offered a superior performance with less than 1\% of the number of the original U-Net parameters in particular and with significantly fewer parameters than other state-of-the-art models. Furthermore, through in-depth investigation of the experimental results, several effective operations and patterns of networks to generate superior retinal vessel segmentations were identified. The codes of this work are available at  https://github.com/96jhwei/Genetic-U-Net .},
  file = {/Users/mavi/Zotero/storage/Z43EWZ38/Wei et al_2021_Genetic U-Net.pdf}
}

@inproceedings{weigertStarconvexPolyhedra3D2020,
  title = {Star-Convex {{Polyhedra}} for {{3D Object Detection}} and {{Segmentation}} in {{Microscopy}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Weigert, Martin and Schmidt, Uwe and Haase, Robert and Sugawara, Ko and Myers, Gene},
  date = {2020-03},
  eprint = {1908.03636},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {3655--3662},
  doi = {10.1109/WACV45572.2020.9093435},
  url = {http://arxiv.org/abs/1908.03636},
  urldate = {2022-05-31},
  abstract = {Accurate detection and segmentation of cell nuclei in volumetric (3D) fluorescence microscopy datasets is an important step in many biomedical research projects. Although many automated methods for these tasks exist, they often struggle for images with low signal-to-noise ratios and/or dense packing of nuclei. It was recently shown for 2D microscopy images that these issues can be alleviated by training a neural network to directly predict a suitable shape representation (star-convex polygon) for cell nuclei. In this paper, we adopt and extend this approach to 3D volumes by using star-convex polyhedra to represent cell nuclei and similar shapes. To that end, we overcome the challenges of 1) finding parameter-efficient star-convex polyhedra representations that can faithfully describe cell nuclei shapes, 2) adapting to anisotropic voxel sizes often found in fluorescence microscopy datasets, and 3) efficiently computing intersections between pairs of star-convex polyhedra (required for non-maximum suppression). Although our approach is quite general, since star-convex polyhedra include common shapes like bounding boxes and spheres as special cases, our focus is on accurate detection and segmentation of cell nuclei. Finally, we demonstrate on two challenging datasets that our approach (STARDIST-3D) leads to superior results when compared to classical and deep learning based methods.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/FZSSLYDX/Weigert et al. - 2020 - Star-convex Polyhedra for 3D Object Detection and .pdf}
}

@article{wekslerPepperPlantsLeaf2021,
  title = {Pepper {{Plants Leaf Spectral Reflectance Changes}} as a {{Result}} of {{Root Rot Damage}}},
  author = {Weksler, Shahar and Rozenstein, Offer and Haish, Nadav and Moshelion, Menachem and Wallach, Rony and Ben-Dor, Eyal},
  date = {2021-03-04},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sensing},
  volume = {13},
  number = {5},
  pages = {980},
  issn = {2072-4292},
  doi = {10.3390/rs13050980},
  url = {https://www.mdpi.com/2072-4292/13/5/980},
  urldate = {2022-05-16},
  abstract = {Symptoms of root stress are hard to detect using non-invasive tools. This study reveals proof of concept for vegetation indices’ ability, usually used to sense canopy status, to detect root stress, and performance status. Pepper plants were grown under controlled greenhouse conditions under different potassium and salinity treatments. The plants’ spectral reflectance was measured on the last day of the experiment when more than half of the plants were already naturally infected by root disease. Vegetation indices were calculated for testing the capability to distinguish between healthy and root-damaged plants using spectral measurements. While no visible symptoms were observed in the leaves, the vegetation indices and red-edge position showed clear differences between the healthy and the root-infected plants. These results were achieved after a growth period of 32 days, indicating the ability to monitor root damage at an early growing stage using leaf spectral reflectance.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/SS58NLAG/Weksler et al. - 2021 - Pepper Plants Leaf Spectral Reflectance Changes as.pdf}
}

@online{wolfHuggingFaceTransformersStateoftheart2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  date = {2020-07-13},
  eprint = {1910.03771},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.03771},
  url = {http://arxiv.org/abs/1910.03771},
  urldate = {2023-05-03},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textbackslash textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textbackslash textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \textbackslash url\{https://github.com/huggingface/transformers\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,read},
  file = {/Users/mavi/Zotero/storage/J3IU5SYH/Wolf et al_2020_HuggingFace's Transformers.pdf;/Users/mavi/Zotero/storage/2XR2GB3N/1910.html}
}

@article{woyzichovskiWorkflowLowcostAutomated2021,
  title = {A Workflow for Low-Cost Automated Image Analysis of Myxomycete Spore Numbers, Size and Shape},
  author = {Woyzichovski, Jan and Shchepin, Oleg and Dagamac, Nikki Heherson and Schnittler, Martin},
  date = {2021-11-16},
  journaltitle = {PeerJ},
  volume = {9},
  pages = {e12471},
  issn = {2167-8359},
  doi = {10.7717/peerj.12471},
  url = {https://peerj.com/articles/12471},
  urldate = {2022-05-20},
  abstract = {Measuring spore size is a standard method for the description of fungal taxa, but in manual microscopic analyses the number of spores that can be measured and information on their morphological traits are typically limited. To overcome this weakness we present a method to analyze the size and shape of large numbers of spherical bodies, such as spores or pollen, by using inexpensive equipment. A spore suspension mounted on a slide is treated with a low-cost, high-vibration device to distribute spores uniformly in a single layer without overlap. Subsequently, 10,000 to 50,000 objects per slide are measured by automated image analysis. The workflow involves (1) slide preparation, (2) automated image acquisition by light microscopy, (3) filtering to separate high-density clusters, (4) image segmentation by applying a machine learning software, Waikato Environment for Knowledge Analysis (WEKA), and (5) statistical evaluation of the results. The technique produced consistent results and compared favorably with manual measurements in terms of precision. Moreover, measuring spore size distribution yields information not obtained by manual microscopic analyses, as shown for the myxomycete Physarum albescens. The exact size distribution of spores revealed irregularities in spore formation resulting from the influence of environmental conditions on spore maturation. A comparison of the spore size distribution within and between sporocarp colonies showed large environmental and likely genetic variation. In addition, the comparison identified specimens with spores roughly twice the normal size. The successful implementation of the presented method for analyzing myxomycete spores also suggests potential for other applications.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/LI7KRE4G/Woyzichovski et al. - 2021 - A workflow for low-cost automated image analysis o.pdf}
}

@inproceedings{wuBidirectionalGraphReasoning2020,
  title = {Bidirectional {{Graph Reasoning Network}} for {{Panoptic Segmentation}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wu, Yangxin and Zhang, Gengwei and Gao, Yiming and Deng, Xiajun and Gong, Ke and Liang, Xiaodan and Lin, Liang},
  date = {2020-06},
  pages = {9077--9086},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00910},
  url = {https://ieeexplore.ieee.org/document/9157651/},
  urldate = {2022-05-17},
  abstract = {Recent researches on panoptic segmentation resort to a single end-to-end network to combine the tasks of instance segmentation and semantic segmentation. However, prior models only unified the two related tasks at the architectural level via a multi-branch scheme or revealed the underlying correlation between them by unidirectional feature fusion, which disregards the explicit semantic and co-occurrence relations among objects and background. Inspired by the fact that context information is critical to recognize and localize the objects, and inclusive object details are significant to parse the background scene, we thus investigate on explicitly modeling the correlations between object and background to achieve a holistic understanding of an image in the panoptic segmentation task. We introduce a Bidirectional Graph Reasoning Network (BGRNet), which incorporates graph structure into the conventional panoptic segmentation network to mine the intra-modular and intermodular relations within and between foreground things and background stuff classes. In particular, BGRNet first constructs image-specific graphs in both instance and semantic segmentation branches that enable flexible reasoning at the proposal level and class level, respectively. To establish the correlations between separate branches and fully leverage the complementary relations between things and stuff, we propose a Bidirectional Graph Connection Module to diffuse information across branches in a learnable fashion. Experimental results demonstrate the superiority of our BGRNet that achieves the new state-of-the-art performance on challenging COCO and ADE20K panoptic segmentation benchmarks.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/9GJV7KTY/Wu et al. - 2020 - Bidirectional Graph Reasoning Network for Panoptic.pdf}
}

@online{wuDemystifyingLearningRate2019,
  title = {Demystifying {{Learning Rate Policies}} for {{High Accuracy Training}} of {{Deep Neural Networks}}},
  author = {Wu, Yanzhao and Liu, Ling and Bae, Juhyun and Chow, Ka-Ho and Iyengar, Arun and Pu, Calton and Wei, Wenqi and Yu, Lei and Zhang, Qi},
  date = {2019-10-26},
  eprint = {1908.06477},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.06477},
  urldate = {2022-06-23},
  abstract = {Learning Rate (LR) is an important hyperparameter to tune for effective training of deep neural networks (DNNs). Even for the baseline of a constant learning rate, it is non-trivial to choose a good constant value for training a DNN. Dynamic learning rates involve multi-step tuning of LR values at various stages of the training process and offer high accuracy and fast convergence. However, they are much harder to tune. In this paper, we present a comprehensive study of 13 learning rate functions and their associated LR policies by examining their range parameters, step parameters, and value update parameters. We propose a set of metrics for evaluating and selecting LR policies, including the classification confidence, variance, cost, and robustness, and implement them in LRBench, an LR benchmarking system. LRBench can assist end-users and DNN developers to select good LR policies and avoid bad LR policies for training their DNNs. We tested LRBench on Caffe, an open source deep learning framework, to showcase the tuning optimization of LR policies. Evaluated through extensive experiments, we attempt to demystify the tuning of LR policies by identifying good LR policies with effective LR value ranges and step sizes for LR update schedules.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/YPY889ES/Wu et al. - 2019 - Demystifying Learning Rate Policies for High Accur.pdf}
}

@online{xiCapsuleNetworkPerformance2017,
  title = {Capsule {{Network Performance}} on {{Complex Data}}},
  author = {Xi, Edgar and Bing, Selina and Jin, Yang},
  date = {2017-12-10},
  eprint = {1712.03480},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1712.03480},
  urldate = {2022-05-25},
  abstract = {In recent years, convolutional neural networks (CNN) have played an important role in the field of deep learning. Variants of CNN's have proven to be very successful in classification tasks across different domains. However, there are two big drawbacks to CNN's: their failure to take into account of important spatial hierarchies between features, and their lack of rotational invariance. As long as certain key features of an object are present in the test data, CNN's classify the test data as the object, disregarding features' relative spatial orientation to each other. This causes false positives. The lack of rotational invariance in CNN's would cause the network to incorrectly assign the object another label, causing false negatives. To address this concern, Hinton et al. propose a novel type of neural network using the concept of capsules in a recent paper. With the use of dynamic routing and reconstruction regularization, the capsule network model would be both rotation invariant and spatially aware. The capsule network has shown its potential by achieving a state-of-the-art result of 0.25\% test error on MNIST without data augmentation such as rotation and scaling, better than the previous baseline of 0.39\%. To further test out the application of capsule networks on data with higher dimensionality, we attempt to find the best set of configurations that yield the optimal test error on CIFAR10 dataset.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/KRXLMMWT/Xi et al. - 2017 - Capsule Network Performance on Complex Data.pdf}
}

@article{xieSegFormerSimpleEfficient,
  title = {{{SegFormer}}: {{Simple}} and {{Efﬁcient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  pages = {14},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5× smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/VII2RG2V/Xie et al. - SegFormer Simple and Efﬁcient Design for Semantic.pdf}
}

@article{xinPseudomonasSyringaePv2013a,
  title = {Pseudomonas Syringae Pv. Tomato {{DC3000}}: {{A Model Pathogen}} for {{Probing Disease Susceptibility}} and {{Hormone Signaling}} in {{Plants}}},
  shorttitle = {Pseudomonas Syringae Pv. Tomato {{DC3000}}},
  author = {Xin, Xiu-Fang and He, Sheng Yang},
  date = {2013},
  journaltitle = {Annual Review of Phytopathology},
  volume = {51},
  number = {1},
  eprint = {23725467},
  eprinttype = {pmid},
  pages = {473--498},
  doi = {10.1146/annurev-phyto-082712-102321},
  url = {https://doi.org/10.1146/annurev-phyto-082712-102321},
  urldate = {2023-04-24},
  abstract = {Since the early 1980s, various strains of the gram-negative bacterial pathogen Pseudomonas syringae have been used as models for understanding plant-bacterial interactions. In 1991, a P. syringae pathovar tomato (Pst) strain, DC3000, was reported to infect not only its natural host tomato but also Arabidopsis in the laboratory, a finding that spurred intensive efforts in the subsequent two decades to characterize the molecular mechanisms by which this strain causes disease in plants. Genomic analysis shows that Pst DC3000 carries a large repertoire of potential virulence factors, including proteinaceous effectors that are secreted through the type III secretion system and a polyketide phytotoxin called coronatine, which structurally mimics the plant hormone jasmonate (JA). Study of Pst DC3000 pathogenesis has not only provided several conceptual advances in understanding how a bacterial pathogen employs type III effectors to suppress plant immune responses and promote disease susceptibility but has also facilitated the discovery of the immune function of stomata and key components of JA signaling in plants. The concepts derived from the study of Pst DC3000 pathogenesis may prove useful in understanding pathogenesis mechanisms of other plant pathogens.},
  keywords = {pathogenesis,plant hormone,plant immunity,plant pathogen,stomata,type III effector},
  file = {/Users/mavi/Zotero/storage/YAEY7PRU/Xin_He_2013_Pseudomonas syringae pv.pdf}
}

@online{yinJointMultiLeafSegmentation2017,
  title = {Joint {{Multi-Leaf Segmentation}}, {{Alignment}} and {{Tracking}} from {{Fluorescence Plant Videos}}},
  author = {Yin, Xi and Liu, Xiaoming and Chen, Jin and Kramer, David M.},
  date = {2017-05-09},
  eprint = {1505.00353},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1505.00353},
  urldate = {2022-05-17},
  abstract = {This paper proposes a novel framework for fluorescence plant video processing. The plant research community is interested in the leaf-level photosynthetic analysis within a plant. A prerequisite for such analysis is to segment all leaves, estimate their structures, and track them over time. We identify this as a joint multi-leaf segmentation, alignment, and tracking problem. First, leaf segmentation and alignment are applied on the last frame of a plant video to find a number of well-aligned leaf candidates. Second, leaf tracking is applied on the remaining frames with leaf candidate transformation from the previous frame. We form two optimization problems with shared terms in their objective functions for leaf alignment and tracking respectively. A quantitative evaluation framework is formulated to evaluate the performance of our algorithm with four metrics. Two models are learned to predict the alignment accuracy and detect tracking failure respectively in order to provide guidance for subsequent plant biology analysis. The limitation of our algorithm is also studied. Experimental results show the effectiveness, efficiency, and robustness of the proposed method.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/ZJQT8RQC/Yin et al. - 2017 - Joint Multi-Leaf Segmentation, Alignment and Track.pdf}
}

@article{zackAutomaticMeasurementSister1977,
  title = {Automatic Measurement of Sister Chromatid Exchange Frequency.},
  author = {Zack, G W and Rogers, W E and Latt, S A},
  date = {1977-07},
  journaltitle = {Journal of Histochemistry \& Cytochemistry},
  shortjournal = {J Histochem Cytochem.},
  volume = {25},
  number = {7},
  pages = {741--753},
  issn = {0022-1554, 1551-5044},
  doi = {10.1177/25.7.70454},
  url = {http://journals.sagepub.com/doi/10.1177/25.7.70454},
  urldate = {2022-06-09},
  abstract = {An automatic system for detecting and counting sister chromatid exchanges in human chromosomes has been developed. Metaphase chromosomes from lymphocytes which had incorporated 5-bromodeoxyuridine for two replication cycles were treated with the dye 33258 Hoechst and photodegraded so that the sister chromatids exhibited differential Giemsa staining. A computer-controlled television-microscope system was used to acquire digitized metaphase spread images by direct scanning of microscope slides. Individual objects in the images were identified by a thresholding procedure. The probability that each object was a single, separate chromosome was estimated from size and shape measurements. An analysis of the spatial relationships of the dark-chromatid regions of each object yielded a set of possible exchange locations and estimated probabilities that such locations corresponded to sister chromatid exchanges. A normalized estimate of the sister chromatid exchange frequency was obtained by summing the joint probabilities that a location contained an exchange within a single, separate chromosome over the set of chromosomes from one or more cells and dividing by the expected value of the total chromosome area analyzed. Comparison with manual scoring of exchanges showed satisfactory agreement up to levels of approximately 30 sister chromatid exchanges/cell, or slightly more than twice control levels. The processing time for this automated sister chromatid exchange detection system was comparable to that of manual scoring.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/F59M5WDT/Zack et al. - 1977 - Automatic measurement of sister chromatid exchange.pdf}
}

@report{zendlerHighthroughputPhenotypingLeaf2021,
  type = {preprint},
  title = {High-Throughput Phenotyping of Leaf Discs Infected with Grapevine Downy Mildew Using Shallow Convolutional Neural Networks},
  author = {Zendler, Daniel and Malagol, Nagarjun and Schwandner, Anna and Töpfer, Reinhard and Hausmann, Ludger and Zyprian, Eva},
  date = {2021-08-19},
  institution = {{Plant Biology}},
  doi = {10.1101/2021.08.19.456931},
  url = {http://biorxiv.org/lookup/doi/10.1101/2021.08.19.456931},
  urldate = {2022-06-02},
  abstract = {Objective and standardized recording of disease severity in mapping crosses and breeding lines is a crucial step in characterizing resistance traits utilized in breeding programs and to conduct QTL or GWAS studies. Here we report a system for automated high-throughput scoring of disease severity on inoculated leaf discs. As proof of concept, we used leaf discs inoculated with Plasmopara viticola ((Berk. and Curt.) Berl. and de Toni) causing grapevine downy mildew (DM). This oomycete is one of the major grapevine pathogens and has the potential to reduce grape yield dramatically if environmental conditions are favorable. Breeding of DM resistant grapevine cultivars is an approach for a novel and more sustainable viticulture. This involves the evaluation of several thousand inoculated leaf discs from mapping crosses and breeding lines every year. Therefore, we trained a shallow convolutional neural-network (SCNN) for efficient detection of leaf disc segments showing P. viticola sporangiophores. We could illustrate a high and significant correlation with manually scored disease severity used as ground truth data for evaluation of the SCNN performance. Combined with an automated imaging system, this leaf disc-scoring pipeline has the potential to considerably reduce the amount of time during leaf disc phenotyping. The pipeline with all necessary documentation for adaptation to other pathogens is freely available.},
  langid = {english},
  keywords = {CNN,Leaf disc,Mildiou,OIV,read,SCNN,Vine},
  file = {/Users/mavi/Zotero/storage/VIZTV72U/Zendler et al. - 2021 - High-throughput phenotyping of leaf discs infected.pdf}
}

@online{zhaiLargescaleStudyRepresentation2020,
  title = {A {{Large-scale Study}} of {{Representation Learning}} with the {{Visual Task Adaptation Benchmark}}},
  author = {Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and Beyer, Lucas and Bachem, Olivier and Tschannen, Michael and Michalski, Marcin and Bousquet, Olivier and Gelly, Sylvain and Houlsby, Neil},
  date = {2020-02-21},
  eprint = {1910.04867},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.04867},
  urldate = {2022-05-16},
  abstract = {Representation learning promises to unlock deep learning for the long tail of vision tasks without expensive labelled datasets. Yet, the absence of a unified evaluation for general visual representations hinders progress. Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error). We present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. With VTAB, we conduct a large-scale study of many popular publicly-available representation learning algorithms. We carefully control confounders such as architecture and tuning budget. We address questions like: How effective are ImageNet representations beyond standard natural datasets? How do representations trained via generative and discriminative models compare? To what extent can self-supervision replace labels? And, how close are we to general visual representations?},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/JRIHYXKG/Zhai et al. - 2020 - A Large-scale Study of Representation Learning wit.pdf}
}

@article{zhangAttentionResidualLearning2019,
  title = {Attention {{Residual Learning}} for {{Skin Lesion Classification}}},
  author = {Zhang, Jianpeng and Xie, Yutong and Xia, Yong and Shen, Chunhua},
  date = {2019-09},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {38},
  number = {9},
  pages = {2092--2103},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2019.2893944},
  url = {https://ieeexplore.ieee.org/document/8620285/},
  urldate = {2022-05-19},
  abstract = {Automated skin lesion classification in dermoscopy images is an essential way to improve the diagnostic performance and reduce melanoma deaths. Although deep convolutional neural networks (DCNNs) have made dramatic breakthroughs in many image classification tasks, accurate classification of skin lesions remains challenging due to the insufficiency of training data, inter-class similarity, intra-class variation, and lack of the ability to focus on semantically meaningful lesion parts. To address these issues, we propose an attention residual learning convolutional neural network (ARL-CNN) model for skin lesion classification in dermoscopy images, which is composed of multiple ARL blocks, a global average pooling layer, and a classification layer. Each ARL block jointly uses the residual learning and a novel attention learning mechanisms to improve its ability for discriminative representation. Instead of using extra learnable layers, the proposed attention learning mechanism aims to exploit the intrinsic self-attention ability of DCNNs, i.e. using the feature maps learned by a high layer to generate the attention map for a low layer. We evaluated our ARL-CNN model on the ISIC-skin 2017 dataset. Our results indicate that the proposed ARL-CNN model can adaptively focus on the discriminative parts of skin lesions, and thus achieve the state-of-the-art performance in skin lesion classification.},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/KRR9DKQY/Zhang et al. - 2019 - Attention Residual Learning for Skin Lesion Classi.pdf}
}

@article{zhangDeepLearningBased2022,
  title = {Deep {{Learning Based Automatic Grape Downy Mildew Detection}}},
  author = {Zhang, Zhao and Qiao, Yongliang and Guo, Yangyang and He, Dongjian},
  date = {2022},
  journaltitle = {Frontiers in Plant Science},
  volume = {13},
  issn = {1664-462X},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2022.872107},
  urldate = {2023-04-25},
  abstract = {Grape downy mildew (GDM) disease is a common plant leaf disease, and it causes serious damage to grape production, reducing yield and fruit quality. Traditional manual disease detection relies on farm experts and is often time-consuming. Computer vision technologies and artificial intelligence could provide automatic disease detection for real-time controlling the spread of disease on the grapevine in precision viticulture. To achieve the best trade-off between GDM detection accuracy and speed under natural environments, a deep learning based approach named YOLOv5-CA is proposed in this study. Here coordinate attention (CA) mechanism is integrated into YOLOv5, which highlights the downy mildew disease-related visual features to enhance the detection performance. A challenging GDM dataset was acquired in a vineyard under a nature scene (consisting of different illuminations, shadows, and backgrounds) to test the proposed approach. Experimental results show that the proposed YOLOv5-CA achieved a detection precision of 85.59\%, a recall of 83.70\%, and a mAP@0.5 of 89.55\%, which is superior to the popular methods, including Faster R-CNN, YOLOv3, and YOLOv5. Furthermore, our proposed approach with inference occurring at 58.82 frames per second, could be deployed for the real-time disease control requirement. In addition, the proposed YOLOv5-CA based approach could effectively capture leaf disease related visual features resulting in higher GDE detection accuracy. Overall, this study provides a favorable deep learning based approach for the rapid and accurate diagnosis of grape leaf diseases in the field of automatic disease detection.},
  keywords = {⛔ No DOI found},
  file = {/Users/mavi/Zotero/storage/HITTTAT9/Zhang et al_2022_Deep Learning Based Automatic Grape Downy Mildew Detection.pdf}
}

@online{zhangLookaheadOptimizerSteps2019,
  title = {Lookahead {{Optimizer}}: k Steps Forward, 1 Step Back},
  shorttitle = {Lookahead {{Optimizer}}},
  author = {Zhang, Michael R. and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},
  date = {2019-12-03},
  eprint = {1907.08610},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1907.08610},
  url = {http://arxiv.org/abs/1907.08610},
  urldate = {2023-02-01},
  abstract = {The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/T92UJJ7I/Zhang et al. - 2019 - Lookahead Optimizer k steps forward, 1 step back.pdf;/Users/mavi/Zotero/storage/328D5RII/1907.html}
}

@inproceedings{zhaoConvexNonconvexLoss2010,
  title = {From {{Convex}} to {{Nonconvex}}: {{A Loss Function Analysis}} for {{Binary Classification}}},
  shorttitle = {From {{Convex}} to {{Nonconvex}}},
  booktitle = {2010 {{IEEE International Conference}} on {{Data Mining Workshops}}},
  author = {Zhao, Lei and Mammadov, Musa and Yearwood, John},
  date = {2010-12},
  pages = {1281--1288},
  publisher = {{IEEE}},
  location = {{Sydney, TBD, Australia}},
  doi = {10.1109/ICDMW.2010.57},
  url = {http://ieeexplore.ieee.org/document/5693441/},
  urldate = {2022-05-19},
  abstract = {Problems of data classification can be studied in the framework of regularization theory as ill-posed problems. In this framework, loss functions play an important role in the application of regularization theory to classification. In this paper, we review some important convex loss functions, including hinge loss, square loss, modified square loss, exponential loss, logistic regression loss, as well as some non-convex loss functions, such as sigmoid loss, ϕ-loss, ramp loss, normalized sigmoid loss, and the loss function of 2 layer neural network. Based on the analysis of these loss functions, we propose a new differentiable nonconvex loss function, called smoothed 0-1 loss function, which is a natural approximation of the 0-1 loss function. To compare the performance of different loss functions, we propose two binary classification algorithms for binary classification, one for convex loss functions, the other for non-convex loss functions. A set of experiments are launched on several binary data sets from the UCI repository. The results show that the proposed smoothed 0-1 loss function is robust, especially for those noisy data sets with many outliers.},
  eventtitle = {2010 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  isbn = {978-1-4244-9244-2},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/2FSV3NZY/Zhao et al. - 2010 - From Convex to Nonconvex A Loss Function Analysis.pdf}
}

@online{zhaoFastSegmentAnything2023,
  title = {Fast {{Segment Anything}}},
  author = {Zhao, Xu and Ding, Wenchao and An, Yongqi and Du, Yinglong and Yu, Tao and Li, Min and Tang, Ming and Wang, Jinqiao},
  date = {2023-06-21},
  eprint = {2306.12156},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.12156},
  url = {http://arxiv.org/abs/2306.12156},
  urldate = {2023-07-03},
  abstract = {The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.},
  pubstate = {preprint},
  keywords = {\_tablet,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/JMUAFHX2/Zhao et al_2023_Fast Segment Anything.pdf;/Users/mavi/Zotero/storage/XNN7I47J/2306.html}
}

@online{zhaoInfoVAEInformationMaximizing2018,
  title = {{{InfoVAE}}: {{Information Maximizing Variational Autoencoders}}},
  shorttitle = {{{InfoVAE}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  date = {2018-05-30},
  eprint = {1706.02262},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1706.02262},
  url = {http://arxiv.org/abs/1706.02262},
  urldate = {2022-12-25},
  abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mavi/Zotero/storage/K5E4QUTA/Zhao et al. - 2018 - InfoVAE Information Maximizing Variational Autoen.pdf;/Users/mavi/Zotero/storage/8P66CTT9/1706.html}
}

@online{zhaoPyramidSceneParsing2017,
  title = {Pyramid {{Scene Parsing Network}}},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  date = {2017-04-27},
  eprint = {1612.01105},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.01105},
  urldate = {2022-06-03},
  abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-regionbased context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixellevel prediction. The proposed approach achieves state-ofthe-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/RKVES75W/Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf}
}

@article{zhouComparisonClassicObjectdetection2022,
  title = {Comparison of Classic Object-Detection Techniques for Automated Sewer Defect Detection},
  author = {Zhou, Qianqian and Situ, Zuxiang and Teng, Shuai and Chen, Weifeng and Chen, Gongfa and Su, Jiongheng},
  date = {2022-03-01},
  journaltitle = {Journal of Hydroinformatics},
  shortjournal = {Journal of Hydroinformatics},
  volume = {24},
  number = {2},
  pages = {406--419},
  issn = {1464-7141},
  doi = {10.2166/hydro.2022.132},
  url = {https://doi.org/10.2166/hydro.2022.132},
  urldate = {2022-10-26},
  abstract = {Sewer systems play a key role in cities to ensure public assets and safety. Timely detection of defects can effectively alleviate system deterioration. Conventional manual inspection is labor-intensive, error-prone and expensive. Object detection is a powerful deep learning technique that can complement and/or replace conventional inspection, especially in complex environments. This study compares two classic object-detection methods, namely faster region-based convolutional neural network (R-CNN) and You Only Look Once (YOLO), for the detection and localization of five types of sewer defects. Model performances are evaluated based on their detection accuracy and processing speed under parameterization impacts of dataset size and training parameters. Results show that faster R-CNN achieved higher prediction accuracy. Training dataset size and maximum number of epochs (MaxE) had dominant impacts on model performances of faster R-CNN and YOLO, respectively. The processing speed increased along with the increasing training data for faster R-CNN, but did not vary significantly for YOLO. The models' abilities to detect disjoint and residential wall were highest, whereas crack and tree root were more difficult to detect. The results help to better understand the strengths and weaknesses of the classic methods and provide a useful user guidance for practical applications in automated sewer defect detection.},
  file = {/Users/mavi/Zotero/storage/7XIVPKNT/Zhou et al. - 2022 - Comparison of classic object-detection techniques .pdf;/Users/mavi/Zotero/storage/BK92RZT2/Comparison-of-classic-object-detection-techniques.html}
}

@online{zhuSemanticAmodalSegmentation2016,
  title = {Semantic {{Amodal Segmentation}}},
  author = {Zhu, Yan and Tian, Yuandong and Mexatas, Dimitris and Dollár, Piotr},
  date = {2016-12-14},
  eprint = {1509.01329},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1509.01329},
  urldate = {2022-06-03},
  abstract = {Common visual recognition tasks such as classification, object detection, and semantic segmentation are rapidly reaching maturity, and given the recent rate of progress, it is not unreasonable to conjecture that techniques for many of these problems will approach human levels of performance in the next few years. In this paper we look to the future: what is the next frontier in visual recognition? We offer one possible answer to this question. We propose a detailed image annotation that captures information beyond the visible pixels and requires complex reasoning about full scene structure. Specifically, we create an amodal segmentation of each image: the full extent of each region is marked, not just the visible pixels. Annotators outline and name all salient regions in the image and specify a partial depth order. The result is a rich scene structure, including visible and occluded portions of each region, figure-ground edge information, semantic labels, and object overlap.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mavi/Zotero/storage/M2U3T5SF/Zhu et al. - 2016 - Semantic Amodal Segmentation.pdf}
}

@inproceedings{zine-el-abidineDimensionalityReductionOrdinal2021,
  title = {Dimensionality {{Reduction}} for {{Ordinal Classification}}},
  booktitle = {2021 29th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Zine-El-Abidine, Mouad and Dutagaci, Helin and Rousseau, David},
  date = {2021-08-23},
  pages = {1531--1535},
  publisher = {{IEEE}},
  location = {{Dublin, Ireland}},
  doi = {10.23919/EUSIPCO54536.2021.9616169},
  url = {https://ieeexplore.ieee.org/document/9616169/},
  urldate = {2023-02-21},
  abstract = {Many unsupervised and supervised dimension reduction techniques are available for visualization and interpretation of high-dimensional data for classification tasks. While the unsupervised techniques do not employ the class information at all, most supervised algorithms are blind to the order of classes in ordinal classification problems. In this paper, we propose a novel and intuitive dimension reduction technique specifically designed for visualization of high-dimensional features in ordinal classification tasks. The technique is an iterative process, where at each iteration a search is conducted in the high-dimensional space to find the viewpoint from which the centers of adjacent classes are seen most distant from each other. The data is then projected to the lower dimensional space defined by the optimum viewpoint. The iteration is terminated when the desired dimensionality is achieved. Experimental results on various ordinal datasets demonstrate that our technique can be used as a complementary tool to the classical dimensionality reduction methods.},
  eventtitle = {2021 29th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  isbn = {978-90-827970-6-0},
  langid = {english},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/UE8F8S7R/Zine-El-Abidine et al_2021_Dimensionality Reduction for Ordinal Classification.pdf}
}

@article{zine-el-abidineOrdinalysisInterpretabilityMultidimensional,
  title = {Ordinalysis: {{Interpretability}} of {{Multidimensional Ordinal Data}}},
  author = {Zine-El-Abidine, Mouad and Dutagaci, Helin and Rousseau, David},
  abstract = {Ordinalysis is a software that enables dimension reduction, visualization and quantitative ordinality analysis of ordinal data. It is provided as a standalone executable file with a video tutorial. Applications of the software are shown on an ordinal synthetic dataset, ordinal real feature spaces and an ordinal image dataset. Ordinalysis allows the detection of violations of ordinality, the selection of the dimension reduction to preserve ordinality best or the demonstration of existing semantic ordinality in a set of raw data.},
  langid = {english},
  keywords = {⛔ No DOI found,read},
  file = {/Users/mavi/Zotero/storage/5KSMV5AA/Zine-El-Abidine et al_Ordinalysis.pdf}
}

@incollection{zineelabidineMachineLearningBasedClassification2020,
  title = {Machine {{Learning-Based Classification}} of {{Powdery Mildew Severity}} on {{Melon Leaves}}},
  author = {Zine el Abidine, Mouad and Merdinoglu-Wiedemann, Sabine and Rasti, Pejman and Dutağacı, Helin and Rousseau, David},
  date = {2020-07-08},
  pages = {74--81},
  doi = {10.1007/978-3-030-51935-3_8},
  abstract = {Precision agriculture faces challenges related to plant disease detection. Plant phenotyping assesses the appearance to select the best genotypes that resist to varying environmental conditions via plant variety testing. In this process, official plant variety tests are currently performed in vitro by visual inspection of samples placed in a culture media. In this communication, we demonstrate the potential of a computer vision approach to perform such tests in a much faster and reproducible way. We highlight the benefit of fusing contrasts coming from front and back light. To the best of our knowledge, this is illustrated for the first time on the classification of the severity of the presence of a fungi, powdery mildew, on melon leaves with 95\% of accuracy.},
  isbn = {978-3-030-51934-6},
  keywords = {read},
  file = {/Users/mavi/Zotero/storage/VPDTI2F3/Zine el Abidine et al_2020_Machine Learning-Based Classification of Powdery Mildew Severity on Melon Leaves.pdf}
}

@article{zouFindingBestClassification2016,
  title = {Finding the {{Best Classification Threshold}} in {{Imbalanced Classification}}},
  author = {Zou, Quan and Xie, Sifa and Lin, Ziyu and Wu, Meihong and Ju, Ying},
  date = {2016-09},
  journaltitle = {Big Data Research},
  shortjournal = {Big Data Research},
  volume = {5},
  pages = {2--8},
  issn = {22145796},
  doi = {10.1016/j.bdr.2015.12.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2214579615000611},
  urldate = {2023-04-18},
  langid = {english},
  file = {/Users/mavi/Zotero/storage/GHCDHZXT/Zou et al. - 2016 - Finding the Best Classification Threshold in Imbal.pdf}
}
